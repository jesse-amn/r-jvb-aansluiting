---
title: "Aansluiting Item Analyse"
author: "J.F.J. (Jesse) van Bussel"
date: "`r format(Sys.Date(), '%Y-%m-%d')`"
output:
  html_document:
    theme: readable
    toc: true # table of contents
    toc_float: true # float: stays up left
    css: ["./css/custom.css", "./css/style_documentation.css"]
---

```{r Environment set-up, echo=FALSE, message=FALSE, warning=FALSE}
# Load files
library(lordif)
library(here)
library(tools)
library(lavaan)
library(dplyr)
```

# Aansluiting Item Response Analyse

## Introductie

Binnen dit document worden de resultaten van de item analyse voorgelegd. De resultaten zijn gebaseerd op 1-factor IRT modellen, waarbij de 1-factor IRT modellen zijn geschat met de `mirt` package.
Specifiek zijn alle onderdelen (behalve metenenmeetkunde, gedrag_houding, gedrag_houding_leerkracht, gedrag_houding_ouder) onderworpen aan een 1-factor IRT model. De resultaten van deze analyses zijn te vinden in de map `data/irt_analyses`r format(Sys.Date(), "%Y")`/`.
De resultaten van de 1-factor IRT modellen zijn vervolgens gebruikt om de vragen van de onderdelen van de aansluiting te beoordelen op hun kwaliteit, waar kwaliteit beoordeelt wordt door
een combinatie van item discriminatie, item fit, en item informatie.

Om een beter beeld te krijgen van de kwaliteit van de vragen, zijn er grafische methodes gebruikt om dit te verder te onderzoeken. Hierbij
zijn, onderanderen, een item informatie plot, item fit plot, en item discriminatie plot gebruikt. De resultaten van deze analyses zijn te vinden in de map `data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs`.

Hieronder volgt een overzicht van de resultaten van de IRT's, en de grafische methoden, als ook van de alternatieve factor analyses. 
De resultaten van de reguliere en alternatieve IRT's zijn weergegeven in de vorm van een tabel, waarbij de factor loadings, item correlaties, en model fit indices zijn weergegeven. 
De grafische methoden zijn weergegeven in de vorm van een grafiek, waarbij de resultaten van de factor analyse, en de principiele componenten analyse zijn weergegeven.

## Resultaten

### Betekenissen

```{r Betekenissen - stat extraction, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
# load data
fit <- readRDS(paste0(here(), "/data/irt_analyses2024/irt_fit_betekenissen.rds"))
data <- read.csv(paste0(here(), "/data/data_interrim/betekenissen.csv"))

# Model fit
model_fit <- as.data.frame(M2(fit, na.rm = TRUE))
model_fit <- cbind(model_fit, nrow(data))
model_fit <- rename(model_fit, `N` = `nrow(data)`)
model_fit <- tidyr::pivot_longer(model_fit, cols = -N, names_to = "Fit Index", values_to = "Value")
model_fit <- dplyr::select(model_fit, `Fit Index`, Value)
model_fit$`Cut-Off` <- c("", "", "> .05", "< .05 | .05 - .1 | > .1", "", "", "< .08 | .08 - .10 | > .10", "> .95 | .90 - .95 | < .90", "> .95 | .90 - .95 | < .90")

# item fit
item_fit <- itemfit(fit, na.rm = TRUE, fit_stats = "PV_Q1")
colnames(item_fit) <- c("Item", "X2", "df", "RMSEA", "p")
item_fit <- item_fit[order(item_fit$X2), ]
rownames(item_fit) <- NULL

robust_item_fit <- itemfit(fit, na.rm = TRUE)
colnames(robust_item_fit) <- c("Item", "PV_Q1", "df", "RMSEA", "p")
robust_item_fit <- robust_item_fit[order(robust_item_fit$PV_Q1), ]
rownames(robust_item_fit) <- NULL

# Item parameters
params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
params <- params[, -which(colnames(params) %in% c("items.g", "items.u", "means", "F1"))]
sink("/dev/null")
sum <- suppressWarnings(suppressMessages(summary(fit, silent = TRUE)))
sink()
params <- cbind(params, sum$rotF)
params <- cbind(params, sum$h2)
params <- params[, c("F1", colnames(params)[-which(colnames(params) %in% "F1")])]
colnames(params) <- c("Factor Loading", "Discrimination Parameter", "Difficulty Parameter ", "Communalities")
params <- params[order(params$`Factor Loading`, decreasing = TRUE), ]


# # Differential Item Functionining DIF
# # Prepare the data
# gender <- data$gender
# gender <- dplyr::recode(gender, "Man" = 1, "Vrouw" = 2, "Onbekend" = 2)
# gender <- as.factor(gender)
# local_df <- data[, !grepl("package_duration_raw|student_number|student_name|birth_date|gender", colnames(data))]

# # Fit the multiple group model
# dif_fit <- mirt::multipleGroup(
#   data = local_df,
#   model = 1,
#   group = gender
# )

# # Perform DIF analysis
# dif <- mirt::DIF(dif_fit, group = gender, which.par = c("a1", "d"), na.rm = TRUE, plotdif = TRUE)

# person fit
person_fit <- personfit(fit) %>%
  reframe(
    infit.outside = prop.table(table(z.infit > 1.96 | z.infit < -1.96)),
    outfit.outside = prop.table(table(z.outfit > 1.96 | z.outfit < -1.96))
  ) # lower row = non-fitting people
person_fit$Correct <- c("Correct", "Incorrect")
person_fit <- person_fit %>% dplyr::select(Correct, everything())
```

#### Model Fit

Hieronder bevinden zich de verschillende model fit indices.Hierbij een korte uitleg wat het exact betekent:

- M2: Dit is de M2-statistiek van Maydeu-Olivares, die een maat is voor de absolute fit van het model. Een lager getal duidt op een betere fit. Er is geen vaste cut-off waarde, maar over het algemeen wordt een lagere waarde als beter beschouwd. Als de bijbehorende p-waarde kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- df: de degrees of freedom van het model. Dit is het aantal parameters dat is geschat, minus het aantal constraints dat is opgelegd.
- p-value: de p-waarde van de Goodness of fit test. Als deze kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- RMSEA: RMSEA meet hoe goed een model de waargenomen data benadert, rekening houdend met de fout in het model. Lagere waarden duiden op een betere pasvorm, typisch onder 0.05 voor goede pasvorm en tussen 0.05 en 0.08 voor redelijke pasvorm.
  - RMSEA_5 en RMSEA_95: de 95% betrouwbaarheidsinterval van de RMSEA. Hoe kleiner dit interval rondom de RMSEA waarde, hoe zekerder we zijn van die waarde. 
- SRMSR: SRMSR staat voor "Standardized Root Mean Square Residual". Het is een maat voor de gemiddelde discrepantie tussen de waargenomen en voorspelde covarianties. Een lagere waarde duidt op een betere fit. Een SRMSR van minder dan 0.08 wordt over het algemeen als goed beschouwd.
- TLI: de Tucker-Lewis Index. TLI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.
- CFI: de Comparative Fit Index. De CFI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.
- N: het aantal respondenten in de steekproef.

```{r Betekenissen - model fit, echo=FALSE, message=FALSE, warning=FALSE}
model_fit$`Conclusion` <- c("", "", "Poor", "Good", "", "", "Good", "Good", "Good")
knitr::kable(model_fit)
```

Als we kijken naar de fit indices, zien dat betekenissen een goeie fit heeft bereikt in het 1-factor-IRT model. Alle fit indices zijn als goed bevonden, met uitzondering van de p-waarde.
Gezien de hoge hoeveelheid observaties (N = `r nrow(data)`), kan de p-waarde als onbetrouwbaar zijn, gezien diens gevoeligheid voor sample size. Daarom wordt de p-waarde genegeerd, en wordt er geconcludeerd dat het model goed past.

#### Person Fit

Hieronder zijn de percentages van person-item-fit indices weergegeven. Hierbij een korte uitleg wat het exact betekent:
- infit.outside: De infit (information-weighted fit) statistiek is gevoelig voor onverwacht gedrag dat invloed heeft op de antwoorden op items die de meeste informatie geven over het vaardigheidsniveau van de persoon. Met andere woorden, het is gevoeliger voor onverwachte antwoorden op items die dicht bij het vaardigheidsniveau van de persoon liggen. De infit.outside waarde in de tabel vertegenwoordigt de proportie van personen wiens infit statistiek buiten het acceptabele bereik valt, wat duidt op een slechte fit.
- outfit.outside: De outfit (outlier-sensitive fit) statistiek is gevoelig voor onverwacht gedrag dat invloed heeft op de antwoorden op items die de minste informatie geven over het vaardigheidsniveau van de persoon. Met andere woorden, het is gevoeliger voor onverwachte antwoorden op items die ver van het vaardigheidsniveau van de persoon liggen. De outfit.outside waarde in de tabel vertegenwoordigt de proportie van personen wiens outfit statistiek buiten het acceptabele bereik valt, wat duidt op een slechte fit.

```{r Betekenissen - person fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(person_fit)
```

Gezien zowel de infit en outfit statistieken boven 95 procent vallen, is er geen reden om aan te nemen dat de personen niet goed passen in het model. De proportie van personen wiens infit en outfit statistieken buiten het acceptabele bereik vallen, is verwaarloosbaar klein.

#### Item Fit

Hieronder bevinden zich de verschillende item fit indices. Hierbij een korte uitleg wat het exact betekent:

- X2: Dit is de X2-statistiek van de item fit test. Een lager getal duidt op een betere fit. Er is geen vaste cut-off waarde, maar over het algemeen wordt een lagere waarde als beter beschouwd. Als de bijbehorende p-waarde kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het item niet goed past.
- df: de degrees of freedom van het model. Dit is het aantal parameters dat is geschat, minus het aantal constraints dat is opgelegd.
- p-value: de p-waarde van de Goodness of fit test. Als deze kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- RMSEA: RMSEA meet hoe goed een model de waargenomen data benadert, rekening houdend met de fout in het model. Lagere waarden duiden op een betere pasvorm, typisch onder 0.05 voor goede pasvorm en tussen 0.05 en 0.08 voor redelijke pasvorm.
 
```{r Betekenissen - item fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(item_fit)
```

Op basis van de resultaten, blijken 4 vragen niet goed te passen:

- ASL_betekenissen_010
- ASL_betekenissen_008
- ASL_betekenissen_016
- ASL_betekenissen_013

Dit betekent dat deze waardes niet goed passen in het 1-factor IRT model.Als bepaalde waarden niet goed passen in dit model, kan dit verschillende dingen betekenen.
Ten eerste kan het zijn dat de items niet allemaal dezelfde onderliggende eigenschap meten. Ten tweede kan het zijn dat sommige items verkeerd worden begrepen door de respondenten.
Ten derde kan het zijn dat de veronderstelling van lokale onafhankelijkheid wordt geschonden. Dit betekent dat de reacties op de items afhankelijk zijn van elkaar, zelfs na controle voor de onderliggende eigenschap. Bijvoorbeeld, als het beantwoorden van het ene item helpt bij het beantwoorden van een ander item, dan zullen deze items niet goed passen in het model.

Hieronder volgt een robuste variant van de X2 test, namelijk de PV_Q1 test. Hieronder volgt een korte uitleg wat het exact betekent:

De 'PV_Q1'-statistiek is een afgeleide versie van de Q1-statistiek, een chi-kwadraatstatistiek gebruikt in item respons theorie (IRT) modellen om te testen of een item goed past bij het model. 
Een hoge 'PV_Q1'-waarde duidt op een slechte overeenkomst tussen de waargenomen gegevens en voorspellingen van het model, wat suggereert dat het item mogelijk niet conform het verwachte gedrag 
volgens het IRT-model functioneert. Een niet-significante p-waarde, meestal p > 0.05, in combinatie met lage 'PV_Q1'- en RMSEA.PV_Q1-waarden, geeft aan dat het item goed past bij het IRT-model.
Daarnaast is het belangrijk op te merken dat de 'PV_Q1'-statistiek robuust is tegen invloeden van steekproefomvang en lokale afhankelijkheid, waardoor het een betrouwbaar instrument is om de pasvorm
van een item met het IRT-model te beoordelen, ongeacht de omvang van de steekproef of eventuele lokale onderlinge afhankelijkheden tussen de waarnemingen. Hieronder de resultaten:

```{r betekenissen - robust item fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(robust_item_fit)
```

Om een beter beeld te krijgen, volgen hieronder the item parameters van de vragen.

```{r Betekenissen - item parameters, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(params)
```

Op basis van de factor loadings, blijken 3 vragen niet goed te passen:

- ASL_betekenissen_008: 0.2941963
- ASL_betekenissen_015: 0.2504362
- ASL_betekenissen_002: 0.1635876

In IRT, meet een factor loading (ook bekend als een discriminatieparameter) hoe goed een item onderscheid maakt tussen personen met verschillende niveaus van de onderliggende eigenschap. Een hoge factor loading betekent dat het item gevoelig is voor verschillen in de onderliggende eigenschap.
Het belangrijkste verschil met factor analyse, is dat in IRT, de factor loadings ook rekening houden met de moeilijkheidsgraad van de items. Dit betekent dat items die dezelfde moeilijkheidsgraad hebben (dat wil zeggen, ze zijn even moeilijk om te beantwoorden) worden geacht dezelfde hoeveelheid informatie te geven over de onderliggende eigenschap, ongeacht hun factor loadings. Dit is niet het geval in factoranalyse, waar de factor loadings alleen de relatie tussen de geobserveerde variabelen en de onderliggende factor meten.
Tevens kan het zijn dat de factor loadings en de item parameters niet overeenkomen. Bijvoorbeeld een item kan een hoge factor loading hebben (wat aangeeft dat het goed discrimineert) maar toch een slechte fit hebben (X2-statistiek) als het bijvoorbeeld verkeerd wordt begrepen door de respondenten. Omgekeerd kan een item goed passen bij het model maar niet goed discrimineren tussen verschillende niveaus van de eigenschap.

##### Item Characteristic Curves

Hieronder bevinden zich de item characteristic curves. Hierbij een korte uitleg wat het exact betekent:

- De lijnen geven de item characteristic curves weer. Dit is de kans dat een persoon met een bepaald niveau van de onderliggende eigenschap het item correct beantwoordt. Hoe hoger de curve, hoe groter de kans dat een persoon met een bepaald niveau van de onderliggende eigenschap het item correct beantwoordt.
- De x-as van de curve vertegenwoordigt het niveau van de onderliggende eigenschap (bijvoorbeeld vaardigheid of kennis) die een persoon heeft, terwijl de y-as de kans vertegenwoordigt dat een persoon het item correct beantwoordt.
- Hoe stijler de ICC-lijn is, hoe beter het item onderscheid maakt tussen personen met verschillende niveaus van de onderliggende eigenschap. Dit betekent dat het item gevoelig is voor verschillen in de onderliggende eigenschap.
- Hoe hoger het y-axale middelpunt, hoe moeilijker het item is. 

De grafieken voor de ICC en IIC hernoemen de items op volgorde die het IRT model aanbied. Hieronder een tabel met de originele namen van de items, en de bijbehorende item nummers.

```{r betekenissen - ICC, echo=FALSE, message=FALSE, warning=FALSE}
params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
item_names <- extract.mirt(fit, what = "itemnames")

df <- data.frame(
  item_number = paste0("Item ", seq_along(item_names)),
  item_names = item_names,
  item_discrimination = params$items.a,
  item_difficulty = params$items.b
)
knitr::kable(df)
```

![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/betekenissen_ICC_combined.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/betekenissen_ICC_separate.png)

##### Item Information Curves

Hieronder bevinden zich de item information curves. Hierbij een korte uitleg wat het exact betekent:

- De lijnen geven de item information curves weer. Dit is de hoeveelheid informatie die een item geeft over de onderliggende eigenschap. Hoe hoger de curve, hoe meer informatie het item geeft over de onderliggende eigenschap.
- Hoe verder naar rechts de piek van de curve ligt, hoe meer informatie het item geeft over hooge niveaus van de onderliggende eigenschap. 
- Hoe verder naar links de piek van de curve ligt, hoe meer informatie het item geeft over lage niveaus van de onderliggende eigenschap.


De grafieken voor de ICC en IIC hernoemen de items op volgorde die het IRT model aanbied. Hieronder een tabel met de originele namen van de items, en de bijbehorende item nummers.

```{r betekenissen - IIC, echo=FALSE, message=FALSE, warning=FALSE}
params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
item_names <- extract.mirt(fit, what = "itemnames")

df <- data.frame(
  item_number = paste0("Item ", seq_along(item_names)),
  item_names = item_names,
  item_discrimination = params$items.a,
  item_difficulty = params$items.b
)
knitr::kable(df)
```


![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/betekenissen_IIC_combined.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/betekenissen_IIC_separate.png)


##### Item Person Map

Hieronder bevindt zich de item person map. Hierbij een korte uitleg wat het exact betekent:

- De x-as van de grafiek vertegenwoordigt het niveau van de onderliggende eigenschap (bijvoorbeeld vaardigheid of kennis) die een persoon heeft, terwijl de y-as de hoeveelheid participanten vertegenwoordigt.
- Onder de grafiek kan bekeken worden welke vraag ongeveer bij welke hoogte van onderliggende eigenschap past, en hoeveel leerlingen hier daadwerkelijk bij horen.

![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/betekenissen_item_person_fit.png)


##### Test Characteristic and Information Curves

Gelijk aan de Item characteristics en information curves, maar dan voor de test als geheel. Hierbij een korte uitleg wat het exact betekent:

- De lijnen geven de test characteristic curves weer. Deze geeft aan hoe goed de test informatie geeft over de onderliggende eigenschap. Hoe hoger de curve, hoe meer informatie de test geeft over de onderliggende eigenschap op dat niveau.
- Grofweg kunnen de lijnen en de curve gezien woren als een amalgamatie van de onderliggende item characteristic en information curves.

![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/betekenissen_TCC.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/betekenissen_TIC.png)


### Cijfers

```{r cijfers - stat extraction, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
# load data
fit <- readRDS(paste0(here(), "/data/irt_analyses2024/irt_fit_cijfers.rds"))
data <- read.csv(paste0(here(), "/data/data_interrim/cijfers.csv"))

# Model fit
model_fit <- as.data.frame(M2(fit, na.rm = TRUE))
model_fit <- cbind(model_fit, nrow(data))
model_fit <- rename(model_fit, `N` = `nrow(data)`)
model_fit <- tidyr::pivot_longer(model_fit, cols = -N, names_to = "Fit Index", values_to = "Value")
model_fit <- dplyr::select(model_fit, `Fit Index`, Value)
model_fit$`Cut-Off` <- c("", "", "> .05", "< .05 | .05 - .1 | > .1", "", "", "< .08 | .08 - .10 | > .10", "> .95 | .90 - .95 | < .90", "> .95 | .90 - .95 | < .90")

# item fit
item_fit <- itemfit(fit, na.rm = TRUE)
colnames(item_fit) <- c("Item", "X2", "df", "RMSEA", "p")
item_fit <- item_fit[order(item_fit$X2), ]
rownames(item_fit) <- NULL

robust_item_fit <- itemfit(fit, na.rm = TRUE)
colnames(robust_item_fit) <- c("Item", "PV_Q1", "df", "RMSEA", "p")
robust_item_fit <- robust_item_fit[order(robust_item_fit$PV_Q1), ]
rownames(robust_item_fit) <- NULL


# Item parameters
params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
params <- params[, -which(colnames(params) %in% c("items.g", "items.u", "means", "F1"))]
sink("/dev/null")
sum <- suppressWarnings(suppressMessages(summary(fit, silent = TRUE)))
sink()
params <- cbind(params, sum$rotF)
params <- cbind(params, sum$h2)
params <- params[, c("F1", colnames(params)[-which(colnames(params) %in% "F1")])]
colnames(params) <- c("Factor Loading", "Discrimination Parameter", "Difficulty Parameter ", "Communalities")
params <- params[order(params$`Factor Loading`, decreasing = TRUE), ]


# # Differential Item Functionining DIF
# # Prepare the data
# gender <- data$gender
# gender <- dplyr::recode(gender, "Man" = 1, "Vrouw" = 2, "Onbekend" = 2)
# gender <- as.factor(gender)
# local_df <- data[, !grepl("package_duration_raw|student_number|student_name|birth_date|gender", colnames(data))]

# # Fit the multiple group model
# dif_fit <- mirt::multipleGroup(
#   data = local_df,
#   model = 1,
#   group = gender
# )

# # Perform DIF analysis
# dif <- mirt::DIF(dif_fit, group = gender, which.par = c("a1", "d"), na.rm = TRUE, plotdif = TRUE)

# person fit
person_fit <- personfit(fit) %>%
  reframe(
    infit.outside = prop.table(table(z.infit > 1.96 | z.infit < -1.96)),
    outfit.outside = prop.table(table(z.outfit > 1.96 | z.outfit < -1.96))
  ) # lower row = non-fitting people
person_fit$Correct <- c("Correct", "Incorrect")
person_fit <- person_fit %>% dplyr::select(Correct, everything())
```

#### Model Fit

Hieronder bevinden zich de verschillende model fit indices.Hierbij een korte uitleg wat het exact betekent:

- M2: Dit is de M2-statistiek van Maydeu-Olivares, die een maat is voor de absolute fit van het model. Een lager getal duidt op een betere fit. Er is geen vaste cut-off waarde, maar over het algemeen wordt een lagere waarde als beter beschouwd. Als de bijbehorende p-waarde kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- df: de degrees of freedom van het model. Dit is het aantal parameters dat is geschat, minus het aantal constraints dat is opgelegd.
- p-value: de p-waarde van de Goodness of fit test. Als deze kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- RMSEA: RMSEA meet hoe goed een model de waargenomen data benadert, rekening houdend met de fout in het model. Lagere waarden duiden op een betere pasvorm, typisch onder 0.05 voor goede pasvorm en tussen 0.05 en 0.08 voor redelijke pasvorm.
  - RMSEA_5 en RMSEA_95: de 95% betrouwbaarheidsinterval van de RMSEA. Hoe kleiner dit interval rondom de RMSEA waarde, hoe zekerder we zijn van die waarde. 
- SRMSR: SRMSR staat voor "Standardized Root Mean Square Residual". Het is een maat voor de gemiddelde discrepantie tussen de waargenomen en voorspelde covarianties. Een lagere waarde duidt op een betere fit. Een SRMSR van minder dan 0.08 wordt over het algemeen als goed beschouwd.
- TLI: de Tucker-Lewis Index. TLI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.
- CFI: de Comparative Fit Index. De CFI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.
- N: het aantal respondenten in de steekproef.

```{r cijfers - model fit, echo=FALSE, message=FALSE, warning=FALSE}
model_fit$`Conclusion` <- c("", "", "Poor", "Acceptable", "", "", "Good", "Good", "Good")
knitr::kable(model_fit)
```

Als we kijken naar de fit indices, zien we dat cijfers een goeie fit heeft bereikt in het 1-factor-IRT model. Alle fit indices zijn als goed of acceptabel bevonden, met uitzondering van de p-waarde.
Gezien de hoge hoeveelheid observaties (N = `r nrow(data)`), kan de p-waarde als onbetrouwbaar zijn, gezien diens gevoeligheid voor sample size. Daarom wordt de p-waarde genegeerd, en wordt er geconcludeerd dat het model goed past.

#### Person Fit

Hieronder zijn de percentages van person-item-fit indices weergegeven. Hierbij een korte uitleg wat het exact betekent:
- infit.outside: De infit (information-weighted fit) statistiek is gevoelig voor onverwacht gedrag dat invloed heeft op de antwoorden op items die de meeste informatie geven over het vaardigheidsniveau van de persoon. Met andere woorden, het is gevoeliger voor onverwachte antwoorden op items die dicht bij het vaardigheidsniveau van de persoon liggen. De infit.outside waarde in de tabel vertegenwoordigt de proportie van personen wiens infit statistiek buiten het acceptabele bereik valt, wat duidt op een slechte fit.
- outfit.outside: De outfit (outlier-sensitive fit) statistiek is gevoelig voor onverwacht gedrag dat invloed heeft op de antwoorden op items die de minste informatie geven over het vaardigheidsniveau van de persoon. Met andere woorden, het is gevoeliger voor onverwachte antwoorden op items die ver van het vaardigheidsniveau van de persoon liggen. De outfit.outside waarde in de tabel vertegenwoordigt de proportie van personen wiens outfit statistiek buiten het acceptabele bereik valt, wat duidt op een slechte fit.

```{r cijfers - person fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(person_fit)
```

Gezien zowel de infit en outfit statistieken rond de 95 procent vallen, is er geen reden om aan te nemen dat de personen niet goed passen in het model. De proportie wiens outfit statistiek buiten de acceptable   

#### Item Fit

Hieronder bevinden zich de verschillende item fit indices. Hierbij een korte uitleg wat het exact betekent:

- X2: Dit is de X2-statistiek van de item fit test. Een lager getal duidt op een betere fit. Er is geen vaste cut-off waarde, maar over het algemeen wordt een lagere waarde als beter beschouwd. Als de bijbehorende p-waarde kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het item niet goed past.
- df: de degrees of freedom van het model. Dit is het aantal parameters dat is geschat, minus het aantal constraints dat is opgelegd.
- p-value: de p-waarde van de Goodness of fit test. Als deze kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- RMSEA: RMSEA meet hoe goed een model de waargenomen data benadert, rekening houdend met de fout in het model. Lagere waarden duiden op een betere pasvorm, typisch onder 0.05 voor goede pasvorm en tussen 0.05 en 0.08 voor redelijke pasvorm.
 
```{r cijfers - item fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(item_fit)
```

Op basis van de resultaten, geen van de vragen goed bij het model te passen.


Dit betekent dat deze waardes niet goed passen in het 1-factor IRT model. Als bepaalde waarden niet goed passen in dit model, kan dit verschillende dingen betekenen.
Ten eerste kan het zijn dat de items niet allemaal dezelfde onderliggende eigenschap meten. Ten tweede kan het zijn dat sommige items verkeerd worden begrepen door de respondenten.
Ten derde kan het zijn dat de veronderstelling van lokale onafhankelijkheid wordt geschonden. Dit betekent dat de reacties op de items afhankelijk zijn van elkaar, zelfs na controle voor de onderliggende eigenschap. Bijvoorbeeld, als het beantwoorden van het ene item helpt bij het beantwoorden van een ander item, dan zullen deze items niet goed passen in het model.

Hieronder volgt een robuste variant van de X2 test, namelijk de PV_Q1 test. Hieronder volgt een korte uitleg wat het exact betekent:

De 'PV_Q1'-statistiek is een afgeleide versie van de Q1-statistiek, een chi-kwadraatstatistiek gebruikt in item respons theorie (IRT) modellen om te testen of een item goed past bij het model. 
Een hoge 'PV_Q1'-waarde duidt op een slechte overeenkomst tussen de waargenomen gegevens en voorspellingen van het model, wat suggereert dat het item mogelijk niet conform het verwachte gedrag 
volgens het IRT-model functioneert. Een niet-significante p-waarde, meestal p > 0.05, in combinatie met lage 'PV_Q1'- en RMSEA.PV_Q1-waarden, geeft aan dat het item goed past bij het IRT-model.
Daarnaast is het belangrijk op te merken dat de 'PV_Q1'-statistiek robuust is tegen invloeden van steekproefomvang en lokale afhankelijkheid, waardoor het een betrouwbaar instrument is om de pasvorm
van een item met het IRT-model te beoordelen, ongeacht de omvang van de steekproef of eventuele lokale onderlinge afhankelijkheden tussen de waarnemingen. Hieronder de resultaten:

```{r cijfers - robust item fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(robust_item_fit)
```

Om een beter beeld te krijgen, volgen hieronder the item parameters van de vragen.

```{r cijfers - item parameters, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(params)
```

Op basis van de factor loadings, blijken 2 vragen niet goed te passen:

- ASL_cijfers_019: 0.2918689
- ASL_cijfers_020: 0.2505499

In IRT, meet een factor loading (ook bekend als een discriminatieparameter) hoe goed een item onderscheid maakt tussen personen met verschillende niveaus van de onderliggende eigenschap. Een hoge factor loading betekent dat het item gevoelig is voor verschillen in de onderliggende eigenschap.
Het belangrijkste verschil met factor analyse, is dat in IRT, de factor loadings ook rekening houden met de moeilijkheidsgraad van de items. Dit betekent dat items die dezelfde moeilijkheidsgraad hebben (dat wil zeggen, ze zijn even moeilijk om te beantwoorden) worden geacht dezelfde hoeveelheid informatie te geven over de onderliggende eigenschap, ongeacht hun factor loadings. Dit is niet het geval in factoranalyse, waar de factor loadings alleen de relatie tussen de geobserveerde variabelen en de onderliggende factor meten.
Tevens kan het zijn dat de factor loadings en de item parameters niet overeenkomen. Bijvoorbeeld een item kan een hoge factor loading hebben (wat aangeeft dat het goed discrimineert) maar toch een slechte fit hebben (X2-statistiek) als het bijvoorbeeld verkeerd wordt begrepen door de respondenten. Omgekeerd kan een item goed passen bij het model maar niet goed discrimineren tussen verschillende niveaus van de eigenschap.

##### Item Characteristic Curves

Hieronder bevinden zich de item characteristic curves. Hierbij een korte uitleg wat het exact betekent:

- De lijnen geven de item characteristic curves weer. Dit is de kans dat een persoon met een bepaald niveau van de onderliggende eigenschap het item correct beantwoordt. Hoe hoger de curve, hoe groter de kans dat een persoon met een bepaald niveau van de onderliggende eigenschap het item correct beantwoordt.
- De x-as van de curve vertegenwoordigt het niveau van de onderliggende eigenschap (bijvoorbeeld vaardigheid of kennis) die een persoon heeft, terwijl de y-as de kans vertegenwoordigt dat een persoon het item correct beantwoordt.
- Hoe stijler de ICC-lijn is, hoe beter het item onderscheid maakt tussen personen met verschillende niveaus van de onderliggende eigenschap. Dit betekent dat het item gevoelig is voor verschillen in de onderliggende eigenschap.
- Hoe hoger het y-axale middelpunt, hoe moeilijker het item is. 

De grafieken voor de ICC en IIC hernoemen de items op volgorde die het IRT model aanbied. Hieronder een tabel met de originele namen van de items, en de bijbehorende item nummers.

```{r cijfers - ICC, echo=FALSE, message=FALSE, warning=FALSE}
params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
item_names <- extract.mirt(fit, what = "itemnames")

df <- data.frame(
  item_number = paste0("Item ", seq_along(item_names)),
  item_names = item_names,
  item_discrimination = params$items.a,
  item_difficulty = params$items.b
)
knitr::kable(df)
```


![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/cijfers_ICC_combined.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/cijfers_ICC_separate.png)

##### Item Information Curves

Hieronder bevinden zich de item information curves. Hierbij een korte uitleg wat het exact betekent:

- De lijnen geven de item information curves weer. Dit is de hoeveelheid informatie die een item geeft over de onderliggende eigenschap. Hoe hoger de curve, hoe meer informatie het item geeft over de onderliggende eigenschap.
- Hoe verder naar rechts de piek van de curve ligt, hoe meer informatie het item geeft over hooge niveaus van de onderliggende eigenschap. 
- Hoe verder naar links de piek van de curve ligt, hoe meer informatie het item geeft over lage niveaus van de onderliggende eigenschap.

De grafieken voor de ICC en IIC hernoemen de items op volgorde die het IRT model aanbied. Hieronder een tabel met de originele namen van de items, en de bijbehorende item nummers.

```{r cijfers - IIC, echo=FALSE, message=FALSE, warning=FALSE}
params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
item_names <- extract.mirt(fit, what = "itemnames")

df <- data.frame(
  item_number = paste0("Item ", seq_along(item_names)),
  item_names = item_names,
  item_discrimination = params$items.a,
  item_difficulty = params$items.b
)
knitr::kable(df)
```


![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/cijfers_IIC_combined.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/cijfers_IIC_separate.png)


##### Item Person Map

Hieronder bevindt zich de item person map. Hierbij een korte uitleg wat het exact betekent:

- De x-as van de grafiek vertegenwoordigt het niveau van de onderliggende eigenschap (bijvoorbeeld vaardigheid of kennis) die een persoon heeft, terwijl de y-as de hoeveelheid participanten vertegenwoordigt.
- Onder de grafiek kan bekeken worden welke vraag ongeveer bij welke hoogte van onderliggende eigenschap past, en hoeveel leerlingen hier daadwerkelijk bij horen.

![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/cijfers_item_person_fit.png)


##### Test Characteristic and Information Curves

Gelijk aan de Item characteristics en information curves, maar dan voor de test als geheel. Hierbij een korte uitleg wat het exact betekent:

- De lijnen geven de test characteristic curves weer. Deze geeft aan hoe goed de test informatie geeft over de onderliggende eigenschap. Hoe hoger de curve, hoe meer informatie de test geeft over de onderliggende eigenschap op dat niveau.
- Grofweg kunnen de lijnen en de curve gezien woren als een amalgamatie van de onderliggende item characteristic en information curves.

![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/cijfers_TCC.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/cijfers_TIC.png)


### Componenten

```{r componenten - stat extraction, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
# load data
fit <- readRDS(paste0(here(), "/data/irt_analyses2024/irt_fit_componenten.rds"))
data <- read.csv(paste0(here(), "/data/data_interrim/componenten.csv"))

# Model fit
model_fit <- as.data.frame(M2(fit, na.rm = TRUE))
model_fit <- cbind(model_fit, nrow(data))
model_fit <- rename(model_fit, `N` = `nrow(data)`)
model_fit <- tidyr::pivot_longer(model_fit, cols = -N, names_to = "Fit Index", values_to = "Value")
model_fit <- dplyr::select(model_fit, `Fit Index`, Value)
model_fit$`Cut-Off` <- c("", "", "> .05", "< .05 | .05 - .1 | > .1", "", "", "< .08 | .08 - .10 | > .10", "> .95 | .90 - .95 | < .90", "> .95 | .90 - .95 | < .90")

# item fit
item_fit <- itemfit(fit, na.rm = TRUE)
colnames(item_fit) <- c("Item", "X2", "df", "RMSEA", "p")
item_fit <- item_fit[order(item_fit$X2), ]
rownames(item_fit) <- NULL

robust_item_fit <- itemfit(fit, na.rm = TRUE)
colnames(robust_item_fit) <- c("Item", "PV_Q1", "df", "RMSEA", "p")
robust_item_fit <- robust_item_fit[order(robust_item_fit$PV_Q1), ]
rownames(robust_item_fit) <- NULL

# Item parameters
params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
params <- params[, -which(colnames(params) %in% c("items.g", "items.u", "means", "F1"))]
sink("/dev/null")
sum <- suppressWarnings(suppressMessages(summary(fit, silent = TRUE)))
sink()
params <- cbind(params, sum$rotF)
params <- cbind(params, sum$h2)
params <- params[, c("F1", colnames(params)[-which(colnames(params) %in% "F1")])]
colnames(params) <- c("Factor Loading", "Discrimination Parameter", "Difficulty Parameter ", "Communalities")
params <- params[order(params$`Factor Loading`, decreasing = TRUE), ]


# # Differential Item Functionining DIF
# # Prepare the data
# gender <- data$gender
# gender <- dplyr::recode(gender, "Man" = 1, "Vrouw" = 2, "Onbekend" = 2)
# gender <- as.factor(gender)
# local_df <- data[, !grepl("package_duration_raw|student_number|student_name|birth_date|gender", colnames(data))]

# # Fit the multiple group model
# dif_fit <- mirt::multipleGroup(
#   data = local_df,
#   model = 1,
#   group = gender
# )

# # Perform DIF analysis
# dif <- mirt::DIF(dif_fit, group = gender, which.par = c("a1", "d"), na.rm = TRUE, plotdif = TRUE)

# Person fit
# person_fit <- personfit(fit)
# person_fit <-
#   reframe(
#     infit.outside = prop.table(table(z.infit > 1.96 | z.infit < -1.96)),
#     outfit.outside = prop.table(table(z.outfit > 1.96 | z.outfit < -1.96))
#   ) # lower row = non-fitting people
# person_fit$Correct <- c("Correct", "Incorrect")
# person_fit <- person_fit %>% dplyr::select(Correct, everything())
```

#### Model Fit

Hieronder bevinden zich de verschillende model fit indices.Hierbij een korte uitleg wat het exact betekent:

- M2: Dit is de M2-statistiek van Maydeu-Olivares, die een maat is voor de absolute fit van het model. Een lager getal duidt op een betere fit. Er is geen vaste cut-off waarde, maar over het algemeen wordt een lagere waarde als beter beschouwd. Als de bijbehorende p-waarde kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- df: de degrees of freedom van het model. Dit is het aantal parameters dat is geschat, minus het aantal constraints dat is opgelegd.
- p-value: de p-waarde van de Goodness of fit test. Als deze kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- RMSEA: RMSEA meet hoe goed een model de waargenomen data benadert, rekening houdend met de fout in het model. Lagere waarden duiden op een betere pasvorm, typisch onder 0.05 voor goede pasvorm en tussen 0.05 en 0.08 voor redelijke pasvorm.
  - RMSEA_5 en RMSEA_95: de 95% betrouwbaarheidsinterval van de RMSEA. Hoe kleiner dit interval rondom de RMSEA waarde, hoe zekerder we zijn van die waarde. 
- SRMSR: SRMSR staat voor "Standardized Root Mean Square Residual". Het is een maat voor de gemiddelde discrepantie tussen de waargenomen en voorspelde covarianties. Een lagere waarde duidt op een betere fit. Een SRMSR van minder dan 0.08 wordt over het algemeen als goed beschouwd.
- TLI: de Tucker-Lewis Index. TLI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.
- CFI: de Comparative Fit Index. De CFI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.
- N: het aantal respondenten in de steekproef.

```{r componenten - model fit, echo=FALSE, message=FALSE, warning=FALSE}
model_fit$`Conclusion` <- c("", "", "Poor", "Good", "", "", "Good", "Good", "Good")
knitr::kable(model_fit)
```

Als we kijken naar de fit indices, zien dat componenten een goeie fit heeft bereikt in het 1-factor-IRT model. Alle fit indices zijn als goed bevonden, met uitzondering van de p-waarde.
Gezien de hoge hoeveelheid observaties (N = `r nrow(data)`), kan de p-waarde als onbetrouwbaar zijn, gezien diens gevoeligheid voor sample size. Daarom wordt de p-waarde genegeerd, en wordt er geconcludeerd dat het model goed past.

#### Person Fit

Hieronder zijn de percentages van person-item-fit indices weergegeven. Hierbij een korte uitleg wat het exact betekent:
- infit.outside: De infit (information-weighted fit) statistiek is gevoelig voor onverwacht gedrag dat invloed heeft op de antwoorden op items die de meeste informatie geven over het vaardigheidsniveau van de persoon. Met andere woorden, het is gevoeliger voor onverwachte antwoorden op items die dicht bij het vaardigheidsniveau van de persoon liggen. De infit.outside waarde in de tabel vertegenwoordigt de proportie van personen wiens infit statistiek buiten het acceptabele bereik valt, wat duidt op een slechte fit.
- outfit.outside: De outfit (outlier-sensitive fit) statistiek is gevoelig voor onverwacht gedrag dat invloed heeft op de antwoorden op items die de minste informatie geven over het vaardigheidsniveau van de persoon. Met andere woorden, het is gevoeliger voor onverwachte antwoorden op items die ver van het vaardigheidsniveau van de persoon liggen. De outfit.outside waarde in de tabel vertegenwoordigt de proportie van personen wiens outfit statistiek buiten het acceptabele bereik valt, wat duidt op een slechte fit.

```{r componenten - person fit, echo=FALSE, message=FALSE, warning=FALSE}
# knitr::kable(person_fit)
```

Gezien zowel de infit en outfit statistieken boven 95 procent vallen, is er geen reden om aan te nemen dat de personen niet goed passen in het model. De proportie van personen wiens infit en outfit statistieken buiten het acceptabele bereik vallen, is verwaarloosbaar klein.

#### Item Fit

Hieronder bevinden zich de verschillende item fit indices. Hierbij een korte uitleg wat het exact betekent:

- X2: Dit is de X2-statistiek van de item fit test. Een lager getal duidt op een betere fit. Er is geen vaste cut-off waarde, maar over het algemeen wordt een lagere waarde als beter beschouwd. Als de bijbehorende p-waarde kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het item niet goed past.
- df: de degrees of freedom van het model. Dit is het aantal parameters dat is geschat, minus het aantal constraints dat is opgelegd.
- p-value: de p-waarde van de Goodness of fit test. Als deze kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- RMSEA: RMSEA meet hoe goed een model de waargenomen data benadert, rekening houdend met de fout in het model. Lagere waarden duiden op een betere pasvorm, typisch onder 0.05 voor goede pasvorm en tussen 0.05 en 0.08 voor redelijke pasvorm.
 
```{r componenten - item fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(item_fit)
```

Op basis van de resultaten, blijken 4 vragen niet goed te passen:

- ASL_componenten_002
- ASL_componenten_006
- ASL_componenten_009
- ASL_componenten_015
- ASL_componenten_004

Dit betekent dat deze waardes niet goed passen in het 1-factor IRT model.Als bepaalde waarden niet goed passen in dit model, kan dit verschillende dingen betekenen.
Ten eerste kan het zijn dat de items niet allemaal dezelfde onderliggende eigenschap meten. Ten tweede kan het zijn dat sommige items verkeerd worden begrepen door de respondenten.
Ten derde kan het zijn dat de veronderstelling van lokale onafhankelijkheid wordt geschonden. Dit betekent dat de reacties op de items afhankelijk zijn van elkaar, zelfs na controle voor de onderliggende eigenschap. Bijvoorbeeld, als het beantwoorden van het ene item helpt bij het beantwoorden van een ander item, dan zullen deze items niet goed passen in het model.

Hieronder volgt een robuste variant van de X2 test, namelijk de PV_Q1 test. Hieronder volgt een korte uitleg wat het exact betekent:

De 'PV_Q1'-statistiek is een afgeleide versie van de Q1-statistiek, een chi-kwadraatstatistiek gebruikt in item respons theorie (IRT) modellen om te testen of een item goed past bij het model. 
Een hoge 'PV_Q1'-waarde duidt op een slechte overeenkomst tussen de waargenomen gegevens en voorspellingen van het model, wat suggereert dat het item mogelijk niet conform het verwachte gedrag 
volgens het IRT-model functioneert. Een niet-significante p-waarde, meestal p > 0.05, in combinatie met lage 'PV_Q1'- en RMSEA.PV_Q1-waarden, geeft aan dat het item goed past bij het IRT-model.
Daarnaast is het belangrijk op te merken dat de 'PV_Q1'-statistiek robuust is tegen invloeden van steekproefomvang en lokale afhankelijkheid, waardoor het een betrouwbaar instrument is om de pasvorm
van een item met het IRT-model te beoordelen, ongeacht de omvang van de steekproef of eventuele lokale onderlinge afhankelijkheden tussen de waarnemingen. Hieronder de resultaten:

```{r componenten - robust item fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(robust_item_fit)
```

Om een beter beeld te krijgen, volgen hieronder the item parameters van de vragen.

```{r componenten - item parameters, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(params)
```

Op basis van de factor loadings, blijken alle vragen goed te passen.

In IRT, meet een factor loading (ook bekend als een discriminatieparameter) hoe goed een item onderscheid maakt tussen personen met verschillende niveaus van de onderliggende eigenschap. Een hoge factor loading betekent dat het item gevoelig is voor verschillen in de onderliggende eigenschap.
Het belangrijkste verschil met factor analyse, is dat in IRT, de factor loadings ook rekening houden met de moeilijkheidsgraad van de items. Dit betekent dat items die dezelfde moeilijkheidsgraad hebben (dat wil zeggen, ze zijn even moeilijk om te beantwoorden) worden geacht dezelfde hoeveelheid informatie te geven over de onderliggende eigenschap, ongeacht hun factor loadings. Dit is niet het geval in factoranalyse, waar de factor loadings alleen de relatie tussen de geobserveerde variabelen en de onderliggende factor meten.
Tevens kan het zijn dat de factor loadings en de item parameters niet overeenkomen. Bijvoorbeeld een item kan een hoge factor loading hebben (wat aangeeft dat het goed discrimineert) maar toch een slechte fit hebben (X2-statistiek) als het bijvoorbeeld verkeerd wordt begrepen door de respondenten. Omgekeerd kan een item goed passen bij het model maar niet goed discrimineren tussen verschillende niveaus van de eigenschap.

##### Item Characteristic Curves

Hieronder bevinden zich de item characteristic curves. Hierbij een korte uitleg wat het exact betekent:

- De lijnen geven de item characteristic curves weer. Dit is de kans dat een persoon met een bepaald niveau van de onderliggende eigenschap het item correct beantwoordt. Hoe hoger de curve, hoe groter de kans dat een persoon met een bepaald niveau van de onderliggende eigenschap het item correct beantwoordt.
- De x-as van de curve vertegenwoordigt het niveau van de onderliggende eigenschap (bijvoorbeeld vaardigheid of kennis) die een persoon heeft, terwijl de y-as de kans vertegenwoordigt dat een persoon het item correct beantwoordt.
- Hoe stijler de ICC-lijn is, hoe beter het item onderscheid maakt tussen personen met verschillende niveaus van de onderliggende eigenschap. Dit betekent dat het item gevoelig is voor verschillen in de onderliggende eigenschap.
- Hoe hoger het y-axale middelpunt, hoe moeilijker het item is. 

De grafieken voor de ICC en IIC hernoemen de items op volgorde die het IRT model aanbied. Hieronder een tabel met de originele namen van de items, en de bijbehorende item nummers.

```{r componenten - ICC, echo=FALSE, message=FALSE, warning=FALSE}
params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
item_names <- extract.mirt(fit, what = "itemnames")

df <- data.frame(
  item_number = paste0("Item ", seq_along(item_names)),
  item_names = item_names,
  item_discrimination = params$items.a,
  item_difficulty = params$items.b
)
knitr::kable(df)
```


![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/componenten_ICC_combined.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/componenten_ICC_separate.png)

##### Item Information Curves

Hieronder bevinden zich de item information curves. Hierbij een korte uitleg wat het exact betekent:

- De lijnen geven de item information curves weer. Dit is de hoeveelheid informatie die een item geeft over de onderliggende eigenschap. Hoe hoger de curve, hoe meer informatie het item geeft over de onderliggende eigenschap.
- Hoe verder naar rechts de piek van de curve ligt, hoe meer informatie het item geeft over hooge niveaus van de onderliggende eigenschap. 
- Hoe verder naar links de piek van de curve ligt, hoe meer informatie het item geeft over lage niveaus van de onderliggende eigenschap.

De grafieken voor de ICC en IIC hernoemen de items op volgorde die het IRT model aanbied. Hieronder een tabel met de originele namen van de items, en de bijbehorende item nummers.

```{r componenten - IIC, echo=FALSE, message=FALSE, warning=FALSE}
params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
item_names <- extract.mirt(fit, what = "itemnames")

df <- data.frame(
  item_number = paste0("Item ", seq_along(item_names)),
  item_names = item_names,
  item_discrimination = params$items.a,
  item_difficulty = params$items.b
)
knitr::kable(df)
```


![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/componenten_IIC_combined.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/componenten_IIC_separate.png)


##### Item Person Map

Hieronder bevindt zich de item person map. Hierbij een korte uitleg wat het exact betekent:

- De x-as van de grafiek vertegenwoordigt het niveau van de onderliggende eigenschap (bijvoorbeeld vaardigheid of kennis) die een persoon heeft, terwijl de y-as de hoeveelheid participanten vertegenwoordigt.
- Onder de grafiek kan bekeken worden welke vraag ongeveer bij welke hoogte van onderliggende eigenschap past, en hoeveel leerlingen hier daadwerkelijk bij horen.

![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/componenten_item_person_fit.png)


##### Test Characteristic and Information Curves

Gelijk aan de Item characteristics en information curves, maar dan voor de test als geheel. Hierbij een korte uitleg wat het exact betekent:

- De lijnen geven de test characteristic curves weer. Deze geeft aan hoe goed de test informatie geeft over de onderliggende eigenschap. Hoe hoger de curve, hoe meer informatie de test geeft over de onderliggende eigenschap op dat niveau.
- Grofweg kunnen de lijnen en de curve gezien woren als een amalgamatie van de onderliggende item characteristic en information curves.

![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/componenten_TCC.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/componenten_TIC.png)

### figuren

```{r figuren - stat extraction, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
# load data
fit <- readRDS(paste0(here(), "/data/irt_analyses2024/irt_fit_figuren.rds"))
data <- read.csv(paste0(here(), "/data/data_interrim/figuren.csv"))

# Model fit
model_fit <- as.data.frame(M2(fit, na.rm = TRUE))
model_fit <- cbind(model_fit, nrow(data))
model_fit <- rename(model_fit, `N` = `nrow(data)`)
model_fit <- tidyr::pivot_longer(model_fit, cols = -N, names_to = "Fit Index", values_to = "Value")
model_fit <- dplyr::select(model_fit, `Fit Index`, Value)
model_fit$`Cut-Off` <- c("", "", "> .05", "< .05 | .05 - .1 | > .1", "", "", "< .08 | .08 - .10 | > .10", "> .95 | .90 - .95 | < .90", "> .95 | .90 - .95 | < .90")

# item fit
item_fit <- itemfit(fit, na.rm = TRUE)
colnames(item_fit) <- c("Item", "X2", "df", "RMSEA", "p")
item_fit <- item_fit[order(item_fit$X2), ]
rownames(item_fit) <- NULL

robust_item_fit <- itemfit(fit, na.rm = TRUE)
colnames(robust_item_fit) <- c("Item", "PV_Q1", "df", "RMSEA", "p")
robust_item_fit <- robust_item_fit[order(robust_item_fit$PV_Q1), ]
rownames(robust_item_fit) <- NULL


# Item parameters
params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
params <- params[, -which(colnames(params) %in% c("items.g", "items.u", "means", "F1"))]
sink("/dev/null")
sum <- suppressWarnings(suppressMessages(summary(fit, silent = TRUE)))
sink()
params <- cbind(params, sum$rotF)
params <- cbind(params, sum$h2)
params <- params[, c("F1", colnames(params)[-which(colnames(params) %in% "F1")])]
colnames(params) <- c("Factor Loading", "Discrimination Parameter", "Difficulty Parameter ", "Communalities")
params <- params[order(params$`Factor Loading`, decreasing = TRUE), ]


# # Differential Item Functionining DIF
# # Prepare the data
# gender <- data$gender
# gender <- dplyr::recode(gender, "Man" = 1, "Vrouw" = 2, "Onbekend" = 2)
# gender <- as.factor(gender)
# local_df <- data[, !grepl("package_duration_raw|student_number|student_name|birth_date|gender", colnames(data))]

# # Fit the multiple group model
# dif_fit <- mirt::multipleGroup(
#   data = local_df,
#   model = 1,
#   group = gender
# )

# # Perform DIF analysis
# dif <- mirt::DIF(dif_fit, group = gender, which.par = c("a1", "d"), na.rm = TRUE, plotdif = TRUE)

# Person fit
person_fit <- personfit(fit) %>%
  reframe(
    infit.outside = prop.table(table(z.infit > 1.96 | z.infit < -1.96)),
    outfit.outside = prop.table(table(z.outfit > 1.96 | z.outfit < -1.96))
  ) # lower row = non-fitting people
person_fit$Correct <- c("Correct", "Incorrect")
person_fit <- person_fit %>% dplyr::select(Correct, everything())
```

#### Model Fit

Hieronder bevinden zich de verschillende model fit indices.Hierbij een korte uitleg wat het exact betekent:

- M2: Dit is de M2-statistiek van Maydeu-Olivares, die een maat is voor de absolute fit van het model. Een lager getal duidt op een betere fit. Er is geen vaste cut-off waarde, maar over het algemeen wordt een lagere waarde als beter beschouwd. Als de bijbehorende p-waarde kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- df: de degrees of freedom van het model. Dit is het aantal parameters dat is geschat, minus het aantal constraints dat is opgelegd.
- p-value: de p-waarde van de Goodness of fit test. Als deze kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- RMSEA: RMSEA meet hoe goed een model de waargenomen data benadert, rekening houdend met de fout in het model. Lagere waarden duiden op een betere pasvorm, typisch onder 0.05 voor goede pasvorm en tussen 0.05 en 0.08 voor redelijke pasvorm.
  - RMSEA_5 en RMSEA_95: de 95% betrouwbaarheidsinterval van de RMSEA. Hoe kleiner dit interval rondom de RMSEA waarde, hoe zekerder we zijn van die waarde. 
- SRMSR: SRMSR staat voor "Standardized Root Mean Square Residual". Het is een maat voor de gemiddelde discrepantie tussen de waargenomen en voorspelde covarianties. Een lagere waarde duidt op een betere fit. Een SRMSR van minder dan 0.08 wordt over het algemeen als goed beschouwd.
- TLI: de Tucker-Lewis Index. TLI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.
- CFI: de Comparative Fit Index. De CFI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.
- N: het aantal respondenten in de steekproef.

```{r figuren - model fit, echo=FALSE, message=FALSE, warning=FALSE}
model_fit$`Conclusion` <- c("", "", "Poor", "Good", "", "", "Good", "Good", "Good")
knitr::kable(model_fit)
```

Als we kijken naar de fit indices, zien dat figuren een goeie fit heeft bereikt in het 1-factor-IRT model. Alle fit indices zijn als goed bevonden, met uitzondering van de p-waarde.
Gezien de hoge hoeveelheid observaties (N = `r nrow(data)`), kan de p-waarde als onbetrouwbaar zijn, gezien diens gevoeligheid voor sample size. Daarom wordt de p-waarde genegeerd, en wordt er geconcludeerd dat het model goed past.

#### Person Fit

Hieronder zijn de percentages van person-item-fit indices weergegeven. Hierbij een korte uitleg wat het exact betekent:
- infit.outside: De infit (information-weighted fit) statistiek is gevoelig voor onverwacht gedrag dat invloed heeft op de antwoorden op items die de meeste informatie geven over het vaardigheidsniveau van de persoon. Met andere woorden, het is gevoeliger voor onverwachte antwoorden op items die dicht bij het vaardigheidsniveau van de persoon liggen. De infit.outside waarde in de tabel vertegenwoordigt de proportie van personen wiens infit statistiek buiten het acceptabele bereik valt, wat duidt op een slechte fit.
- outfit.outside: De outfit (outlier-sensitive fit) statistiek is gevoelig voor onverwacht gedrag dat invloed heeft op de antwoorden op items die de minste informatie geven over het vaardigheidsniveau van de persoon. Met andere woorden, het is gevoeliger voor onverwachte antwoorden op items die ver van het vaardigheidsniveau van de persoon liggen. De outfit.outside waarde in de tabel vertegenwoordigt de proportie van personen wiens outfit statistiek buiten het acceptabele bereik valt, wat duidt op een slechte fit.

```{r figuren - person fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(person_fit)
```

Gezien zowel de infit en outfit statistieken rond de 95 procent vallen, is er geen reden om aan te nemen dat de personen niet goed passen in het model. De proportie van personen wiens infit en outfit statistieken buiten het acceptabele bereik vallen, is verwaarloosbaar klein.

#### Item Fit

Hieronder bevinden zich de verschillende item fit indices. Hierbij een korte uitleg wat het exact betekent:

- X2: Dit is de X2-statistiek van de item fit test. Een lager getal duidt op een betere fit. Er is geen vaste cut-off waarde, maar over het algemeen wordt een lagere waarde als beter beschouwd. Als de bijbehorende p-waarde kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het item niet goed past.
- df: de degrees of freedom van het model. Dit is het aantal parameters dat is geschat, minus het aantal constraints dat is opgelegd.
- p-value: de p-waarde van de Goodness of fit test. Als deze kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- RMSEA: RMSEA meet hoe goed een model de waargenomen data benadert, rekening houdend met de fout in het model. Lagere waarden duiden op een betere pasvorm, typisch onder 0.05 voor goede pasvorm en tussen 0.05 en 0.08 voor redelijke pasvorm.
 
```{r figuren - item fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(item_fit)
```

Op basis van de resultaten, blijken 5 vragen niet goed te passen:

- ASL_figuren_010
- ASL_figuren_018
- ASL_figuren_016
- ASL_figuren_009
- ASL_figuren_012

Dit betekent dat deze waardes niet goed passen in het 1-factor IRT model.Als bepaalde waarden niet goed passen in dit model, kan dit verschillende dingen betekenen.
Ten eerste kan het zijn dat de items niet allemaal dezelfde onderliggende eigenschap meten. Ten tweede kan het zijn dat sommige items verkeerd worden begrepen door de respondenten.
Ten derde kan het zijn dat de veronderstelling van lokale onafhankelijkheid wordt geschonden. Dit betekent dat de reacties op de items afhankelijk zijn van elkaar, zelfs na controle voor de onderliggende eigenschap. Bijvoorbeeld, als het beantwoorden van het ene item helpt bij het beantwoorden van een ander item, dan zullen deze items niet goed passen in het model.

Hieronder volgt een robuste variant van de X2 test, namelijk de PV_Q1 test. Hieronder volgt een korte uitleg wat het exact betekent:

De 'PV_Q1'-statistiek is een afgeleide versie van de Q1-statistiek, een chi-kwadraatstatistiek gebruikt in item respons theorie (IRT) modellen om te testen of een item goed past bij het model. 
Een hoge 'PV_Q1'-waarde duidt op een slechte overeenkomst tussen de waargenomen gegevens en voorspellingen van het model, wat suggereert dat het item mogelijk niet conform het verwachte gedrag 
volgens het IRT-model functioneert. Een niet-significante p-waarde, meestal p > 0.05, in combinatie met lage 'PV_Q1'- en RMSEA.PV_Q1-waarden, geeft aan dat het item goed past bij het IRT-model.
Daarnaast is het belangrijk op te merken dat de 'PV_Q1'-statistiek robuust is tegen invloeden van steekproefomvang en lokale afhankelijkheid, waardoor het een betrouwbaar instrument is om de pasvorm
van een item met het IRT-model te beoordelen, ongeacht de omvang van de steekproef of eventuele lokale onderlinge afhankelijkheden tussen de waarnemingen. Hieronder de resultaten:

```{r figuren - robust item fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(robust_item_fit)
```

Om een beter beeld te krijgen, volgen hieronder the item parameters van de vragen.

```{r figuren - item parameters, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(params)
```

Op basis van de factor loadings, blijken alle vragen goed te passen.

In IRT, meet een factor loading (ook bekend als een discriminatieparameter) hoe goed een item onderscheid maakt tussen personen met verschillende niveaus van de onderliggende eigenschap. Een hoge factor loading betekent dat het item gevoelig is voor verschillen in de onderliggende eigenschap.
Het belangrijkste verschil met factor analyse, is dat in IRT, de factor loadings ook rekening houden met de moeilijkheidsgraad van de items. Dit betekent dat items die dezelfde moeilijkheidsgraad hebben (dat wil zeggen, ze zijn even moeilijk om te beantwoorden) worden geacht dezelfde hoeveelheid informatie te geven over de onderliggende eigenschap, ongeacht hun factor loadings. Dit is niet het geval in factoranalyse, waar de factor loadings alleen de relatie tussen de geobserveerde variabelen en de onderliggende factor meten.
Tevens kan het zijn dat de factor loadings en de item parameters niet overeenkomen. Bijvoorbeeld een item kan een hoge factor loading hebben (wat aangeeft dat het goed discrimineert) maar toch een slechte fit hebben (X2-statistiek) als het bijvoorbeeld verkeerd wordt begrepen door de respondenten. Omgekeerd kan een item goed passen bij het model maar niet goed discrimineren tussen verschillende niveaus van de eigenschap.

##### Item Characteristic Curves

Hieronder bevinden zich de item characteristic curves. Hierbij een korte uitleg wat het exact betekent:

- De lijnen geven de item characteristic curves weer. Dit is de kans dat een persoon met een bepaald niveau van de onderliggende eigenschap het item correct beantwoordt. Hoe hoger de curve, hoe groter de kans dat een persoon met een bepaald niveau van de onderliggende eigenschap het item correct beantwoordt.
- De x-as van de curve vertegenwoordigt het niveau van de onderliggende eigenschap (bijvoorbeeld vaardigheid of kennis) die een persoon heeft, terwijl de y-as de kans vertegenwoordigt dat een persoon het item correct beantwoordt.
- Hoe stijler de ICC-lijn is, hoe beter het item onderscheid maakt tussen personen met verschillende niveaus van de onderliggende eigenschap. Dit betekent dat het item gevoelig is voor verschillen in de onderliggende eigenschap.
- Hoe hoger het y-axale middelpunt, hoe moeilijker het item is. 

De grafieken voor de ICC en IIC hernoemen de items op volgorde die het IRT model aanbied. Hieronder een tabel met de originele namen van de items, en de bijbehorende item nummers.

```{r figuren - ICC, echo=FALSE, message=FALSE, warning=FALSE}
params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
item_names <- extract.mirt(fit, what = "itemnames")

df <- data.frame(
  item_number = paste0("Item ", seq_along(item_names)),
  item_names = item_names,
  item_discrimination = params$items.a,
  item_difficulty = params$items.b
)
knitr::kable(df)
```


![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/figuren_ICC_combined.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/figuren_ICC_separate.png)

##### Item Information Curves

Hieronder bevinden zich de item information curves. Hierbij een korte uitleg wat het exact betekent:

- De lijnen geven de item information curves weer. Dit is de hoeveelheid informatie die een item geeft over de onderliggende eigenschap. Hoe hoger de curve, hoe meer informatie het item geeft over de onderliggende eigenschap.
- Hoe verder naar rechts de piek van de curve ligt, hoe meer informatie het item geeft over hooge niveaus van de onderliggende eigenschap. 
- Hoe verder naar links de piek van de curve ligt, hoe meer informatie het item geeft over lage niveaus van de onderliggende eigenschap.

De grafieken voor de ICC en IIC hernoemen de items op volgorde die het IRT model aanbied. Hieronder een tabel met de originele namen van de items, en de bijbehorende item nummers.

```{r figuren - IIC, echo=FALSE, message=FALSE, warning=FALSE}
params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
item_names <- extract.mirt(fit, what = "itemnames")

df <- data.frame(
  item_number = paste0("Item ", seq_along(item_names)),
  item_names = item_names,
  item_discrimination = params$items.a,
  item_difficulty = params$items.b
)
knitr::kable(df)
```


![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/figuren_IIC_combined.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/figuren_IIC_separate.png)


##### Item Person Map

Hieronder bevindt zich de item person map. Hierbij een korte uitleg wat het exact betekent:

- De x-as van de grafiek vertegenwoordigt het niveau van de onderliggende eigenschap (bijvoorbeeld vaardigheid of kennis) die een persoon heeft, terwijl de y-as de hoeveelheid participanten vertegenwoordigt.
- Onder de grafiek kan bekeken worden welke vraag ongeveer bij welke hoogte van onderliggende eigenschap past, en hoeveel leerlingen hier daadwerkelijk bij horen.

![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/figuren_item_person_fit.png)


##### Test Characteristic and Information Curves

Gelijk aan de Item characteristics en information curves, maar dan voor de test als geheel. Hierbij een korte uitleg wat het exact betekent:

- De lijnen geven de test characteristic curves weer. Deze geeft aan hoe goed de test informatie geeft over de onderliggende eigenschap. Hoe hoger de curve, hoe meer informatie de test geeft over de onderliggende eigenschap op dat niveau.
- Grofweg kunnen de lijnen en de curve gezien woren als een amalgamatie van de onderliggende item characteristic en information curves.

![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/figuren_TCC.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/figuren_TIC.png)

### getallen

```{r getallen - stat extraction, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
# load data
fit <- readRDS(paste0(here(), "/data/irt_analyses2024/irt_fit_getallen.rds"))
data <- read.csv(paste0(here(), "/data/data_interrim/getallen.csv"))

# Model fit
model_fit <- as.data.frame(M2(fit, na.rm = TRUE))
model_fit <- cbind(model_fit, nrow(data))
model_fit <- rename(model_fit, `N` = `nrow(data)`)
model_fit <- tidyr::pivot_longer(model_fit, cols = -N, names_to = "Fit Index", values_to = "Value")
model_fit <- dplyr::select(model_fit, `Fit Index`, Value)
model_fit$`Cut-Off` <- c("", "", "> .05", "< .05 | .05 - .1 | > .1", "", "", "< .08 | .08 - .10 | > .10", "> .95 | .90 - .95 | < .90", "> .95 | .90 - .95 | < .90")

# item fit
item_fit <- itemfit(fit, na.rm = TRUE)
colnames(item_fit) <- c("Item", "X2", "df", "RMSEA", "p")
item_fit <- item_fit[order(item_fit$X2), ]
rownames(item_fit) <- NULL

robust_item_fit <- itemfit(fit, na.rm = TRUE)
colnames(robust_item_fit) <- c("Item", "PV_Q1", "df", "RMSEA", "p")
robust_item_fit <- robust_item_fit[order(robust_item_fit$PV_Q1), ]
rownames(robust_item_fit) <- NULL


# Item parameters
params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
params <- params[, -which(colnames(params) %in% c("items.g", "items.u", "means", "F1"))]
sink("/dev/null")
sum <- suppressWarnings(suppressMessages(summary(fit, silent = TRUE)))
sink()
params <- cbind(params, sum$rotF)
params <- cbind(params, sum$h2)
params <- params[, c("F1", colnames(params)[-which(colnames(params) %in% "F1")])]
colnames(params) <- c("Factor Loading", "Discrimination Parameter", "Difficulty Parameter ", "Communalities")
params <- params[order(params$`Factor Loading`, decreasing = TRUE), ]


# # Differential Item Functionining DIF
# # Prepare the data
# gender <- data$gender
# gender <- dplyr::recode(gender, "Man" = 1, "Vrouw" = 2, "Onbekend" = 2)
# gender <- as.factor(gender)
# local_df <- data[, !grepl("package_duration_raw|student_number|student_name|birth_date|gender", colnames(data))]

# # Fit the multiple group model
# dif_fit <- mirt::multipleGroup(
#   data = local_df,
#   model = 1,
#   group = gender
# )

# # Perform DIF analysis
# dif <- mirt::DIF(dif_fit, group = gender, which.par = c("a1", "d"), na.rm = TRUE, plotdif = TRUE)

# Person fit
person_fit <- personfit(fit) %>%
  reframe(
    infit.outside = prop.table(table(z.infit > 1.96 | z.infit < -1.96)),
    outfit.outside = prop.table(table(z.outfit > 1.96 | z.outfit < -1.96))
  ) # lower row = non-fitting people
person_fit$Correct <- c("Correct", "Incorrect")
person_fit <- person_fit %>% dplyr::select(Correct, everything())
```

#### Model Fit

Hieronder bevinden zich de verschillende model fit indices.Hierbij een korte uitleg wat het exact betekent:

- M2: Dit is de M2-statistiek van Maydeu-Olivares, die een maat is voor de absolute fit van het model. Een lager getal duidt op een betere fit. Er is geen vaste cut-off waarde, maar over het algemeen wordt een lagere waarde als beter beschouwd. Als de bijbehorende p-waarde kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- df: de degrees of freedom van het model. Dit is het aantal parameters dat is geschat, minus het aantal constraints dat is opgelegd.
- p-value: de p-waarde van de Goodness of fit test. Als deze kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- RMSEA: RMSEA meet hoe goed een model de waargenomen data benadert, rekening houdend met de fout in het model. Lagere waarden duiden op een betere pasvorm, typisch onder 0.05 voor goede pasvorm en tussen 0.05 en 0.08 voor redelijke pasvorm.
  - RMSEA_5 en RMSEA_95: de 95% betrouwbaarheidsinterval van de RMSEA. Hoe kleiner dit interval rondom de RMSEA waarde, hoe zekerder we zijn van die waarde. 
- SRMSR: SRMSR staat voor "Standardized Root Mean Square Residual". Het is een maat voor de gemiddelde discrepantie tussen de waargenomen en voorspelde covarianties. Een lagere waarde duidt op een betere fit. Een SRMSR van minder dan 0.08 wordt over het algemeen als goed beschouwd.
- TLI: de Tucker-Lewis Index. TLI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.
- CFI: de Comparative Fit Index. De CFI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.
- N: het aantal respondenten in de steekproef.

```{r getallen - model fit, echo=FALSE, message=FALSE, warning=FALSE}
model_fit$`Conclusion` <- c("", "", "Poor", "Good", "", "", "Good", "Good", "Good")
knitr::kable(model_fit)
```

Als we kijken naar de fit indices, zien dat getallen een goeie fit heeft bereikt in het 1-factor-IRT model. Alle fit indices zijn als goed bevonden, met uitzondering van de p-waarde.
Gezien de hoge hoeveelheid observaties (N = `r nrow(data)`), kan de p-waarde als onbetrouwbaar zijn, gezien diens gevoeligheid voor sample size. Daarom wordt de p-waarde genegeerd, en wordt er geconcludeerd dat het model goed past.

#### Person Fit

Hieronder zijn de percentages van person-item-fit indices weergegeven. Hierbij een korte uitleg wat het exact betekent:
- infit.outside: De infit (information-weighted fit) statistiek is gevoelig voor onverwacht gedrag dat invloed heeft op de antwoorden op items die de meeste informatie geven over het vaardigheidsniveau van de persoon. Met andere woorden, het is gevoeliger voor onverwachte antwoorden op items die dicht bij het vaardigheidsniveau van de persoon liggen. De infit.outside waarde in de tabel vertegenwoordigt de proportie van personen wiens infit statistiek buiten het acceptabele bereik valt, wat duidt op een slechte fit.
- outfit.outside: De outfit (outlier-sensitive fit) statistiek is gevoelig voor onverwacht gedrag dat invloed heeft op de antwoorden op items die de minste informatie geven over het vaardigheidsniveau van de persoon. Met andere woorden, het is gevoeliger voor onverwachte antwoorden op items die ver van het vaardigheidsniveau van de persoon liggen. De outfit.outside waarde in de tabel vertegenwoordigt de proportie van personen wiens outfit statistiek buiten het acceptabele bereik valt, wat duidt op een slechte fit.

```{r getallen - person fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(person_fit)
```

Gezien zowel de infit en outfit statistieken rond de 95 procent vallen, is er geen reden om aan te nemen dat de personen niet goed passen in het model. De proportie van personen wiens infit statistiek buiten het acceptabele bereik valt, wat groter te zijn (~6%).

Hieronder in de grafiek is te zien welke vragen in hoeverre bijdragen aan de infit en outfit waardes:

![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/getallen_item_fit.png)

#### Item Fit

Hieronder bevinden zich de verschillende item fit indices. Hierbij een korte uitleg wat het exact betekent:

- X2: Dit is de X2-statistiek van de item fit test. Een lager getal duidt op een betere fit. Er is geen vaste cut-off waarde, maar over het algemeen wordt een lagere waarde als beter beschouwd. Als de bijbehorende p-waarde kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het item niet goed past.
- df: de degrees of freedom van het model. Dit is het aantal parameters dat is geschat, minus het aantal constraints dat is opgelegd.
- p-value: de p-waarde van de Goodness of fit test. Als deze kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- RMSEA: RMSEA meet hoe goed een model de waargenomen data benadert, rekening houdend met de fout in het model. Lagere waarden duiden op een betere pasvorm, typisch onder 0.05 voor goede pasvorm en tussen 0.05 en 0.08 voor redelijke pasvorm.
 
```{r getallen - item fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(item_fit)
```

Op basis van de resultaten, blijken meerdere vragen niet goed te passen (zie tabel: waardes meg een p lager dan .05).

Dit betekent dat deze waardes niet goed passen in het 1-factor IRT model.Als bepaalde waarden niet goed passen in dit model, kan dit verschillende dingen betekenen.
Ten eerste kan het zijn dat de items niet allemaal dezelfde onderliggende eigenschap meten. Ten tweede kan het zijn dat sommige items verkeerd worden begrepen door de respondenten.
Ten derde kan het zijn dat de veronderstelling van lokale onafhankelijkheid wordt geschonden. Dit betekent dat de reacties op de items afhankelijk zijn van elkaar, zelfs na controle voor de onderliggende eigenschap. Bijvoorbeeld, als het beantwoorden van het ene item helpt bij het beantwoorden van een ander item, dan zullen deze items niet goed passen in het model.

Hieronder volgt een robuste variant van de X2 test, namelijk de PV_Q1 test. Hieronder volgt een korte uitleg wat het exact betekent:

De 'PV_Q1'-statistiek is een afgeleide versie van de Q1-statistiek, een chi-kwadraatstatistiek gebruikt in item respons theorie (IRT) modellen om te testen of een item goed past bij het model. 
Een hoge 'PV_Q1'-waarde duidt op een slechte overeenkomst tussen de waargenomen gegevens en voorspellingen van het model, wat suggereert dat het item mogelijk niet conform het verwachte gedrag 
volgens het IRT-model functioneert. Een niet-significante p-waarde, meestal p > 0.05, in combinatie met lage 'PV_Q1'- en RMSEA.PV_Q1-waarden, geeft aan dat het item goed past bij het IRT-model.
Daarnaast is het belangrijk op te merken dat de 'PV_Q1'-statistiek robuust is tegen invloeden van steekproefomvang en lokale afhankelijkheid, waardoor het een betrouwbaar instrument is om de pasvorm
van een item met het IRT-model te beoordelen, ongeacht de omvang van de steekproef of eventuele lokale onderlinge afhankelijkheden tussen de waarnemingen. Hieronder de resultaten:

```{r getallen - robust item fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(robust_item_fit)
```

Om een beter beeld te krijgen, volgen hieronder the item parameters van de vragen.

```{r getallen - item parameters, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(params)
```

Op basis van de factor loadings, blijken de volgende vragen niet goed te passen:

- ASL7_rekenen_GT_A_2F_043_pilot: 0.2753362
- ASL7_rekenen_GT_C_1S_040_pilot: 0.0435946

In IRT, meet een factor loading (ook bekend als een discriminatieparameter) hoe goed een item onderscheid maakt tussen personen met verschillende niveaus van de onderliggende eigenschap. Een hoge factor loading betekent dat het item gevoelig is voor verschillen in de onderliggende eigenschap.
Het belangrijkste verschil met factor analyse, is dat in IRT, de factor loadings ook rekening houden met de moeilijkheidsgraad van de items. Dit betekent dat items die dezelfde moeilijkheidsgraad hebben (dat wil zeggen, ze zijn even moeilijk om te beantwoorden) worden geacht dezelfde hoeveelheid informatie te geven over de onderliggende eigenschap, ongeacht hun factor loadings. Dit is niet het geval in factoranalyse, waar de factor loadings alleen de relatie tussen de geobserveerde variabelen en de onderliggende factor meten.
Tevens kan het zijn dat de factor loadings en de item parameters niet overeenkomen. Bijvoorbeeld een item kan een hoge factor loading hebben (wat aangeeft dat het goed discrimineert) maar toch een slechte fit hebben (X2-statistiek) als het bijvoorbeeld verkeerd wordt begrepen door de respondenten. Omgekeerd kan een item goed passen bij het model maar niet goed discrimineren tussen verschillende niveaus van de eigenschap.

##### Item Characteristic Curves

Hieronder bevinden zich de item characteristic curves. Hierbij een korte uitleg wat het exact betekent:

- De lijnen geven de item characteristic curves weer. Dit is de kans dat een persoon met een bepaald niveau van de onderliggende eigenschap het item correct beantwoordt. Hoe hoger de curve, hoe groter de kans dat een persoon met een bepaald niveau van de onderliggende eigenschap het item correct beantwoordt.
- De x-as van de curve vertegenwoordigt het niveau van de onderliggende eigenschap (bijvoorbeeld vaardigheid of kennis) die een persoon heeft, terwijl de y-as de kans vertegenwoordigt dat een persoon het item correct beantwoordt.
- Hoe stijler de ICC-lijn is, hoe beter het item onderscheid maakt tussen personen met verschillende niveaus van de onderliggende eigenschap. Dit betekent dat het item gevoelig is voor verschillen in de onderliggende eigenschap.
- Hoe hoger het y-axale middelpunt, hoe moeilijker het item is. 

De grafieken voor de ICC en IIC hernoemen de items op volgorde die het IRT model aanbied. Hieronder een tabel met de originele namen van de items, en de bijbehorende item nummers.

```{r getallen - ICC, echo=FALSE, message=FALSE, warning=FALSE}
params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
item_names <- extract.mirt(fit, what = "itemnames")

df <- data.frame(
  item_number = paste0("Item ", seq_along(item_names)),
  item_names = item_names,
  item_discrimination = params$items.a,
  item_difficulty = params$items.b
)
knitr::kable(df)
```


![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/getallen_ICC_combined.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/getallen_ICC_separate.png)

##### Item Information Curves

Hieronder bevinden zich de item information curves. Hierbij een korte uitleg wat het exact betekent:

- De lijnen geven de item information curves weer. Dit is de hoeveelheid informatie die een item geeft over de onderliggende eigenschap. Hoe hoger de curve, hoe meer informatie het item geeft over de onderliggende eigenschap.
- Hoe verder naar rechts de piek van de curve ligt, hoe meer informatie het item geeft over hooge niveaus van de onderliggende eigenschap. 
- Hoe verder naar links de piek van de curve ligt, hoe meer informatie het item geeft over lage niveaus van de onderliggende eigenschap.

De grafieken voor de ICC en IIC hernoemen de items op volgorde die het IRT model aanbied. Hieronder een tabel met de originele namen van de items, en de bijbehorende item nummers.

```{r getallen - IIC, echo=FALSE, message=FALSE, warning=FALSE}
params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
item_names <- extract.mirt(fit, what = "itemnames")

df <- data.frame(
  item_number = paste0("Item ", seq_along(item_names)),
  item_names = item_names,
  item_discrimination = params$items.a,
  item_difficulty = params$items.b
)
knitr::kable(df)
```


![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/getallen_IIC_combined.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/getallen_IIC_separate.png)


##### Item Person Map

Hieronder bevindt zich de item person map. Hierbij een korte uitleg wat het exact betekent:

- De x-as van de grafiek vertegenwoordigt het niveau van de onderliggende eigenschap (bijvoorbeeld vaardigheid of kennis) die een persoon heeft, terwijl de y-as de hoeveelheid participanten vertegenwoordigt.
- Onder de grafiek kan bekeken worden welke vraag ongeveer bij welke hoogte van onderliggende eigenschap past, en hoeveel leerlingen hier daadwerkelijk bij horen.

![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/getallen_item_person_fit.png)


##### Test Characteristic and Information Curves

Gelijk aan de Item characteristics en information curves, maar dan voor de test als geheel. Hierbij een korte uitleg wat het exact betekent:

- De lijnen geven de test characteristic curves weer. Deze geeft aan hoe goed de test informatie geeft over de onderliggende eigenschap. Hoe hoger de curve, hoe meer informatie de test geeft over de onderliggende eigenschap op dat niveau.
- Grofweg kunnen de lijnen en de curve gezien woren als een amalgamatie van de onderliggende item characteristic en information curves.

![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/getallen_TCC.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/getallen_TIC.png)

### Leestekens

```{r leestekens - stat extraction, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
# load data
fit <- readRDS(paste0(here(), "/data/irt_analyses2024/irt_fit_leestekens.rds"))
data <- read.csv(paste0(here(), "/data/data_interrim/leestekens.csv"))

# Model fit
model_fit <- as.data.frame(M2(fit, na.rm = TRUE))
model_fit <- cbind(model_fit, nrow(data))
model_fit <- rename(model_fit, `N` = `nrow(data)`)
model_fit <- tidyr::pivot_longer(model_fit, cols = -N, names_to = "Fit Index", values_to = "Value")
model_fit <- dplyr::select(model_fit, `Fit Index`, Value)
model_fit$`Cut-Off` <- c("", "", "> .05", "< .05 | .05 - .1 | > .1", "", "", "< .08 | .08 - .10 | > .10", "> .95 | .90 - .95 | < .90", "> .95 | .90 - .95 | < .90")

# item fit
item_fit <- itemfit(fit, na.rm = TRUE)
colnames(item_fit) <- c("Item", "X2", "df", "RMSEA", "p")
item_fit <- item_fit[order(item_fit$X2), ]
rownames(item_fit) <- NULL

robust_item_fit <- itemfit(fit, na.rm = TRUE)
colnames(robust_item_fit) <- c("Item", "PV_Q1", "df", "RMSEA", "p")
robust_item_fit <- robust_item_fit[order(robust_item_fit$PV_Q1), ]
rownames(robust_item_fit) <- NULL


# Item parameters
params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
params <- params[, -which(colnames(params) %in% c("items.g", "items.u", "means", "F1"))]
sink("/dev/null")
sum <- suppressWarnings(suppressMessages(summary(fit, silent = TRUE)))
sink()
params <- cbind(params, sum$rotF)
params <- cbind(params, sum$h2)
params <- params[, c("F1", colnames(params)[-which(colnames(params) %in% "F1")])]
colnames(params) <- c("Factor Loading", "Discrimination Parameter", "Difficulty Parameter ", "Communalities")
params <- params[order(params$`Factor Loading`, decreasing = TRUE), ]


# # Differential Item Functionining DIF
# # Prepare the data
# gender <- data$gender
# gender <- dplyr::recode(gender, "Man" = 1, "Vrouw" = 2, "Onbekend" = 2)
# gender <- as.factor(gender)
# local_df <- data[, !grepl("package_duration_raw|student_number|student_name|birth_date|gender", colnames(data))]

# # Fit the multiple group model
# dif_fit <- mirt::multipleGroup(
#   data = local_df,
#   model = 1,
#   group = gender
# )

# # Perform DIF analysis
# dif <- mirt::DIF(dif_fit, group = gender, which.par = c("a1", "d"), na.rm = TRUE, plotdif = TRUE)

# Person fit
person_fit <- personfit(fit) %>%
  reframe(
    infit.outside = prop.table(table(z.infit > 1.96 | z.infit < -1.96)),
    outfit.outside = prop.table(table(z.outfit > 1.96 | z.outfit < -1.96))
  ) # lower row = non-fitting people
person_fit$Correct <- c("Correct", "Incorrect")
person_fit <- person_fit %>% dplyr::select(Correct, everything())
```

#### Model Fit

Hieronder bevinden zich de verschillende model fit indices.Hierbij een korte uitleg wat het exact betekent:

- M2: Dit is de M2-statistiek van Maydeu-Olivares, die een maat is voor de absolute fit van het model. Een lager getal duidt op een betere fit. Er is geen vaste cut-off waarde, maar over het algemeen wordt een lagere waarde als beter beschouwd. Als de bijbehorende p-waarde kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- df: de degrees of freedom van het model. Dit is het aantal parameters dat is geschat, minus het aantal constraints dat is opgelegd.
- p-value: de p-waarde van de Goodness of fit test. Als deze kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- RMSEA: RMSEA meet hoe goed een model de waargenomen data benadert, rekening houdend met de fout in het model. Lagere waarden duiden op een betere pasvorm, typisch onder 0.05 voor goede pasvorm en tussen 0.05 en 0.08 voor redelijke pasvorm.
  - RMSEA_5 en RMSEA_95: de 95% betrouwbaarheidsinterval van de RMSEA. Hoe kleiner dit interval rondom de RMSEA waarde, hoe zekerder we zijn van die waarde. 
- SRMSR: SRMSR staat voor "Standardized Root Mean Square Residual". Het is een maat voor de gemiddelde discrepantie tussen de waargenomen en voorspelde covarianties. Een lagere waarde duidt op een betere fit. Een SRMSR van minder dan 0.08 wordt over het algemeen als goed beschouwd.
- TLI: de Tucker-Lewis Index. TLI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.
- CFI: de Comparative Fit Index. De CFI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.
- N: het aantal respondenten in de steekproef.

```{r leestekens - model fit, echo=FALSE, message=FALSE, warning=FALSE}
model_fit$`Conclusion` <- c("", "", "Poor", "Good", "", "", "Good", "Poor", "Acceptable")
knitr::kable(model_fit)
```

Als we kijken naar de fit indices, zien dat leestekens een minder goeie fit heeft. Zo zijn de TLI, en CFI, slecht en acceptabel respectievelijk.
Gezien de hoge hoeveelheid observaties (N = `r nrow(data)`), kan de p-waarde als onbetrouwbaar zijn, gezien diens gevoeligheid voor sample size. Daarom wordt de p-waarde genegeerd, en wordt er geconcludeerd dat het model goed past.

#### Person Fit

Hieronder zijn de percentages van person-item-fit indices weergegeven. Hierbij een korte uitleg wat het exact betekent:
- infit.outside: De infit (information-weighted fit) statistiek is gevoelig voor onverwacht gedrag dat invloed heeft op de antwoorden op items die de meeste informatie geven over het vaardigheidsniveau van de persoon. Met andere woorden, het is gevoeliger voor onverwachte antwoorden op items die dicht bij het vaardigheidsniveau van de persoon liggen. De infit.outside waarde in de tabel vertegenwoordigt de proportie van personen wiens infit statistiek buiten het acceptabele bereik valt, wat duidt op een slechte fit.
- outfit.outside: De outfit (outlier-sensitive fit) statistiek is gevoelig voor onverwacht gedrag dat invloed heeft op de antwoorden op items die de minste informatie geven over het vaardigheidsniveau van de persoon. Met andere woorden, het is gevoeliger voor onverwachte antwoorden op items die ver van het vaardigheidsniveau van de persoon liggen. De outfit.outside waarde in de tabel vertegenwoordigt de proportie van personen wiens outfit statistiek buiten het acceptabele bereik valt, wat duidt op een slechte fit.

```{r leestekens - person fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(person_fit)
```

Gezien zowel de infit en outfit statistieken rond de 95 procent vallen, is er geen reden om aan te nemen dat de personen niet goed passen in het model. De proportie van personen wiens infit statistiek buiten het acceptabele bereik valt, wat groter te zijn (~6%).

Hieronder in de grafiek is te zien welke vragen in hoeverre bijdragen aan de infit en outfit waardes:

![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/leestekens_item_fit.png)

#### Item Fit

Hieronder bevinden zich de verschillende item fit indices. Hierbij een korte uitleg wat het exact betekent:

- X2: Dit is de X2-statistiek van de item fit test. Een lager getal duidt op een betere fit. Er is geen vaste cut-off waarde, maar over het algemeen wordt een lagere waarde als beter beschouwd. Als de bijbehorende p-waarde kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het item niet goed past.
- df: de degrees of freedom van het model. Dit is het aantal parameters dat is geschat, minus het aantal constraints dat is opgelegd.
- p-value: de p-waarde van de Goodness of fit test. Als deze kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- RMSEA: RMSEA meet hoe goed een model de waargenomen data benadert, rekening houdend met de fout in het model. Lagere waarden duiden op een betere pasvorm, typisch onder 0.05 voor goede pasvorm en tussen 0.05 en 0.08 voor redelijke pasvorm.
 
```{r leestekens - item fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(item_fit)
```

Op basis van de resultaten, blijken meerdere vragen niet goed te passen (zie tabel: vragen met een p lager dan .05).

Dit betekent dat deze waardes niet goed passen in het 1-factor IRT model.Als bepaalde waarden niet goed passen in dit model, kan dit verschillende dingen betekenen.
Ten eerste kan het zijn dat de items niet allemaal dezelfde onderliggende eigenschap meten. Ten tweede kan het zijn dat sommige items verkeerd worden begrepen door de respondenten.
Ten derde kan het zijn dat de veronderstelling van lokale onafhankelijkheid wordt geschonden. Dit betekent dat de reacties op de items afhankelijk zijn van elkaar, zelfs na controle voor de onderliggende eigenschap. Bijvoorbeeld, als het beantwoorden van het ene item helpt bij het beantwoorden van een ander item, dan zullen deze items niet goed passen in het model.

Hieronder volgt een robuste variant van de X2 test, namelijk de PV_Q1 test. Hieronder volgt een korte uitleg wat het exact betekent:

De 'PV_Q1'-statistiek is een afgeleide versie van de Q1-statistiek, een chi-kwadraatstatistiek gebruikt in item respons theorie (IRT) modellen om te testen of een item goed past bij het model. 
Een hoge 'PV_Q1'-waarde duidt op een slechte overeenkomst tussen de waargenomen gegevens en voorspellingen van het model, wat suggereert dat het item mogelijk niet conform het verwachte gedrag 
volgens het IRT-model functioneert. Een niet-significante p-waarde, meestal p > 0.05, in combinatie met lage 'PV_Q1'- en RMSEA.PV_Q1-waarden, geeft aan dat het item goed past bij het IRT-model.
Daarnaast is het belangrijk op te merken dat de 'PV_Q1'-statistiek robuust is tegen invloeden van steekproefomvang en lokale afhankelijkheid, waardoor het een betrouwbaar instrument is om de pasvorm
van een item met het IRT-model te beoordelen, ongeacht de omvang van de steekproef of eventuele lokale onderlinge afhankelijkheden tussen de waarnemingen. Hieronder de resultaten:

```{r leestekens - robust item fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(robust_item_fit)
```

Om een beter beeld te krijgen, volgen hieronder the item parameters van de vragen.

```{r leestekens - item parameters, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(params)
```

Op basis van de factor loadings, blijken alle vragen goed te passen.

In IRT, meet een factor loading (ook bekend als een discriminatieparameter) hoe goed een item onderscheid maakt tussen personen met verschillende niveaus van de onderliggende eigenschap. Een hoge factor loading betekent dat het item gevoelig is voor verschillen in de onderliggende eigenschap.
Het belangrijkste verschil met factor analyse, is dat in IRT, de factor loadings ook rekening houden met de moeilijkheidsgraad van de items. Dit betekent dat items die dezelfde moeilijkheidsgraad hebben (dat wil zeggen, ze zijn even moeilijk om te beantwoorden) worden geacht dezelfde hoeveelheid informatie te geven over de onderliggende eigenschap, ongeacht hun factor loadings. Dit is niet het geval in factoranalyse, waar de factor loadings alleen de relatie tussen de geobserveerde variabelen en de onderliggende factor meten.
Tevens kan het zijn dat de factor loadings en de item parameters niet overeenkomen. Bijvoorbeeld een item kan een hoge factor loading hebben (wat aangeeft dat het goed discrimineert) maar toch een slechte fit hebben (X2-statistiek) als het bijvoorbeeld verkeerd wordt begrepen door de respondenten. Omgekeerd kan een item goed passen bij het model maar niet goed discrimineren tussen verschillende niveaus van de eigenschap.

##### Item Characteristic Curves

Hieronder bevinden zich de item characteristic curves. Hierbij een korte uitleg wat het exact betekent:

- De lijnen geven de item characteristic curves weer. Dit is de kans dat een persoon met een bepaald niveau van de onderliggende eigenschap het item correct beantwoordt. Hoe hoger de curve, hoe groter de kans dat een persoon met een bepaald niveau van de onderliggende eigenschap het item correct beantwoordt.
- De x-as van de curve vertegenwoordigt het niveau van de onderliggende eigenschap (bijvoorbeeld vaardigheid of kennis) die een persoon heeft, terwijl de y-as de kans vertegenwoordigt dat een persoon het item correct beantwoordt.
- Hoe stijler de ICC-lijn is, hoe beter het item onderscheid maakt tussen personen met verschillende niveaus van de onderliggende eigenschap. Dit betekent dat het item gevoelig is voor verschillen in de onderliggende eigenschap.
- Hoe hoger het y-axale middelpunt, hoe moeilijker het item is. 

De grafieken voor de ICC en IIC hernoemen de items op volgorde die het IRT model aanbied. Hieronder een tabel met de originele namen van de items, en de bijbehorende item nummers.

```{r leestekens - ICC, echo=FALSE, message=FALSE, warning=FALSE}
params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
item_names <- extract.mirt(fit, what = "itemnames")

df <- data.frame(
  item_number = paste0("Item ", seq_along(item_names)),
  item_names = item_names,
  item_discrimination = params$items.a,
  item_difficulty = params$items.b
)
knitr::kable(df)
```


![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/leestekens_ICC_combined.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/leestekens_ICC_separate.png)

##### Item Information Curves

Hieronder bevinden zich de item information curves. Hierbij een korte uitleg wat het exact betekent:

- De lijnen geven de item information curves weer. Dit is de hoeveelheid informatie die een item geeft over de onderliggende eigenschap. Hoe hoger de curve, hoe meer informatie het item geeft over de onderliggende eigenschap.
- Hoe verder naar rechts de piek van de curve ligt, hoe meer informatie het item geeft over hooge niveaus van de onderliggende eigenschap. 
- Hoe verder naar links de piek van de curve ligt, hoe meer informatie het item geeft over lage niveaus van de onderliggende eigenschap.

De grafieken voor de ICC en IIC hernoemen de items op volgorde die het IRT model aanbied. Hieronder een tabel met de originele namen van de items, en de bijbehorende item nummers.

```{r leestekens - IIC, echo=FALSE, message=FALSE, warning=FALSE}
params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
item_names <- extract.mirt(fit, what = "itemnames")

df <- data.frame(
  item_number = paste0("Item ", seq_along(item_names)),
  item_names = item_names,
  item_discrimination = params$items.a,
  item_difficulty = params$items.b
)
knitr::kable(df)
```


![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/leestekens_IIC_combined.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/leestekens_IIC_separate.png)

##### Item Person Map

Hieronder bevindt zich de item person map. Hierbij een korte uitleg wat het exact betekent:

- De x-as van de grafiek vertegenwoordigt het niveau van de onderliggende eigenschap (bijvoorbeeld vaardigheid of kennis) die een persoon heeft, terwijl de y-as de hoeveelheid participanten vertegenwoordigt.
- Onder de grafiek kan bekeken worden welke vraag ongeveer bij welke hoogte van onderliggende eigenschap past, en hoeveel leerlingen hier daadwerkelijk bij horen.

![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/leestekens_item_person_fit.png)

##### Test Characteristic and Information Curves

Gelijk aan de Item characteristics en information curves, maar dan voor de test als geheel. Hierbij een korte uitleg wat het exact betekent:

- De lijnen geven de test characteristic curves weer. Deze geeft aan hoe goed de test informatie geeft over de onderliggende eigenschap. Hoe hoger de curve, hoe meer informatie de test geeft over de onderliggende eigenschap op dat niveau.
- Grofweg kunnen de lijnen en de curve gezien woren als een amalgamatie van de onderliggende item characteristic en information curves.

![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/leestekens_TCC.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/leestekens_TIC.png)

### Lezen Deel 1

```{r lezen_deel1 - stat extraction, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
# load data
fit <- readRDS(paste0(here(), "/data/irt_analyses2024/irt_fit_lezen_deel1.rds"))
data <- read.csv(paste0(here(), "/data/data_interrim/lezen_deel1.csv"))

# Model fit
model_fit <- as.data.frame(M2(fit, na.rm = TRUE))
model_fit <- cbind(model_fit, nrow(data))
model_fit <- rename(model_fit, `N` = `nrow(data)`)
model_fit <- tidyr::pivot_longer(model_fit, cols = -N, names_to = "Fit Index", values_to = "Value")
model_fit <- dplyr::select(model_fit, `Fit Index`, Value)
model_fit$`Cut-Off` <- c("", "", "> .05", "< .05 | .05 - .1 | > .1", "", "", "< .08 | .08 - .10 | > .10", "> .95 | .90 - .95 | < .90", "> .95 | .90 - .95 | < .90")

# item fit
item_fit <- itemfit(fit, na.rm = TRUE)
colnames(item_fit) <- c("Item", "X2", "df", "RMSEA", "p")
item_fit <- item_fit[order(item_fit$X2), ]
rownames(item_fit) <- NULL

robust_item_fit <- itemfit(fit, na.rm = TRUE)
colnames(robust_item_fit) <- c("Item", "PV_Q1", "df", "RMSEA", "p")
robust_item_fit <- robust_item_fit[order(robust_item_fit$PV_Q1), ]
rownames(robust_item_fit) <- NULL


# Item parameters
params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
params <- params[, -which(colnames(params) %in% c("items.g", "items.u", "means", "F1"))]
sink("/dev/null")
sum <- suppressWarnings(suppressMessages(summary(fit, silent = TRUE)))
sink()
params <- cbind(params, sum$rotF)
params <- cbind(params, sum$h2)
params <- params[, c("F1", colnames(params)[-which(colnames(params) %in% "F1")])]
colnames(params) <- c("Factor Loading", "Discrimination Parameter", "Difficulty Parameter ", "Communalities")
params <- params[order(params$`Factor Loading`, decreasing = TRUE), ]


# # Differential Item Functionining DIF
# # Prepare the data
# gender <- data$gender
# gender <- dplyr::recode(gender, "Man" = 1, "Vrouw" = 2, "Onbekend" = 2)
# gender <- as.factor(gender)
# local_df <- data[, !grepl("package_duration_raw|student_number|student_name|birth_date|gender", colnames(data))]

# # Fit the multiple group model
# dif_fit <- mirt::multipleGroup(
#   data = local_df,
#   model = 1,
#   group = gender
# )

# # Perform DIF analysis
# dif <- mirt::DIF(dif_fit, group = gender, which.par = c("a1", "d"), na.rm = TRUE, plotdif = TRUE)

# Person fit
person_fit <- personfit(fit) %>%
  reframe(
    infit.outside = prop.table(table(z.infit > 1.96 | z.infit < -1.96)),
    outfit.outside = prop.table(table(z.outfit > 1.96 | z.outfit < -1.96))
  ) # lower row = non-fitting people
person_fit$Correct <- c("Correct", "Incorrect")
person_fit <- person_fit %>% dplyr::select(Correct, everything())
```

#### Model Fit

Hieronder bevinden zich de verschillende model fit indices.Hierbij een korte uitleg wat het exact betekent:

- M2: Dit is de M2-statistiek van Maydeu-Olivares, die een maat is voor de absolute fit van het model. Een lager getal duidt op een betere fit. Er is geen vaste cut-off waarde, maar over het algemeen wordt een lagere waarde als beter beschouwd. Als de bijbehorende p-waarde kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- df: de degrees of freedom van het model. Dit is het aantal parameters dat is geschat, minus het aantal constraints dat is opgelegd.
- p-value: de p-waarde van de Goodness of fit test. Als deze kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- RMSEA: RMSEA meet hoe goed een model de waargenomen data benadert, rekening houdend met de fout in het model. Lagere waarden duiden op een betere pasvorm, typisch onder 0.05 voor goede pasvorm en tussen 0.05 en 0.08 voor redelijke pasvorm.
  - RMSEA_5 en RMSEA_95: de 95% betrouwbaarheidsinterval van de RMSEA. Hoe kleiner dit interval rondom de RMSEA waarde, hoe zekerder we zijn van die waarde. 
- SRMSR: SRMSR staat voor "Standardized Root Mean Square Residual". Het is een maat voor de gemiddelde discrepantie tussen de waargenomen en voorspelde covarianties. Een lagere waarde duidt op een betere fit. Een SRMSR van minder dan 0.08 wordt over het algemeen als goed beschouwd.
- TLI: de Tucker-Lewis Index. TLI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.
- CFI: de Comparative Fit Index. De CFI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.
- N: het aantal respondenten in de steekproef.

```{r lezen_deel1 - model fit, echo=FALSE, message=FALSE, warning=FALSE}
model_fit$`Conclusion` <- c("", "", "Poor", "Good", "", "", "Good", "Acceptabel", "Acceptable")
knitr::kable(model_fit)
```

Als we kijken naar de fit indices, zien dat lezen_deel1 een relatiev goeie fit. Alle waardes zijn goed of acceptabel.
Gezien de hoge hoeveelheid observaties (N = `r nrow(data)`), kan de p-waarde als onbetrouwbaar zijn, gezien diens gevoeligheid voor sample size. Daarom wordt de p-waarde genegeerd, en wordt er geconcludeerd dat het model goed past.

#### Person Fit

Hieronder zijn de percentages van person-item-fit indices weergegeven. Hierbij een korte uitleg wat het exact betekent:
- infit.outside: De infit (information-weighted fit) statistiek is gevoelig voor onverwacht gedrag dat invloed heeft op de antwoorden op items die de meeste informatie geven over het vaardigheidsniveau van de persoon. Met andere woorden, het is gevoeliger voor onverwachte antwoorden op items die dicht bij het vaardigheidsniveau van de persoon liggen. De infit.outside waarde in de tabel vertegenwoordigt de proportie van personen wiens infit statistiek buiten het acceptabele bereik valt, wat duidt op een slechte fit.
- outfit.outside: De outfit (outlier-sensitive fit) statistiek is gevoelig voor onverwacht gedrag dat invloed heeft op de antwoorden op items die de minste informatie geven over het vaardigheidsniveau van de persoon. Met andere woorden, het is gevoeliger voor onverwachte antwoorden op items die ver van het vaardigheidsniveau van de persoon liggen. De outfit.outside waarde in de tabel vertegenwoordigt de proportie van personen wiens outfit statistiek buiten het acceptabele bereik valt, wat duidt op een slechte fit.

```{r lezen_deel1 - person fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(person_fit)
```

Gezien zowel de infit en outfit statistieken rond de 95 procent vallen, is er geen reden om aan te nemen dat de personen niet goed passen in het model. De proportie van personen wiens infit statistiek buiten het acceptabele bereik valt, wat groter te zijn (~6%).

Hieronder in de grafiek is te zien welke vragen in hoeverre bijdragen aan de infit en outfit waardes:

![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/lezen_deel1_item_fit.png)

#### Item Fit

Hieronder bevinden zich de verschillende item fit indices. Hierbij een korte uitleg wat het exact betekent:

- X2: Dit is de X2-statistiek van de item fit test. Een lager getal duidt op een betere fit. Er is geen vaste cut-off waarde, maar over het algemeen wordt een lagere waarde als beter beschouwd. Als de bijbehorende p-waarde kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het item niet goed past.
- df: de degrees of freedom van het model. Dit is het aantal parameters dat is geschat, minus het aantal constraints dat is opgelegd.
- p-value: de p-waarde van de Goodness of fit test. Als deze kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- RMSEA: RMSEA meet hoe goed een model de waargenomen data benadert, rekening houdend met de fout in het model. Lagere waarden duiden op een betere pasvorm, typisch onder 0.05 voor goede pasvorm en tussen 0.05 en 0.08 voor redelijke pasvorm.
 
```{r lezen_deel1 - item fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(item_fit)
```

Op basis van de resultaten, blijken meerdere vragen niet goed te passen (zie tabel: vragen met een p lager dan .05).

Dit betekent dat deze waardes niet goed passen in het 1-factor IRT model.Als bepaalde waarden niet goed passen in dit model, kan dit verschillende dingen betekenen.
Ten eerste kan het zijn dat de items niet allemaal dezelfde onderliggende eigenschap meten. Ten tweede kan het zijn dat sommige items verkeerd worden begrepen door de respondenten.
Ten derde kan het zijn dat de veronderstelling van lokale onafhankelijkheid wordt geschonden. Dit betekent dat de reacties op de items afhankelijk zijn van elkaar, zelfs na controle voor de onderliggende eigenschap. Bijvoorbeeld, als het beantwoorden van het ene item helpt bij het beantwoorden van een ander item, dan zullen deze items niet goed passen in het model.

Hieronder volgt een robuste variant van de X2 test, namelijk de PV_Q1 test. Hieronder volgt een korte uitleg wat het exact betekent:

De 'PV_Q1'-statistiek is een afgeleide versie van de Q1-statistiek, een chi-kwadraatstatistiek gebruikt in item respons theorie (IRT) modellen om te testen of een item goed past bij het model. 
Een hoge 'PV_Q1'-waarde duidt op een slechte overeenkomst tussen de waargenomen gegevens en voorspellingen van het model, wat suggereert dat het item mogelijk niet conform het verwachte gedrag 
volgens het IRT-model functioneert. Een niet-significante p-waarde, meestal p > 0.05, in combinatie met lage 'PV_Q1'- en RMSEA.PV_Q1-waarden, geeft aan dat het item goed past bij het IRT-model.
Daarnaast is het belangrijk op te merken dat de 'PV_Q1'-statistiek robuust is tegen invloeden van steekproefomvang en lokale afhankelijkheid, waardoor het een betrouwbaar instrument is om de pasvorm
van een item met het IRT-model te beoordelen, ongeacht de omvang van de steekproef of eventuele lokale onderlinge afhankelijkheden tussen de waarnemingen. Hieronder de resultaten:

```{r lezen_deel1 - robust item fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(robust_item_fit)
```

Om een beter beeld te krijgen, volgen hieronder the item parameters van de vragen.

```{r lezen_deel1 - item parameters, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(params)
```

Op basis van de factor loadings, lijken de volgende vragen niet goed te passen:

- ASL7_lezen_LV_SUPERSOEP_1F_006: 0.2409255
- ASL7_lezen_LV_OCHTEND_2F_009  : 0.2111942
- ASL7_lezen_LV_SUPERSOEP_1F_001: 0.1516813

In IRT, meet een factor loading (ook bekend als een discriminatieparameter) hoe goed een item onderscheid maakt tussen personen met verschillende niveaus van de onderliggende eigenschap. Een hoge factor loading betekent dat het item gevoelig is voor verschillen in de onderliggende eigenschap.
Het belangrijkste verschil met factor analyse, is dat in IRT, de factor loadings ook rekening houden met de moeilijkheidsgraad van de items. Dit betekent dat items die dezelfde moeilijkheidsgraad hebben (dat wil zeggen, ze zijn even moeilijk om te beantwoorden) worden geacht dezelfde hoeveelheid informatie te geven over de onderliggende eigenschap, ongeacht hun factor loadings. Dit is niet het geval in factoranalyse, waar de factor loadings alleen de relatie tussen de geobserveerde variabelen en de onderliggende factor meten.
Tevens kan het zijn dat de factor loadings en de item parameters niet overeenkomen. Bijvoorbeeld een item kan een hoge factor loading hebben (wat aangeeft dat het goed discrimineert) maar toch een slechte fit hebben (X2-statistiek) als het bijvoorbeeld verkeerd wordt begrepen door de respondenten. Omgekeerd kan een item goed passen bij het model maar niet goed discrimineren tussen verschillende niveaus van de eigenschap.

##### Item Characteristic Curves

Hieronder bevinden zich de item characteristic curves. Hierbij een korte uitleg wat het exact betekent:

- De lijnen geven de item characteristic curves weer. Dit is de kans dat een persoon met een bepaald niveau van de onderliggende eigenschap het item correct beantwoordt. Hoe hoger de curve, hoe groter de kans dat een persoon met een bepaald niveau van de onderliggende eigenschap het item correct beantwoordt.
- De x-as van de curve vertegenwoordigt het niveau van de onderliggende eigenschap (bijvoorbeeld vaardigheid of kennis) die een persoon heeft, terwijl de y-as de kans vertegenwoordigt dat een persoon het item correct beantwoordt.
- Hoe stijler de ICC-lijn is, hoe beter het item onderscheid maakt tussen personen met verschillende niveaus van de onderliggende eigenschap. Dit betekent dat het item gevoelig is voor verschillen in de onderliggende eigenschap.
- Hoe hoger het y-axale middelpunt, hoe moeilijker het item is. 

De grafieken voor de ICC en IIC hernoemen de items op volgorde die het IRT model aanbied. Hieronder een tabel met de originele namen van de items, en de bijbehorende item nummers.

```{r lezen_deel1 - ICC, echo=FALSE, message=FALSE, warning=FALSE}
params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
item_names <- extract.mirt(fit, what = "itemnames")

df <- data.frame(
  item_number = paste0("Item ", seq_along(item_names)),
  item_names = item_names,
  item_discrimination = params$items.a,
  item_difficulty = params$items.b
)
knitr::kable(df)
```


![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/lezen_deel1_ICC_combined.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/lezen_deel1_ICC_separate.png)

##### Item Information Curves

Hieronder bevinden zich de item information curves. Hierbij een korte uitleg wat het exact betekent:

- De lijnen geven de item information curves weer. Dit is de hoeveelheid informatie die een item geeft over de onderliggende eigenschap. Hoe hoger de curve, hoe meer informatie het item geeft over de onderliggende eigenschap.
- Hoe verder naar rechts de piek van de curve ligt, hoe meer informatie het item geeft over hooge niveaus van de onderliggende eigenschap. 
- Hoe verder naar links de piek van de curve ligt, hoe meer informatie het item geeft over lage niveaus van de onderliggende eigenschap.

De grafieken voor de ICC en IIC hernoemen de items op volgorde die het IRT model aanbied. Hieronder een tabel met de originele namen van de items, en de bijbehorende item nummers.

```{r lezen_deel1 - IIC, echo=FALSE, message=FALSE, warning=FALSE}
params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
item_names <- extract.mirt(fit, what = "itemnames")

df <- data.frame(
  item_number = paste0("Item ", seq_along(item_names)),
  item_names = item_names,
  item_discrimination = params$items.a,
  item_difficulty = params$items.b
)
knitr::kable(df)
```


![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/lezen_deel1_IIC_combined.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/lezen_deel1_IIC_separate.png)

##### Item Person Map

Hieronder bevindt zich de item person map. Hierbij een korte uitleg wat het exact betekent:

- De x-as van de grafiek vertegenwoordigt het niveau van de onderliggende eigenschap (bijvoorbeeld vaardigheid of kennis) die een persoon heeft, terwijl de y-as de hoeveelheid participanten vertegenwoordigt.
- Onder de grafiek kan bekeken worden welke vraag ongeveer bij welke hoogte van onderliggende eigenschap past, en hoeveel leerlingen hier daadwerkelijk bij horen.

![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/lezen_deel1_item_person_fit.png)

##### Test Characteristic and Information Curves

Gelijk aan de Item characteristics en information curves, maar dan voor de test als geheel. Hierbij een korte uitleg wat het exact betekent:

- De lijnen geven de test characteristic curves weer. Deze geeft aan hoe goed de test informatie geeft over de onderliggende eigenschap. Hoe hoger de curve, hoe meer informatie de test geeft over de onderliggende eigenschap op dat niveau.
- Grofweg kunnen de lijnen en de curve gezien woren als een amalgamatie van de onderliggende item characteristic en information curves.

![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/lezen_deel1_TCC.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/lezen_deel1_TIC.png)

### Lezen Deel 2

```{r lezen_deel2 - stat extraction, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
# load data
fit <- readRDS(paste0(here(), "/data/irt_analyses2024/irt_fit_lezen_deel2.rds"))
data <- read.csv(paste0(here(), "/data/data_interrim/lezen_deel2.csv"))

# Model fit
model_fit <- as.data.frame(M2(fit, na.rm = TRUE))
model_fit <- cbind(model_fit, nrow(data))
model_fit <- rename(model_fit, `N` = `nrow(data)`)
model_fit <- tidyr::pivot_longer(model_fit, cols = -N, names_to = "Fit Index", values_to = "Value")
model_fit <- dplyr::select(model_fit, `Fit Index`, Value)
model_fit$`Cut-Off` <- c("", "", "> .05", "< .05 | .05 - .1 | > .1", "", "", "< .08 | .08 - .10 | > .10", "> .95 | .90 - .95 | < .90", "> .95 | .90 - .95 | < .90")

# item fit
item_fit <- itemfit(fit, na.rm = TRUE)
colnames(item_fit) <- c("Item", "X2", "df", "RMSEA", "p")
item_fit <- item_fit[order(item_fit$X2), ]
rownames(item_fit) <- NULL

robust_item_fit <- itemfit(fit, na.rm = TRUE)
colnames(robust_item_fit) <- c("Item", "PV_Q1", "df", "RMSEA", "p")
robust_item_fit <- robust_item_fit[order(robust_item_fit$PV_Q1), ]
rownames(robust_item_fit) <- NULL


# Item parameters
params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
params <- params[, -which(colnames(params) %in% c("items.g", "items.u", "means", "F1"))]
sink("/dev/null")
sum <- suppressWarnings(suppressMessages(summary(fit, silent = TRUE)))
sink()
params <- cbind(params, sum$rotF)
params <- cbind(params, sum$h2)
params <- params[, c("F1", colnames(params)[-which(colnames(params) %in% "F1")])]
colnames(params) <- c("Factor Loading", "Discrimination Parameter", "Difficulty Parameter ", "Communalities")
params <- params[order(params$`Factor Loading`, decreasing = TRUE), ]


# # Differential Item Functionining DIF
# # Prepare the data
# gender <- data$gender
# gender <- dplyr::recode(gender, "Man" = 1, "Vrouw" = 2, "Onbekend" = 2)
# gender <- as.factor(gender)
# local_df <- data[, !grepl("package_duration_raw|student_number|student_name|birth_date|gender", colnames(data))]

# # Fit the multiple group model
# dif_fit <- mirt::multipleGroup(
#   data = local_df,
#   model = 1,
#   group = gender
# )

# # Perform DIF analysis
# dif <- mirt::DIF(dif_fit, group = gender, which.par = c("a1", "d"), na.rm = TRUE, plotdif = TRUE)

# Person fit
person_fit <- personfit(fit) %>%
  reframe(
    infit.outside = prop.table(table(z.infit > 1.96 | z.infit < -1.96)),
    outfit.outside = prop.table(table(z.outfit > 1.96 | z.outfit < -1.96))
  ) # lower row = non-fitting people
person_fit$Correct <- c("Correct", "Incorrect")
person_fit <- person_fit %>% dplyr::select(Correct, everything())
```

#### Model Fit

Hieronder bevinden zich de verschillende model fit indices.Hierbij een korte uitleg wat het exact betekent:

- M2: Dit is de M2-statistiek van Maydeu-Olivares, die een maat is voor de absolute fit van het model. Een lager getal duidt op een betere fit. Er is geen vaste cut-off waarde, maar over het algemeen wordt een lagere waarde als beter beschouwd. Als de bijbehorende p-waarde kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- df: de degrees of freedom van het model. Dit is het aantal parameters dat is geschat, minus het aantal constraints dat is opgelegd.
- p-value: de p-waarde van de Goodness of fit test. Als deze kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- RMSEA: RMSEA meet hoe goed een model de waargenomen data benadert, rekening houdend met de fout in het model. Lagere waarden duiden op een betere pasvorm, typisch onder 0.05 voor goede pasvorm en tussen 0.05 en 0.08 voor redelijke pasvorm.
  - RMSEA_5 en RMSEA_95: de 95% betrouwbaarheidsinterval van de RMSEA. Hoe kleiner dit interval rondom de RMSEA waarde, hoe zekerder we zijn van die waarde. 
- SRMSR: SRMSR staat voor "Standardized Root Mean Square Residual". Het is een maat voor de gemiddelde discrepantie tussen de waargenomen en voorspelde covarianties. Een lagere waarde duidt op een betere fit. Een SRMSR van minder dan 0.08 wordt over het algemeen als goed beschouwd.
- TLI: de Tucker-Lewis Index. TLI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.
- CFI: de Comparative Fit Index. De CFI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.
- N: het aantal respondenten in de steekproef.

```{r lezen_deel2 - model fit, echo=FALSE, message=FALSE, warning=FALSE}
model_fit$`Conclusion` <- c("", "", "Poor", "Good", "", "", "Good", "Good", "Good")
knitr::kable(model_fit)
```

Als we kijken naar de fit indices, zien dat lezen_deel2 een relatiev goeie fit. Alle waardes zijn goed of acceptabel.
Gezien de hoge hoeveelheid observaties (N = `r nrow(data)`), kan de p-waarde als onbetrouwbaar zijn, gezien diens gevoeligheid voor sample size. Daarom wordt de p-waarde genegeerd, en wordt er geconcludeerd dat het model goed past.

#### Person Fit

Hieronder zijn de percentages van person-item-fit indices weergegeven. Hierbij een korte uitleg wat het exact betekent:
- infit.outside: De infit (information-weighted fit) statistiek is gevoelig voor onverwacht gedrag dat invloed heeft op de antwoorden op items die de meeste informatie geven over het vaardigheidsniveau van de persoon. Met andere woorden, het is gevoeliger voor onverwachte antwoorden op items die dicht bij het vaardigheidsniveau van de persoon liggen. De infit.outside waarde in de tabel vertegenwoordigt de proportie van personen wiens infit statistiek buiten het acceptabele bereik valt, wat duidt op een slechte fit.
- outfit.outside: De outfit (outlier-sensitive fit) statistiek is gevoelig voor onverwacht gedrag dat invloed heeft op de antwoorden op items die de minste informatie geven over het vaardigheidsniveau van de persoon. Met andere woorden, het is gevoeliger voor onverwachte antwoorden op items die ver van het vaardigheidsniveau van de persoon liggen. De outfit.outside waarde in de tabel vertegenwoordigt de proportie van personen wiens outfit statistiek buiten het acceptabele bereik valt, wat duidt op een slechte fit.

```{r lezen_deel2 - person fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(person_fit)
```

Gezien zowel de infit en outfit statistieken rond de 95 procent vallen, is er geen reden om aan te nemen dat de personen niet goed passen in het model. De proportie van personen wiens infit statistiek buiten het acceptabele bereik valt, wat groter te zijn (~6%).

Hieronder in de grafiek is te zien welke vragen in hoeverre bijdragen aan de infit en outfit waardes:

![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/lezen_deel2_item_fit.png)

#### Item Fit

Hieronder bevinden zich de verschillende item fit indices. Hierbij een korte uitleg wat het exact betekent:

- X2: Dit is de X2-statistiek van de item fit test. Een lager getal duidt op een betere fit. Er is geen vaste cut-off waarde, maar over het algemeen wordt een lagere waarde als beter beschouwd. Als de bijbehorende p-waarde kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het item niet goed past.
- df: de degrees of freedom van het model. Dit is het aantal parameters dat is geschat, minus het aantal constraints dat is opgelegd.
- p-value: de p-waarde van de Goodness of fit test. Als deze kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- RMSEA: RMSEA meet hoe goed een model de waargenomen data benadert, rekening houdend met de fout in het model. Lagere waarden duiden op een betere pasvorm, typisch onder 0.05 voor goede pasvorm en tussen 0.05 en 0.08 voor redelijke pasvorm.
 
```{r lezen_deel2 - item fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(item_fit)
```

Op basis van de resultaten, blijken meerdere vragen niet goed te passen (zie tabel: vragen met een p lager dan .05).

Dit betekent dat deze waardes niet goed passen in het 1-factor IRT model.Als bepaalde waarden niet goed passen in dit model, kan dit verschillende dingen betekenen.
Ten eerste kan het zijn dat de items niet allemaal dezelfde onderliggende eigenschap meten. Ten tweede kan het zijn dat sommige items verkeerd worden begrepen door de respondenten.
Ten derde kan het zijn dat de veronderstelling van lokale onafhankelijkheid wordt geschonden. Dit betekent dat de reacties op de items afhankelijk zijn van elkaar, zelfs na controle voor de onderliggende eigenschap. Bijvoorbeeld, als het beantwoorden van het ene item helpt bij het beantwoorden van een ander item, dan zullen deze items niet goed passen in het model.

Hieronder volgt een robuste variant van de X2 test, namelijk de PV_Q1 test. Hieronder volgt een korte uitleg wat het exact betekent:

De 'PV_Q1'-statistiek is een afgeleide versie van de Q1-statistiek, een chi-kwadraatstatistiek gebruikt in item respons theorie (IRT) modellen om te testen of een item goed past bij het model. 
Een hoge 'PV_Q1'-waarde duidt op een slechte overeenkomst tussen de waargenomen gegevens en voorspellingen van het model, wat suggereert dat het item mogelijk niet conform het verwachte gedrag 
volgens het IRT-model functioneert. Een niet-significante p-waarde, meestal p > 0.05, in combinatie met lage 'PV_Q1'- en RMSEA.PV_Q1-waarden, geeft aan dat het item goed past bij het IRT-model.
Daarnaast is het belangrijk op te merken dat de 'PV_Q1'-statistiek robuust is tegen invloeden van steekproefomvang en lokale afhankelijkheid, waardoor het een betrouwbaar instrument is om de pasvorm
van een item met het IRT-model te beoordelen, ongeacht de omvang van de steekproef of eventuele lokale onderlinge afhankelijkheden tussen de waarnemingen. Hieronder de resultaten:

```{r lezen_deel2 - robust item fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(robust_item_fit)
```

Om een beter beeld te krijgen, volgen hieronder the item parameters van de vragen.

```{r lezen_deel2 - item parameters, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(params)
```

Op basis van de factor loadings, lijken de volgende vragen niet goed te passen:

- ASL7_lezen_LV_SIBERIË_1F_001: 0.2933211
- ASL7_lezen_LV_REIS_2F_004   : 0.2900740
- ASL7_lezen_LV_SIBERIË_1F_005: 0.2830041
- ASL7_lezen_LV_SIBERIË_1F_008: 0.2272174

In IRT, meet een factor loading (ook bekend als een discriminatieparameter) hoe goed een item onderscheid maakt tussen personen met verschillende niveaus van de onderliggende eigenschap. Een hoge factor loading betekent dat het item gevoelig is voor verschillen in de onderliggende eigenschap.
Het belangrijkste verschil met factor analyse, is dat in IRT, de factor loadings ook rekening houden met de moeilijkheidsgraad van de items. Dit betekent dat items die dezelfde moeilijkheidsgraad hebben (dat wil zeggen, ze zijn even moeilijk om te beantwoorden) worden geacht dezelfde hoeveelheid informatie te geven over de onderliggende eigenschap, ongeacht hun factor loadings. Dit is niet het geval in factoranalyse, waar de factor loadings alleen de relatie tussen de geobserveerde variabelen en de onderliggende factor meten.
Tevens kan het zijn dat de factor loadings en de item parameters niet overeenkomen. Bijvoorbeeld een item kan een hoge factor loading hebben (wat aangeeft dat het goed discrimineert) maar toch een slechte fit hebben (X2-statistiek) als het bijvoorbeeld verkeerd wordt begrepen door de respondenten. Omgekeerd kan een item goed passen bij het model maar niet goed discrimineren tussen verschillende niveaus van de eigenschap.

##### Item Characteristic Curves

Hieronder bevinden zich de item characteristic curves. Hierbij een korte uitleg wat het exact betekent:

- De lijnen geven de item characteristic curves weer. Dit is de kans dat een persoon met een bepaald niveau van de onderliggende eigenschap het item correct beantwoordt. Hoe hoger de curve, hoe groter de kans dat een persoon met een bepaald niveau van de onderliggende eigenschap het item correct beantwoordt.
- De x-as van de curve vertegenwoordigt het niveau van de onderliggende eigenschap (bijvoorbeeld vaardigheid of kennis) die een persoon heeft, terwijl de y-as de kans vertegenwoordigt dat een persoon het item correct beantwoordt.
- Hoe stijler de ICC-lijn is, hoe beter het item onderscheid maakt tussen personen met verschillende niveaus van de onderliggende eigenschap. Dit betekent dat het item gevoelig is voor verschillen in de onderliggende eigenschap.
- Hoe hoger het y-axale middelpunt, hoe moeilijker het item is. 

De grafieken voor de ICC en IIC hernoemen de items op volgorde die het IRT model aanbied. Hieronder een tabel met de originele namen van de items, en de bijbehorende item nummers.

```{r lezen_deel2 - ICC, echo=FALSE, message=FALSE, warning=FALSE}
params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
item_names <- extract.mirt(fit, what = "itemnames")

df <- data.frame(
  item_number = paste0("Item ", seq_along(item_names)),
  item_names = item_names,
  item_discrimination = params$items.a,
  item_difficulty = params$items.b
)
knitr::kable(df)
```


![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/lezen_deel2_ICC_combined.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/lezen_deel2_ICC_separate.png)

##### Item Information Curves

Hieronder bevinden zich de item information curves. Hierbij een korte uitleg wat het exact betekent:

- De lijnen geven de item information curves weer. Dit is de hoeveelheid informatie die een item geeft over de onderliggende eigenschap. Hoe hoger de curve, hoe meer informatie het item geeft over de onderliggende eigenschap.
- Hoe verder naar rechts de piek van de curve ligt, hoe meer informatie het item geeft over hooge niveaus van de onderliggende eigenschap. 
- Hoe verder naar links de piek van de curve ligt, hoe meer informatie het item geeft over lage niveaus van de onderliggende eigenschap.

De grafieken voor de ICC en IIC hernoemen de items op volgorde die het IRT model aanbied. Hieronder een tabel met de originele namen van de items, en de bijbehorende item nummers.

```{r lezen_deel2 - IIC, echo=FALSE, message=FALSE, warning=FALSE}
params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
item_names <- extract.mirt(fit, what = "itemnames")

df <- data.frame(
  item_number = paste0("Item ", seq_along(item_names)),
  item_names = item_names,
  item_discrimination = params$items.a,
  item_difficulty = params$items.b
)
knitr::kable(df)
```


![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/lezen_deel2_IIC_combined.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/lezen_deel2_IIC_separate.png)

##### Item Person Map

Hieronder bevindt zich de item person map. Hierbij een korte uitleg wat het exact betekent:

- De x-as van de grafiek vertegenwoordigt het niveau van de onderliggende eigenschap (bijvoorbeeld vaardigheid of kennis) die een persoon heeft, terwijl de y-as de hoeveelheid participanten vertegenwoordigt.
- Onder de grafiek kan bekeken worden welke vraag ongeveer bij welke hoogte van onderliggende eigenschap past, en hoeveel leerlingen hier daadwerkelijk bij horen.

![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/lezen_deel2_item_person_fit.png)

##### Test Characteristic and Information Curves

Gelijk aan de Item characteristics en information curves, maar dan voor de test als geheel. Hierbij een korte uitleg wat het exact betekent:

- De lijnen geven de test characteristic curves weer. Deze geeft aan hoe goed de test informatie geeft over de onderliggende eigenschap. Hoe hoger de curve, hoe meer informatie de test geeft over de onderliggende eigenschap op dat niveau.
- Grofweg kunnen de lijnen en de curve gezien woren als een amalgamatie van de onderliggende item characteristic en information curves.

![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/lezen_deel2_TCC.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/lezen_deel2_TIC.png)

### Opzoeken

```{r opzoeken - stat extraction, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
# load data
fit <- readRDS(paste0(here(), "/data/irt_analyses2024/irt_fit_opzoeken.rds"))
data <- read.csv(paste0(here(), "/data/data_interrim/opzoeken.csv"))

# Model fit
model_fit <- as.data.frame(M2(fit, na.rm = TRUE))
model_fit <- cbind(model_fit, nrow(data))
model_fit <- rename(model_fit, `N` = `nrow(data)`)
model_fit <- tidyr::pivot_longer(model_fit, cols = -N, names_to = "Fit Index", values_to = "Value")
model_fit <- dplyr::select(model_fit, `Fit Index`, Value)
model_fit$`Cut-Off` <- c("", "", "> .05", "< .05 | .05 - .1 | > .1", "", "", "< .08 | .08 - .10 | > .10", "> .95 | .90 - .95 | < .90", "> .95 | .90 - .95 | < .90")

# item fit
item_fit <- itemfit(fit, na.rm = TRUE)
colnames(item_fit) <- c("Item", "X2", "df", "RMSEA", "p")
item_fit <- item_fit[order(item_fit$X2), ]
rownames(item_fit) <- NULL

robust_item_fit <- itemfit(fit, na.rm = TRUE)
colnames(robust_item_fit) <- c("Item", "PV_Q1", "df", "RMSEA", "p")
robust_item_fit <- robust_item_fit[order(robust_item_fit$PV_Q1), ]
rownames(robust_item_fit) <- NULL


# Item parameters
params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
params <- params[, -which(colnames(params) %in% c("items.g", "items.u", "means", "F1"))]
sink("/dev/null")
sum <- suppressWarnings(suppressMessages(summary(fit, silent = TRUE)))
sink()
params <- cbind(params, sum$rotF)
params <- cbind(params, sum$h2)
params <- params[, c("F1", colnames(params)[-which(colnames(params) %in% "F1")])]
colnames(params) <- c("Factor Loading", "Discrimination Parameter", "Difficulty Parameter ", "Communalities")
params <- params[order(params$`Factor Loading`, decreasing = TRUE), ]


# # Differential Item Functionining DIF
# # Prepare the data
# gender <- data$gender
# gender <- dplyr::recode(gender, "Man" = 1, "Vrouw" = 2, "Onbekend" = 2)
# gender <- as.factor(gender)
# local_df <- data[, !grepl("package_duration_raw|student_number|student_name|birth_date|gender", colnames(data))]

# # Fit the multiple group model
# dif_fit <- mirt::multipleGroup(
#   data = local_df,
#   model = 1,
#   group = gender
# )

# # Perform DIF analysis
# dif <- mirt::DIF(dif_fit, group = gender, which.par = c("a1", "d"), na.rm = TRUE, plotdif = TRUE)

# Person fit
person_fit <- personfit(fit) %>%
  reframe(
    infit.outside = prop.table(table(z.infit > 1.96 | z.infit < -1.96)),
    outfit.outside = prop.table(table(z.outfit > 1.96 | z.outfit < -1.96))
  ) # lower row = non-fitting people
person_fit$Correct <- c("Correct", "Incorrect")
person_fit <- person_fit %>% dplyr::select(Correct, everything())
```

#### Model Fit

Hieronder bevinden zich de verschillende model fit indices.Hierbij een korte uitleg wat het exact betekent:

- M2: Dit is de M2-statistiek van Maydeu-Olivares, die een maat is voor de absolute fit van het model. Een lager getal duidt op een betere fit. Er is geen vaste cut-off waarde, maar over het algemeen wordt een lagere waarde als beter beschouwd. Als de bijbehorende p-waarde kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- df: de degrees of freedom van het model. Dit is het aantal parameters dat is geschat, minus het aantal constraints dat is opgelegd.
- p-value: de p-waarde van de Goodness of fit test. Als deze kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- RMSEA: RMSEA meet hoe goed een model de waargenomen data benadert, rekening houdend met de fout in het model. Lagere waarden duiden op een betere pasvorm, typisch onder 0.05 voor goede pasvorm en tussen 0.05 en 0.08 voor redelijke pasvorm.
  - RMSEA_5 en RMSEA_95: de 95% betrouwbaarheidsinterval van de RMSEA. Hoe kleiner dit interval rondom de RMSEA waarde, hoe zekerder we zijn van die waarde. 
- SRMSR: SRMSR staat voor "Standardized Root Mean Square Residual". Het is een maat voor de gemiddelde discrepantie tussen de waargenomen en voorspelde covarianties. Een lagere waarde duidt op een betere fit. Een SRMSR van minder dan 0.08 wordt over het algemeen als goed beschouwd.
- TLI: de Tucker-Lewis Index. TLI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.
- CFI: de Comparative Fit Index. De CFI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.
- N: het aantal respondenten in de steekproef.

```{r opzoeken - model fit, echo=FALSE, message=FALSE, warning=FALSE}
model_fit$`Conclusion` <- c("", "", "Poor", "Good", "", "", "Good", "Good", "Good")
knitr::kable(model_fit)
```

Als we kijken naar de fit indices, zien dat opzoeken een relatiev goeie fit. Alle waardes zijn goed of acceptabel.
Gezien de hoge hoeveelheid observaties (N = `r nrow(data)`), kan de p-waarde als onbetrouwbaar zijn, gezien diens gevoeligheid voor sample size. Daarom wordt de p-waarde genegeerd, en wordt er geconcludeerd dat het model goed past.

#### Person Fit

Hieronder zijn de percentages van person-item-fit indices weergegeven. Hierbij een korte uitleg wat het exact betekent:
- infit.outside: De infit (information-weighted fit) statistiek is gevoelig voor onverwacht gedrag dat invloed heeft op de antwoorden op items die de meeste informatie geven over het vaardigheidsniveau van de persoon. Met andere woorden, het is gevoeliger voor onverwachte antwoorden op items die dicht bij het vaardigheidsniveau van de persoon liggen. De infit.outside waarde in de tabel vertegenwoordigt de proportie van personen wiens infit statistiek buiten het acceptabele bereik valt, wat duidt op een slechte fit.
- outfit.outside: De outfit (outlier-sensitive fit) statistiek is gevoelig voor onverwacht gedrag dat invloed heeft op de antwoorden op items die de minste informatie geven over het vaardigheidsniveau van de persoon. Met andere woorden, het is gevoeliger voor onverwachte antwoorden op items die ver van het vaardigheidsniveau van de persoon liggen. De outfit.outside waarde in de tabel vertegenwoordigt de proportie van personen wiens outfit statistiek buiten het acceptabele bereik valt, wat duidt op een slechte fit.

```{r opzoeken - person fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(person_fit)
```

Gezien zowel de infit en outfit statistieken rond de 95 procent vallen, is er geen reden om aan te nemen dat de personen niet goed passen in het model. De proportie van personen wiens infit statistiek buiten het acceptabele bereik valt, wat groter te zijn (~6%).

Hieronder in de grafiek is te zien welke vragen in hoeverre bijdragen aan de infit en outfit waardes:

![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/opzoeken_item_fit.png)

#### Item Fit

Hieronder bevinden zich de verschillende item fit indices. Hierbij een korte uitleg wat het exact betekent:

- X2: Dit is de X2-statistiek van de item fit test. Een lager getal duidt op een betere fit. Er is geen vaste cut-off waarde, maar over het algemeen wordt een lagere waarde als beter beschouwd. Als de bijbehorende p-waarde kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het item niet goed past.
- df: de degrees of freedom van het model. Dit is het aantal parameters dat is geschat, minus het aantal constraints dat is opgelegd.
- p-value: de p-waarde van de Goodness of fit test. Als deze kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- RMSEA: RMSEA meet hoe goed een model de waargenomen data benadert, rekening houdend met de fout in het model. Lagere waarden duiden op een betere pasvorm, typisch onder 0.05 voor goede pasvorm en tussen 0.05 en 0.08 voor redelijke pasvorm.
 
```{r opzoeken - item fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(item_fit)
```

Op basis van de resultaten, blijken meerdere vragen niet goed te passen (zie tabel: vragen met een p lager dan .05).

Dit betekent dat deze waardes niet goed passen in het 1-factor IRT model.Als bepaalde waarden niet goed passen in dit model, kan dit verschillende dingen betekenen.
Ten eerste kan het zijn dat de items niet allemaal dezelfde onderliggende eigenschap meten. Ten tweede kan het zijn dat sommige items verkeerd worden begrepen door de respondenten.
Ten derde kan het zijn dat de veronderstelling van lokale onafhankelijkheid wordt geschonden. Dit betekent dat de reacties op de items afhankelijk zijn van elkaar, zelfs na controle voor de onderliggende eigenschap. Bijvoorbeeld, als het beantwoorden van het ene item helpt bij het beantwoorden van een ander item, dan zullen deze items niet goed passen in het model.

Hieronder volgt een robuste variant van de X2 test, namelijk de PV_Q1 test. Hieronder volgt een korte uitleg wat het exact betekent:

De 'PV_Q1'-statistiek is een afgeleide versie van de Q1-statistiek, een chi-kwadraatstatistiek gebruikt in item respons theorie (IRT) modellen om te testen of een item goed past bij het model. 
Een hoge 'PV_Q1'-waarde duidt op een slechte overeenkomst tussen de waargenomen gegevens en voorspellingen van het model, wat suggereert dat het item mogelijk niet conform het verwachte gedrag 
volgens het IRT-model functioneert. Een niet-significante p-waarde, meestal p > 0.05, in combinatie met lage 'PV_Q1'- en RMSEA.PV_Q1-waarden, geeft aan dat het item goed past bij het IRT-model.
Daarnaast is het belangrijk op te merken dat de 'PV_Q1'-statistiek robuust is tegen invloeden van steekproefomvang en lokale afhankelijkheid, waardoor het een betrouwbaar instrument is om de pasvorm
van een item met het IRT-model te beoordelen, ongeacht de omvang van de steekproef of eventuele lokale onderlinge afhankelijkheden tussen de waarnemingen. Hieronder de resultaten:

```{r opzoeken - robust item fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(robust_item_fit)
```

Om een beter beeld te krijgen, volgen hieronder the item parameters van de vragen.

```{r opzoeken - item parameters, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(params)
```

Op basis van de factor loadings, zijn er geen vragen die slecht laaden. 

In IRT, meet een factor loading (ook bekend als een discriminatieparameter) hoe goed een item onderscheid maakt tussen personen met verschillende niveaus van de onderliggende eigenschap. Een hoge factor loading betekent dat het item gevoelig is voor verschillen in de onderliggende eigenschap.
Het belangrijkste verschil met factor analyse, is dat in IRT, de factor loadings ook rekening houden met de moeilijkheidsgraad van de items. Dit betekent dat items die dezelfde moeilijkheidsgraad hebben (dat wil zeggen, ze zijn even moeilijk om te beantwoorden) worden geacht dezelfde hoeveelheid informatie te geven over de onderliggende eigenschap, ongeacht hun factor loadings. Dit is niet het geval in factoranalyse, waar de factor loadings alleen de relatie tussen de geobserveerde variabelen en de onderliggende factor meten.
Tevens kan het zijn dat de factor loadings en de item parameters niet overeenkomen. Bijvoorbeeld een item kan een hoge factor loading hebben (wat aangeeft dat het goed discrimineert) maar toch een slechte fit hebben (X2-statistiek) als het bijvoorbeeld verkeerd wordt begrepen door de respondenten. Omgekeerd kan een item goed passen bij het model maar niet goed discrimineren tussen verschillende niveaus van de eigenschap.

##### Item Characteristic Curves

Hieronder bevinden zich de item characteristic curves. Hierbij een korte uitleg wat het exact betekent:

- De lijnen geven de item characteristic curves weer. Dit is de kans dat een persoon met een bepaald niveau van de onderliggende eigenschap het item correct beantwoordt. Hoe hoger de curve, hoe groter de kans dat een persoon met een bepaald niveau van de onderliggende eigenschap het item correct beantwoordt.
- De x-as van de curve vertegenwoordigt het niveau van de onderliggende eigenschap (bijvoorbeeld vaardigheid of kennis) die een persoon heeft, terwijl de y-as de kans vertegenwoordigt dat een persoon het item correct beantwoordt.
- Hoe stijler de ICC-lijn is, hoe beter het item onderscheid maakt tussen personen met verschillende niveaus van de onderliggende eigenschap. Dit betekent dat het item gevoelig is voor verschillen in de onderliggende eigenschap.
- Hoe hoger het y-axale middelpunt, hoe moeilijker het item is. 

De grafieken voor de ICC en IIC hernoemen de items op volgorde die het IRT model aanbied. Hieronder een tabel met de originele namen van de items, en de bijbehorende item nummers.

```{r opzoeken - ICC, echo=FALSE, message=FALSE, warning=FALSE}
params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
item_names <- extract.mirt(fit, what = "itemnames")

df <- data.frame(
  item_number = paste0("Item ", seq_along(item_names)),
  item_names = item_names,
  item_discrimination = params$items.a,
  item_difficulty = params$items.b
)
knitr::kable(df)
```


![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/opzoeken_ICC_combined.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/opzoeken_ICC_separate.png)

##### Item Information Curves

Hieronder bevinden zich de item information curves. Hierbij een korte uitleg wat het exact betekent:

- De lijnen geven de item information curves weer. Dit is de hoeveelheid informatie die een item geeft over de onderliggende eigenschap. Hoe hoger de curve, hoe meer informatie het item geeft over de onderliggende eigenschap.
- Hoe verder naar rechts de piek van de curve ligt, hoe meer informatie het item geeft over hooge niveaus van de onderliggende eigenschap. 
- Hoe verder naar links de piek van de curve ligt, hoe meer informatie het item geeft over lage niveaus van de onderliggende eigenschap.

De grafieken voor de ICC en IIC hernoemen de items op volgorde die het IRT model aanbied. Hieronder een tabel met de originele namen van de items, en de bijbehorende item nummers.

```{r opzoeken - IIC, echo=FALSE, message=FALSE, warning=FALSE}
params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
item_names <- extract.mirt(fit, what = "itemnames")

df <- data.frame(
  item_number = paste0("Item ", seq_along(item_names)),
  item_names = item_names,
  item_discrimination = params$items.a,
  item_difficulty = params$items.b
)
knitr::kable(df)
```


![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/opzoeken_IIC_combined.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/opzoeken_IIC_separate.png)

##### Item Person Map

Hieronder bevindt zich de item person map. Hierbij een korte uitleg wat het exact betekent:

- De x-as van de grafiek vertegenwoordigt het niveau van de onderliggende eigenschap (bijvoorbeeld vaardigheid of kennis) die een persoon heeft, terwijl de y-as de hoeveelheid participanten vertegenwoordigt.
- Onder de grafiek kan bekeken worden welke vraag ongeveer bij welke hoogte van onderliggende eigenschap past, en hoeveel leerlingen hier daadwerkelijk bij horen.

![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/opzoeken_item_person_fit.png)

##### Test Characteristic and Information Curves

Gelijk aan de Item characteristics en information curves, maar dan voor de test als geheel. Hierbij een korte uitleg wat het exact betekent:

- De lijnen geven de test characteristic curves weer. Deze geeft aan hoe goed de test informatie geeft over de onderliggende eigenschap. Hoe hoger de curve, hoe meer informatie de test geeft over de onderliggende eigenschap op dat niveau.
- Grofweg kunnen de lijnen en de curve gezien woren als een amalgamatie van de onderliggende item characteristic en information curves.

![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/opzoeken_TCC.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/opzoeken_TIC.png)

### Sommen Maken

```{r sommenmaken - stat extraction, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
# load data
fit <- readRDS(paste0(here(), "/data/irt_analyses2024/irt_fit_sommenmaken.rds"))
data <- read.csv(paste0(here(), "/data/data_interrim/sommenmaken.csv"))

# Model fit
model_fit <- as.data.frame(M2(fit, na.rm = TRUE))
model_fit <- cbind(model_fit, nrow(data))
model_fit <- rename(model_fit, `N` = `nrow(data)`)
model_fit <- tidyr::pivot_longer(model_fit, cols = -N, names_to = "Fit Index", values_to = "Value")
model_fit <- dplyr::select(model_fit, `Fit Index`, Value)
model_fit$`Cut-Off` <- c("", "", "> .05", "< .05 | .05 - .1 | > .1", "", "", "< .08 | .08 - .10 | > .10", "> .95 | .90 - .95 | < .90", "> .95 | .90 - .95 | < .90")

# item fit
item_fit <- itemfit(fit, na.rm = TRUE)
colnames(item_fit) <- c("Item", "X2", "df", "RMSEA", "p")
item_fit <- item_fit[order(item_fit$X2), ]
rownames(item_fit) <- NULL

robust_item_fit <- itemfit(fit, na.rm = TRUE)
colnames(robust_item_fit) <- c("Item", "PV_Q1", "df", "RMSEA", "p")
robust_item_fit <- robust_item_fit[order(robust_item_fit$PV_Q1), ]
rownames(robust_item_fit) <- NULL


# Item parameters
params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
params <- params[, -which(colnames(params) %in% c("items.g", "items.u", "means", "F1"))]
sink("/dev/null")
sum <- suppressWarnings(suppressMessages(summary(fit, silent = TRUE)))
sink()
params <- cbind(params, sum$rotF)
params <- cbind(params, sum$h2)
params <- params[, c("F1", colnames(params)[-which(colnames(params) %in% "F1")])]
colnames(params) <- c("Factor Loading", "Discrimination Parameter", "Difficulty Parameter ", "Communalities")
params <- params[order(params$`Factor Loading`, decreasing = TRUE), ]


# # Differential Item Functionining DIF
# # Prepare the data
# gender <- data$gender
# gender <- dplyr::recode(gender, "Man" = 1, "Vrouw" = 2, "Onbekend" = 2)
# gender <- as.factor(gender)
# local_df <- data[, !grepl("package_duration_raw|student_number|student_name|birth_date|gender", colnames(data))]

# # Fit the multiple group model
# dif_fit <- mirt::multipleGroup(
#   data = local_df,
#   model = 1,
#   group = gender
# )

# # Perform DIF analysis
# dif <- mirt::DIF(dif_fit, group = gender, which.par = c("a1", "d"), na.rm = TRUE, plotdif = TRUE)

# Person fit
person_fit <- personfit(fit) %>%
  reframe(
    infit.outside = prop.table(table(z.infit > 1.96 | z.infit < -1.96)),
    outfit.outside = prop.table(table(z.outfit > 1.96 | z.outfit < -1.96))
  ) # lower row = non-fitting people
person_fit$Correct <- c("Correct", "Incorrect")
person_fit <- person_fit %>% dplyr::select(Correct, everything())
```

#### Model Fit

Hieronder bevinden zich de verschillende model fit indices.Hierbij een korte uitleg wat het exact betekent:

- M2: Dit is de M2-statistiek van Maydeu-Olivares, die een maat is voor de absolute fit van het model. Een lager getal duidt op een betere fit. Er is geen vaste cut-off waarde, maar over het algemeen wordt een lagere waarde als beter beschouwd. Als de bijbehorende p-waarde kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- df: de degrees of freedom van het model. Dit is het aantal parameters dat is geschat, minus het aantal constraints dat is opgelegd.
- p-value: de p-waarde van de Goodness of fit test. Als deze kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- RMSEA: RMSEA meet hoe goed een model de waargenomen data benadert, rekening houdend met de fout in het model. Lagere waarden duiden op een betere pasvorm, typisch onder 0.05 voor goede pasvorm en tussen 0.05 en 0.08 voor redelijke pasvorm.
  - RMSEA_5 en RMSEA_95: de 95% betrouwbaarheidsinterval van de RMSEA. Hoe kleiner dit interval rondom de RMSEA waarde, hoe zekerder we zijn van die waarde. 
- SRMSR: SRMSR staat voor "Standardized Root Mean Square Residual". Het is een maat voor de gemiddelde discrepantie tussen de waargenomen en voorspelde covarianties. Een lagere waarde duidt op een betere fit. Een SRMSR van minder dan 0.08 wordt over het algemeen als goed beschouwd.
- TLI: de Tucker-Lewis Index. TLI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.
- CFI: de Comparative Fit Index. De CFI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.
- N: het aantal respondenten in de steekproef.

```{r sommenmaken - model fit, echo=FALSE, message=FALSE, warning=FALSE}
model_fit$`Conclusion` <- c("", "", "Poor", "Good", "", "", "Good", "Good", "Good")
knitr::kable(model_fit)
```

Als we kijken naar de fit indices, zien dat sommenmaken een relatiev goeie fit. Alle waardes zijn goed of acceptabel.
Gezien de hoge hoeveelheid observaties (N = `r nrow(data)`), kan de p-waarde als onbetrouwbaar zijn, gezien diens gevoeligheid voor sample size. Daarom wordt de p-waarde genegeerd, en wordt er geconcludeerd dat het model goed past.

#### Person Fit

Hieronder zijn de percentages van person-item-fit indices weergegeven. Hierbij een korte uitleg wat het exact betekent:
- infit.outside: De infit (information-weighted fit) statistiek is gevoelig voor onverwacht gedrag dat invloed heeft op de antwoorden op items die de meeste informatie geven over het vaardigheidsniveau van de persoon. Met andere woorden, het is gevoeliger voor onverwachte antwoorden op items die dicht bij het vaardigheidsniveau van de persoon liggen. De infit.outside waarde in de tabel vertegenwoordigt de proportie van personen wiens infit statistiek buiten het acceptabele bereik valt, wat duidt op een slechte fit.
- outfit.outside: De outfit (outlier-sensitive fit) statistiek is gevoelig voor onverwacht gedrag dat invloed heeft op de antwoorden op items die de minste informatie geven over het vaardigheidsniveau van de persoon. Met andere woorden, het is gevoeliger voor onverwachte antwoorden op items die ver van het vaardigheidsniveau van de persoon liggen. De outfit.outside waarde in de tabel vertegenwoordigt de proportie van personen wiens outfit statistiek buiten het acceptabele bereik valt, wat duidt op een slechte fit.

```{r sommenmaken - person fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(person_fit)
```

Gezien zowel de infit en outfit statistieken rond de 95 procent vallen, is er geen reden om aan te nemen dat de personen niet goed passen in het model. De proportie van personen wiens infit statistiek buiten het acceptabele bereik valt, wat groter te zijn (~6%).

#### Item Fit

Hieronder bevinden zich de verschillende item fit indices. Hierbij een korte uitleg wat het exact betekent:

- X2: Dit is de X2-statistiek van de item fit test. Een lager getal duidt op een betere fit. Er is geen vaste cut-off waarde, maar over het algemeen wordt een lagere waarde als beter beschouwd. Als de bijbehorende p-waarde kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het item niet goed past.
- df: de degrees of freedom van het model. Dit is het aantal parameters dat is geschat, minus het aantal constraints dat is opgelegd.
- p-value: de p-waarde van de Goodness of fit test. Als deze kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- RMSEA: RMSEA meet hoe goed een model de waargenomen data benadert, rekening houdend met de fout in het model. Lagere waarden duiden op een betere pasvorm, typisch onder 0.05 voor goede pasvorm en tussen 0.05 en 0.08 voor redelijke pasvorm.
 
```{r sommenmaken - item fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(item_fit)
```

Op basis van de resultaten, blijken meerdere vragen niet goed te passen (zie tabel: vragen met een p lager dan .05).

Dit betekent dat deze waardes niet goed passen in het 1-factor IRT model.Als bepaalde waarden niet goed passen in dit model, kan dit verschillende dingen betekenen.
Ten eerste kan het zijn dat de items niet allemaal dezelfde onderliggende eigenschap meten. Ten tweede kan het zijn dat sommige items verkeerd worden begrepen door de respondenten.
Ten derde kan het zijn dat de veronderstelling van lokale onafhankelijkheid wordt geschonden. Dit betekent dat de reacties op de items afhankelijk zijn van elkaar, zelfs na controle voor de onderliggende eigenschap. Bijvoorbeeld, als het beantwoorden van het ene item helpt bij het beantwoorden van een ander item, dan zullen deze items niet goed passen in het model.

Hieronder volgt een robuste variant van de X2 test, namelijk de PV_Q1 test. Hieronder volgt een korte uitleg wat het exact betekent:

De 'PV_Q1'-statistiek is een afgeleide versie van de Q1-statistiek, een chi-kwadraatstatistiek gebruikt in item respons theorie (IRT) modellen om te testen of een item goed past bij het model. 
Een hoge 'PV_Q1'-waarde duidt op een slechte overeenkomst tussen de waargenomen gegevens en voorspellingen van het model, wat suggereert dat het item mogelijk niet conform het verwachte gedrag 
volgens het IRT-model functioneert. Een niet-significante p-waarde, meestal p > 0.05, in combinatie met lage 'PV_Q1'- en RMSEA.PV_Q1-waarden, geeft aan dat het item goed past bij het IRT-model.
Daarnaast is het belangrijk op te merken dat de 'PV_Q1'-statistiek robuust is tegen invloeden van steekproefomvang en lokale afhankelijkheid, waardoor het een betrouwbaar instrument is om de pasvorm
van een item met het IRT-model te beoordelen, ongeacht de omvang van de steekproef of eventuele lokale onderlinge afhankelijkheden tussen de waarnemingen. Hieronder de resultaten:

```{r sommenmaken - robust item fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(robust_item_fit)
```

Om een beter beeld te krijgen, volgen hieronder the item parameters van de vragen.

```{r sommenmaken - item parameters, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(params)
```

Op basis van de factor loadings, zijn er geen vragen die slecht laaden. 

In IRT, meet een factor loading (ook bekend als een discriminatieparameter) hoe goed een item onderscheid maakt tussen personen met verschillende niveaus van de onderliggende eigenschap. Een hoge factor loading betekent dat het item gevoelig is voor verschillen in de onderliggende eigenschap.
Het belangrijkste verschil met factor analyse, is dat in IRT, de factor loadings ook rekening houden met de moeilijkheidsgraad van de items. Dit betekent dat items die dezelfde moeilijkheidsgraad hebben (dat wil zeggen, ze zijn even moeilijk om te beantwoorden) worden geacht dezelfde hoeveelheid informatie te geven over de onderliggende eigenschap, ongeacht hun factor loadings. Dit is niet het geval in factoranalyse, waar de factor loadings alleen de relatie tussen de geobserveerde variabelen en de onderliggende factor meten.
Tevens kan het zijn dat de factor loadings en de item parameters niet overeenkomen. Bijvoorbeeld een item kan een hoge factor loading hebben (wat aangeeft dat het goed discrimineert) maar toch een slechte fit hebben (X2-statistiek) als het bijvoorbeeld verkeerd wordt begrepen door de respondenten. Omgekeerd kan een item goed passen bij het model maar niet goed discrimineren tussen verschillende niveaus van de eigenschap.

##### Item Characteristic Curves

Hieronder bevinden zich de item characteristic curves. Hierbij een korte uitleg wat het exact betekent:

- De lijnen geven de item characteristic curves weer. Dit is de kans dat een persoon met een bepaald niveau van de onderliggende eigenschap het item correct beantwoordt. Hoe hoger de curve, hoe groter de kans dat een persoon met een bepaald niveau van de onderliggende eigenschap het item correct beantwoordt.
- De x-as van de curve vertegenwoordigt het niveau van de onderliggende eigenschap (bijvoorbeeld vaardigheid of kennis) die een persoon heeft, terwijl de y-as de kans vertegenwoordigt dat een persoon het item correct beantwoordt.
- Hoe stijler de ICC-lijn is, hoe beter het item onderscheid maakt tussen personen met verschillende niveaus van de onderliggende eigenschap. Dit betekent dat het item gevoelig is voor verschillen in de onderliggende eigenschap.
- Hoe hoger het y-axale middelpunt, hoe moeilijker het item is. 

De grafieken voor de ICC en IIC hernoemen de items op volgorde die het IRT model aanbied. Hieronder een tabel met de originele namen van de items, en de bijbehorende item nummers.

```{r sommenmaken - ICC, echo=FALSE, message=FALSE, warning=FALSE}
params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
item_names <- extract.mirt(fit, what = "itemnames")

df <- data.frame(
  item_number = paste0("Item ", seq_along(item_names)),
  item_names = item_names,
  item_discrimination = params$items.a,
  item_difficulty = params$items.b
)
knitr::kable(df)
```


![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/sommenmaken_ICC_combined.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/sommenmaken_ICC_separate.png)

##### Item Information Curves

Hieronder bevinden zich de item information curves. Hierbij een korte uitleg wat het exact betekent:

- De lijnen geven de item information curves weer. Dit is de hoeveelheid informatie die een item geeft over de onderliggende eigenschap. Hoe hoger de curve, hoe meer informatie het item geeft over de onderliggende eigenschap.
- Hoe verder naar rechts de piek van de curve ligt, hoe meer informatie het item geeft over hooge niveaus van de onderliggende eigenschap. 
- Hoe verder naar links de piek van de curve ligt, hoe meer informatie het item geeft over lage niveaus van de onderliggende eigenschap.

De grafieken voor de ICC en IIC hernoemen de items op volgorde die het IRT model aanbied. Hieronder een tabel met de originele namen van de items, en de bijbehorende item nummers.

```{r sommenmaken - IIC, echo=FALSE, message=FALSE, warning=FALSE}
params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
item_names <- extract.mirt(fit, what = "itemnames")

df <- data.frame(
  item_number = paste0("Item ", seq_along(item_names)),
  item_names = item_names,
  item_discrimination = params$items.a,
  item_difficulty = params$items.b
)
knitr::kable(df)
```


![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/sommenmaken_IIC_combined.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/sommenmaken_IIC_separate.png)

##### Item Person Map

Hieronder bevindt zich de item person map. Hierbij een korte uitleg wat het exact betekent:

- De x-as van de grafiek vertegenwoordigt het niveau van de onderliggende eigenschap (bijvoorbeeld vaardigheid of kennis) die een persoon heeft, terwijl de y-as de hoeveelheid participanten vertegenwoordigt.
- Onder de grafiek kan bekeken worden welke vraag ongeveer bij welke hoogte van onderliggende eigenschap past, en hoeveel leerlingen hier daadwerkelijk bij horen.

![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/sommenmaken_item_person_fit.png)

##### Test Characteristic and Information Curves

Gelijk aan de Item characteristics en information curves, maar dan voor de test als geheel. Hierbij een korte uitleg wat het exact betekent:

- De lijnen geven de test characteristic curves weer. Deze geeft aan hoe goed de test informatie geeft over de onderliggende eigenschap. Hoe hoger de curve, hoe meer informatie de test geeft over de onderliggende eigenschap op dat niveau.
- Grofweg kunnen de lijnen en de curve gezien woren als een amalgamatie van de onderliggende item characteristic en information curves.

![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/sommenmaken_TCC.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/sommenmaken_TIC.png)

### Spelling

```{r spelling - stat extraction, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
# load data
fit <- readRDS(paste0(here(), "/data/irt_analyses2024/irt_fit_spelling.rds"))
data <- read.csv(paste0(here(), "/data/data_interrim/spelling.csv"))

# Model fit
model_fit <- as.data.frame(M2(fit, na.rm = TRUE))
model_fit <- cbind(model_fit, nrow(data))
model_fit <- rename(model_fit, `N` = `nrow(data)`)
model_fit <- tidyr::pivot_longer(model_fit, cols = -N, names_to = "Fit Index", values_to = "Value")
model_fit <- dplyr::select(model_fit, `Fit Index`, Value)
model_fit$`Cut-Off` <- c("", "", "> .05", "< .05 | .05 - .1 | > .1", "", "", "< .08 | .08 - .10 | > .10", "> .95 | .90 - .95 | < .90", "> .95 | .90 - .95 | < .90")

# item fit
item_fit <- itemfit(fit, na.rm = TRUE)
colnames(item_fit) <- c("Item", "X2", "df", "RMSEA", "p")
item_fit <- item_fit[order(item_fit$X2), ]
rownames(item_fit) <- NULL

robust_item_fit <- itemfit(fit, na.rm = TRUE)
colnames(robust_item_fit) <- c("Item", "PV_Q1", "df", "RMSEA", "p")
robust_item_fit <- robust_item_fit[order(robust_item_fit$PV_Q1), ]
rownames(robust_item_fit) <- NULL


# Item parameters
params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
params <- params[, -which(colnames(params) %in% c("items.g", "items.u", "means", "F1"))]
sink("/dev/null")
sum <- suppressWarnings(suppressMessages(summary(fit, silent = TRUE)))
sink()
params <- cbind(params, sum$rotF)
params <- cbind(params, sum$h2)
params <- params[, c("F1", colnames(params)[-which(colnames(params) %in% "F1")])]
colnames(params) <- c("Factor Loading", "Discrimination Parameter", "Difficulty Parameter ", "Communalities")
params <- params[order(params$`Factor Loading`, decreasing = TRUE), ]


# # Differential Item Functionining DIF
# # Prepare the data
# gender <- data$gender
# gender <- dplyr::recode(gender, "Man" = 1, "Vrouw" = 2, "Onbekend" = 2)
# gender <- as.factor(gender)
# local_df <- data[, !grepl("package_duration_raw|student_number|student_name|birth_date|gender", colnames(data))]

# # Fit the multiple group model
# dif_fit <- mirt::multipleGroup(
#   data = local_df,
#   model = 1,
#   group = gender
# )

# # Perform DIF analysis
# dif <- mirt::DIF(dif_fit, group = gender, which.par = c("a1", "d"), na.rm = TRUE, plotdif = TRUE)

# Person fit
person_fit <- personfit(fit) %>%
  reframe(
    infit.outside = prop.table(table(z.infit > 1.96 | z.infit < -1.96)),
    outfit.outside = prop.table(table(z.outfit > 1.96 | z.outfit < -1.96))
  ) # lower row = non-fitting people
person_fit$Correct <- c("Correct", "Incorrect")
person_fit <- person_fit %>% dplyr::select(Correct, everything())
```

#### Model Fit

Hieronder bevinden zich de verschillende model fit indices.Hierbij een korte uitleg wat het exact betekent:

- M2: Dit is de M2-statistiek van Maydeu-Olivares, die een maat is voor de absolute fit van het model. Een lager getal duidt op een betere fit. Er is geen vaste cut-off waarde, maar over het algemeen wordt een lagere waarde als beter beschouwd. Als de bijbehorende p-waarde kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- df: de degrees of freedom van het model. Dit is het aantal parameters dat is geschat, minus het aantal constraints dat is opgelegd.
- p-value: de p-waarde van de Goodness of fit test. Als deze kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- RMSEA: RMSEA meet hoe goed een model de waargenomen data benadert, rekening houdend met de fout in het model. Lagere waarden duiden op een betere pasvorm, typisch onder 0.05 voor goede pasvorm en tussen 0.05 en 0.08 voor redelijke pasvorm.
  - RMSEA_5 en RMSEA_95: de 95% betrouwbaarheidsinterval van de RMSEA. Hoe kleiner dit interval rondom de RMSEA waarde, hoe zekerder we zijn van die waarde. 
- SRMSR: SRMSR staat voor "Standardized Root Mean Square Residual". Het is een maat voor de gemiddelde discrepantie tussen de waargenomen en voorspelde covarianties. Een lagere waarde duidt op een betere fit. Een SRMSR van minder dan 0.08 wordt over het algemeen als goed beschouwd.
- TLI: de Tucker-Lewis Index. TLI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.
- CFI: de Comparative Fit Index. De CFI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.
- N: het aantal respondenten in de steekproef.

```{r spelling - model fit, echo=FALSE, message=FALSE, warning=FALSE}
model_fit$`Conclusion` <- c("", "", "Poor", "Good", "", "", "Good", "Good", "Good")
knitr::kable(model_fit, digits = 3)
```

Als we kijken naar de fit indices, zien dat spelling een relatiev goeie fit. Alle waardes zijn goed of acceptabel.
Gezien de hoge hoeveelheid observaties (N = `r nrow(data)`), kan de p-waarde als onbetrouwbaar zijn, gezien diens gevoeligheid voor sample size. Daarom wordt de p-waarde genegeerd, en wordt er geconcludeerd dat het model goed past.

#### Person Fit

Hieronder zijn de percentages van person-item-fit indices weergegeven. Hierbij een korte uitleg wat het exact betekent:
- infit.outside: De infit (information-weighted fit) statistiek is gevoelig voor onverwacht gedrag dat invloed heeft op de antwoorden op items die de meeste informatie geven over het vaardigheidsniveau van de persoon. Met andere woorden, het is gevoeliger voor onverwachte antwoorden op items die dicht bij het vaardigheidsniveau van de persoon liggen. De infit.outside waarde in de tabel vertegenwoordigt de proportie van personen wiens infit statistiek buiten het acceptabele bereik valt, wat duidt op een slechte fit.
- outfit.outside: De outfit (outlier-sensitive fit) statistiek is gevoelig voor onverwacht gedrag dat invloed heeft op de antwoorden op items die de minste informatie geven over het vaardigheidsniveau van de persoon. Met andere woorden, het is gevoeliger voor onverwachte antwoorden op items die ver van het vaardigheidsniveau van de persoon liggen. De outfit.outside waarde in de tabel vertegenwoordigt de proportie van personen wiens outfit statistiek buiten het acceptabele bereik valt, wat duidt op een slechte fit.

```{r spelling - person fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(person_fit)
```

Gezien zowel de infit en outfit statistieken rond de 95 procent vallen, is er geen reden om aan te nemen dat de personen niet goed passen in het model. De proportie van personen wiens infit statistiek buiten het acceptabele bereik valt, wat groter te zijn (~6%).

Hieronder in de grafiek is te zien welke vragen in hoeverre bijdragen aan de infit en outfit waardes:

![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/spelling_item_fit.png)

#### Item Fit

Hieronder bevinden zich de verschillende item fit indices. Hierbij een korte uitleg wat het exact betekent:

- X2: Dit is de X2-statistiek van de item fit test. Een lager getal duidt op een betere fit. Er is geen vaste cut-off waarde, maar over het algemeen wordt een lagere waarde als beter beschouwd. Als de bijbehorende p-waarde kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het item niet goed past.
- df: de degrees of freedom van het model. Dit is het aantal parameters dat is geschat, minus het aantal constraints dat is opgelegd.
- p-value: de p-waarde van de Goodness of fit test. Als deze kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- RMSEA: RMSEA meet hoe goed een model de waargenomen data benadert, rekening houdend met de fout in het model. Lagere waarden duiden op een betere pasvorm, typisch onder 0.05 voor goede pasvorm en tussen 0.05 en 0.08 voor redelijke pasvorm.
 
```{r spelling - item fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(item_fit)
```

Op basis van de resultaten, blijken meerdere vragen niet goed te passen (zie tabel: vragen met een p lager dan .05).

Dit betekent dat deze waardes niet goed passen in het 1-factor IRT model.Als bepaalde waarden niet goed passen in dit model, kan dit verschillende dingen betekenen.
Ten eerste kan het zijn dat de items niet allemaal dezelfde onderliggende eigenschap meten. Ten tweede kan het zijn dat sommige items verkeerd worden begrepen door de respondenten.
Ten derde kan het zijn dat de veronderstelling van lokale onafhankelijkheid wordt geschonden. Dit betekent dat de reacties op de items afhankelijk zijn van elkaar, zelfs na controle voor de onderliggende eigenschap. Bijvoorbeeld, als het beantwoorden van het ene item helpt bij het beantwoorden van een ander item, dan zullen deze items niet goed passen in het model.

Hieronder volgt een robuste variant van de X2 test, namelijk de PV_Q1 test. Hieronder volgt een korte uitleg wat het exact betekent:

De 'PV_Q1'-statistiek is een afgeleide versie van de Q1-statistiek, een chi-kwadraatstatistiek gebruikt in item respons theorie (IRT) modellen om te testen of een item goed past bij het model. 
Een hoge 'PV_Q1'-waarde duidt op een slechte overeenkomst tussen de waargenomen gegevens en voorspellingen van het model, wat suggereert dat het item mogelijk niet conform het verwachte gedrag 
volgens het IRT-model functioneert. Een niet-significante p-waarde, meestal p > 0.05, in combinatie met lage 'PV_Q1'- en RMSEA.PV_Q1-waarden, geeft aan dat het item goed past bij het IRT-model.
Daarnaast is het belangrijk op te merken dat de 'PV_Q1'-statistiek robuust is tegen invloeden van steekproefomvang en lokale afhankelijkheid, waardoor het een betrouwbaar instrument is om de pasvorm
van een item met het IRT-model te beoordelen, ongeacht de omvang van de steekproef of eventuele lokale onderlinge afhankelijkheden tussen de waarnemingen. Hieronder de resultaten:

```{r spelling - robust item fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(robust_item_fit)
```

Om een beter beeld te krijgen, volgen hieronder the item parameters van de vragen.

```{r spelling - item parameters, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(params)
```

Op basis van de factor loadings, zijn er meerdere vragen die slecht laden:

- ASL7_taalverzorging_SP_2F_028  : 0.2962259
- ASL7_taalverzorging_SP_2F_029  : 0.2720349
- ET2020_taalverzorging_SP_3F_139: 0.2429672
- ET2020_taalverzorging_SP_3F_118: 0.2241707
- ET2020_taalverzorging_SP_1F_178: 0.2237742
- ASL7_taalverzorging_SP_2F_031  : 0.1889426
- ET2020_taalverzorging_SP_3F_182: 0.1689964
- ET2020_taalverzorging_SP_3F_150: 0.0039184

Er wordt op geattendeerd dat de laatste vraag (ET2020_taalverzorging_SP_3F_150) een extreem hoge moeilijkheidsgraad heeft. Dit betekent dat deze vraag extreem moeilijk is om te beantwoorden. Dit kan betekenen dat de vraag niet goed is geformuleerd, of dat de vraag niet goed is begrepen door de respondenten.

In IRT, meet een factor loading (ook bekend als een discriminatieparameter) hoe goed een item onderscheid maakt tussen personen met verschillende niveaus van de onderliggende eigenschap. Een hoge factor loading betekent dat het item gevoelig is voor verschillen in de onderliggende eigenschap.
Het belangrijkste verschil met factor analyse, is dat in IRT, de factor loadings ook rekening houden met de moeilijkheidsgraad van de items. Dit betekent dat items die dezelfde moeilijkheidsgraad hebben (dat wil zeggen, ze zijn even moeilijk om te beantwoorden) worden geacht dezelfde hoeveelheid informatie te geven over de onderliggende eigenschap, ongeacht hun factor loadings. Dit is niet het geval in factoranalyse, waar de factor loadings alleen de relatie tussen de geobserveerde variabelen en de onderliggende factor meten.
Tevens kan het zijn dat de factor loadings en de item parameters niet overeenkomen. Bijvoorbeeld een item kan een hoge factor loading hebben (wat aangeeft dat het goed discrimineert) maar toch een slechte fit hebben (X2-statistiek) als het bijvoorbeeld verkeerd wordt begrepen door de respondenten. Omgekeerd kan een item goed passen bij het model maar niet goed discrimineren tussen verschillende niveaus van de eigenschap.

##### Item Characteristic Curves

Hieronder bevinden zich de item characteristic curves. Hierbij een korte uitleg wat het exact betekent:

- De lijnen geven de item characteristic curves weer. Dit is de kans dat een persoon met een bepaald niveau van de onderliggende eigenschap het item correct beantwoordt. Hoe hoger de curve, hoe groter de kans dat een persoon met een bepaald niveau van de onderliggende eigenschap het item correct beantwoordt.
- De x-as van de curve vertegenwoordigt het niveau van de onderliggende eigenschap (bijvoorbeeld vaardigheid of kennis) die een persoon heeft, terwijl de y-as de kans vertegenwoordigt dat een persoon het item correct beantwoordt.
- Hoe stijler de ICC-lijn is, hoe beter het item onderscheid maakt tussen personen met verschillende niveaus van de onderliggende eigenschap. Dit betekent dat het item gevoelig is voor verschillen in de onderliggende eigenschap.
- Hoe hoger het y-axale middelpunt, hoe moeilijker het item is. 

De grafieken voor de ICC en IIC hernoemen de items op volgorde die het IRT model aanbied. Hieronder een tabel met de originele namen van de items, en de bijbehorende item nummers.

```{r spelling - ICC, echo=FALSE, message=FALSE, warning=FALSE}
params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
item_names <- extract.mirt(fit, what = "itemnames")

df <- data.frame(
  item_number = paste0("Item ", seq_along(item_names)),
  item_names = item_names,
  item_discrimination = params$items.a,
  item_difficulty = params$items.b
)
knitr::kable(df)
```


![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/spelling_ICC_combined.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/spelling_ICC_separate.png)

##### Item Information Curves

Hieronder bevinden zich de item information curves. Hierbij een korte uitleg wat het exact betekent:

- De lijnen geven de item information curves weer. Dit is de hoeveelheid informatie die een item geeft over de onderliggende eigenschap. Hoe hoger de curve, hoe meer informatie het item geeft over de onderliggende eigenschap.
- Hoe verder naar rechts de piek van de curve ligt, hoe meer informatie het item geeft over hooge niveaus van de onderliggende eigenschap. 
- Hoe verder naar links de piek van de curve ligt, hoe meer informatie het item geeft over lage niveaus van de onderliggende eigenschap.

De grafieken voor de ICC en IIC hernoemen de items op volgorde die het IRT model aanbied. Hieronder een tabel met de originele namen van de items, en de bijbehorende item nummers.

```{r spelling - IIC, echo=FALSE, message=FALSE, warning=FALSE}
params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
item_names <- extract.mirt(fit, what = "itemnames")

df <- data.frame(
  item_number = paste0("Item ", seq_along(item_names)),
  item_names = item_names,
  item_discrimination = params$items.a,
  item_difficulty = params$items.b
)
knitr::kable(df)
```


![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/spelling_IIC_combined.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/spelling_IIC_separate.png)

##### Item Person Map

Hieronder bevindt zich de item person map. Hierbij een korte uitleg wat het exact betekent:

- De x-as van de grafiek vertegenwoordigt het niveau van de onderliggende eigenschap (bijvoorbeeld vaardigheid of kennis) die een persoon heeft, terwijl de y-as de hoeveelheid participanten vertegenwoordigt.
- Onder de grafiek kan bekeken worden welke vraag ongeveer bij welke hoogte van onderliggende eigenschap past, en hoeveel leerlingen hier daadwerkelijk bij horen.

![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/spelling_item_person_fit.png)

##### Test Characteristic and Information Curves

Gelijk aan de Item characteristics en information curves, maar dan voor de test als geheel. Hierbij een korte uitleg wat het exact betekent:

- De lijnen geven de test characteristic curves weer. Deze geeft aan hoe goed de test informatie geeft over de onderliggende eigenschap. Hoe hoger de curve, hoe meer informatie de test geeft over de onderliggende eigenschap op dat niveau.
- Grofweg kunnen de lijnen en de curve gezien woren als een amalgamatie van de onderliggende item characteristic en information curves.

![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/spelling_TCC.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/spelling_TIC.png)

### Verbanden

```{r verbanden - stat extraction, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
# load data
fit <- readRDS(paste0(here(), "/data/irt_analyses2024/irt_fit_verbanden.rds"))
data <- read.csv(paste0(here(), "/data/data_interrim/verbanden.csv"))

# Model fit
model_fit <- as.data.frame(M2(fit, na.rm = TRUE))
model_fit <- cbind(model_fit, nrow(data))
model_fit <- rename(model_fit, `N` = `nrow(data)`)
model_fit <- tidyr::pivot_longer(model_fit, cols = -N, names_to = "Fit Index", values_to = "Value")
model_fit <- dplyr::select(model_fit, `Fit Index`, Value)
model_fit$`Cut-Off` <- c("", "", "> .05", "< .05 | .05 - .1 | > .1", "", "", "< .08 | .08 - .10 | > .10", "> .95 | .90 - .95 | < .90", "> .95 | .90 - .95 | < .90")

# item fit
item_fit <- itemfit(fit, na.rm = TRUE)
colnames(item_fit) <- c("Item", "X2", "df", "RMSEA", "p")
item_fit <- item_fit[order(item_fit$X2), ]
rownames(item_fit) <- NULL

robust_item_fit <- itemfit(fit, na.rm = TRUE)
colnames(robust_item_fit) <- c("Item", "PV_Q1", "df", "RMSEA", "p")
robust_item_fit <- robust_item_fit[order(robust_item_fit$PV_Q1), ]
rownames(robust_item_fit) <- NULL


# Item parameters
params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
params <- params[, -which(colnames(params) %in% c("items.g", "items.u", "means", "F1"))]
sink("/dev/null")
sum <- suppressWarnings(suppressMessages(summary(fit, silent = TRUE)))
sink()
params <- cbind(params, sum$rotF)
params <- cbind(params, sum$h2)
params <- params[, c("F1", colnames(params)[-which(colnames(params) %in% "F1")])]
colnames(params) <- c("Factor Loading", "Discrimination Parameter", "Difficulty Parameter ", "Communalities")
params <- params[order(params$`Factor Loading`, decreasing = TRUE), ]


# # Differential Item Functionining DIF
# # Prepare the data
# gender <- data$gender
# gender <- dplyr::recode(gender, "Man" = 1, "Vrouw" = 2, "Onbekend" = 2)
# gender <- as.factor(gender)
# local_df <- data[, !grepl("package_duration_raw|student_number|student_name|birth_date|gender", colnames(data))]

# # Fit the multiple group model
# dif_fit <- mirt::multipleGroup(
#   data = local_df,
#   model = 1,
#   group = gender
# )

# # Perform DIF analysis
# dif <- mirt::DIF(dif_fit, group = gender, which.par = c("a1", "d"), na.rm = TRUE, plotdif = TRUE)

# Person fit
person_fit <- personfit(fit) %>%
  reframe(
    infit.outside = prop.table(table(z.infit > 1.96 | z.infit < -1.96)),
    outfit.outside = prop.table(table(z.outfit > 1.96 | z.outfit < -1.96))
  ) # lower row = non-fitting people
person_fit$Correct <- c("Correct", "Incorrect")
person_fit <- person_fit %>% dplyr::select(Correct, everything())
```

#### Model Fit

Hieronder bevinden zich de verschillende model fit indices.Hierbij een korte uitleg wat het exact betekent:

- M2: Dit is de M2-statistiek van Maydeu-Olivares, die een maat is voor de absolute fit van het model. Een lager getal duidt op een betere fit. Er is geen vaste cut-off waarde, maar over het algemeen wordt een lagere waarde als beter beschouwd. Als de bijbehorende p-waarde kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- df: de degrees of freedom van het model. Dit is het aantal parameters dat is geschat, minus het aantal constraints dat is opgelegd.
- p-value: de p-waarde van de Goodness of fit test. Als deze kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- RMSEA: RMSEA meet hoe goed een model de waargenomen data benadert, rekening houdend met de fout in het model. Lagere waarden duiden op een betere pasvorm, typisch onder 0.05 voor goede pasvorm en tussen 0.05 en 0.08 voor redelijke pasvorm.
  - RMSEA_5 en RMSEA_95: de 95% betrouwbaarheidsinterval van de RMSEA. Hoe kleiner dit interval rondom de RMSEA waarde, hoe zekerder we zijn van die waarde. 
- SRMSR: SRMSR staat voor "Standardized Root Mean Square Residual". Het is een maat voor de gemiddelde discrepantie tussen de waargenomen en voorspelde covarianties. Een lagere waarde duidt op een betere fit. Een SRMSR van minder dan 0.08 wordt over het algemeen als goed beschouwd.
- TLI: de Tucker-Lewis Index. TLI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.
- CFI: de Comparative Fit Index. De CFI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.
- N: het aantal respondenten in de steekproef.

```{r verbanden - model fit, echo=FALSE, message=FALSE, warning=FALSE}
model_fit$`Conclusion` <- c("", "", "Poor", "Good", "", "", "Good", "Good", "Good")
knitr::kable(model_fit, digits = 3)
```

Als we kijken naar de fit indices, zien dat verbanden een relatiev goeie fit. Alle waardes zijn goed of acceptabel.
Gezien de hoge hoeveelheid observaties (N = `r nrow(data)`), kan de p-waarde als onbetrouwbaar zijn, gezien diens gevoeligheid voor sample size. Daarom wordt de p-waarde genegeerd, en wordt er geconcludeerd dat het model goed past.

#### Person Fit

Hieronder zijn de percentages van person-item-fit indices weergegeven. Hierbij een korte uitleg wat het exact betekent:
- infit.outside: De infit (information-weighted fit) statistiek is gevoelig voor onverwacht gedrag dat invloed heeft op de antwoorden op items die de meeste informatie geven over het vaardigheidsniveau van de persoon. Met andere woorden, het is gevoeliger voor onverwachte antwoorden op items die dicht bij het vaardigheidsniveau van de persoon liggen. De infit.outside waarde in de tabel vertegenwoordigt de proportie van personen wiens infit statistiek buiten het acceptabele bereik valt, wat duidt op een slechte fit.
- outfit.outside: De outfit (outlier-sensitive fit) statistiek is gevoelig voor onverwacht gedrag dat invloed heeft op de antwoorden op items die de minste informatie geven over het vaardigheidsniveau van de persoon. Met andere woorden, het is gevoeliger voor onverwachte antwoorden op items die ver van het vaardigheidsniveau van de persoon liggen. De outfit.outside waarde in de tabel vertegenwoordigt de proportie van personen wiens outfit statistiek buiten het acceptabele bereik valt, wat duidt op een slechte fit.

```{r verbanden - person fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(person_fit)
```

Gezien zowel de infit en outfit statistieken rond de 95 procent vallen, is er geen reden om aan te nemen dat de personen niet goed passen in het model. De proportie van personen wiens infit statistiek buiten het acceptabele bereik valt, wat groter te zijn (~6%).

Hieronder in de grafiek is te zien welke vragen in hoeverre bijdragen aan de infit en outfit waardes:

![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/verbanden_item_fit.png)

#### Item Fit

Hieronder bevinden zich de verschillende item fit indices. Hierbij een korte uitleg wat het exact betekent:

- X2: Dit is de X2-statistiek van de item fit test. Een lager getal duidt op een betere fit. Er is geen vaste cut-off waarde, maar over het algemeen wordt een lagere waarde als beter beschouwd. Als de bijbehorende p-waarde kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het item niet goed past.
- df: de degrees of freedom van het model. Dit is het aantal parameters dat is geschat, minus het aantal constraints dat is opgelegd.
- p-value: de p-waarde van de Goodness of fit test. Als deze kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- RMSEA: RMSEA meet hoe goed een model de waargenomen data benadert, rekening houdend met de fout in het model. Lagere waarden duiden op een betere pasvorm, typisch onder 0.05 voor goede pasvorm en tussen 0.05 en 0.08 voor redelijke pasvorm.
 
```{r verbanden - item fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(item_fit)
```

Op basis van de resultaten, blijken meerdere vragen niet goed te passen (zie tabel: vragen met een p lager dan .05).

Dit betekent dat deze waardes niet goed passen in het 1-factor IRT model.Als bepaalde waarden niet goed passen in dit model, kan dit verschillende dingen betekenen.
Ten eerste kan het zijn dat de items niet allemaal dezelfde onderliggende eigenschap meten. Ten tweede kan het zijn dat sommige items verkeerd worden begrepen door de respondenten.
Ten derde kan het zijn dat de veronderstelling van lokale onafhankelijkheid wordt geschonden. Dit betekent dat de reacties op de items afhankelijk zijn van elkaar, zelfs na controle voor de onderliggende eigenschap. Bijvoorbeeld, als het beantwoorden van het ene item helpt bij het beantwoorden van een ander item, dan zullen deze items niet goed passen in het model.

Hieronder volgt een robuste variant van de X2 test, namelijk de PV_Q1 test. Hieronder volgt een korte uitleg wat het exact betekent:

De 'PV_Q1'-statistiek is een afgeleide versie van de Q1-statistiek, een chi-kwadraatstatistiek gebruikt in item respons theorie (IRT) modellen om te testen of een item goed past bij het model. 
Een hoge 'PV_Q1'-waarde duidt op een slechte overeenkomst tussen de waargenomen gegevens en voorspellingen van het model, wat suggereert dat het item mogelijk niet conform het verwachte gedrag 
volgens het IRT-model functioneert. Een niet-significante p-waarde, meestal p > 0.05, in combinatie met lage 'PV_Q1'- en RMSEA.PV_Q1-waarden, geeft aan dat het item goed past bij het IRT-model.
Daarnaast is het belangrijk op te merken dat de 'PV_Q1'-statistiek robuust is tegen invloeden van steekproefomvang en lokale afhankelijkheid, waardoor het een betrouwbaar instrument is om de pasvorm
van een item met het IRT-model te beoordelen, ongeacht de omvang van de steekproef of eventuele lokale onderlinge afhankelijkheden tussen de waarnemingen. Hieronder de resultaten:

```{r verbanden - robust item fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(robust_item_fit)
```

Om een beter beeld te krijgen, volgen hieronder the item parameters van de vragen.

```{r verbanden - item parameters, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(params)
```

Op basis van de factor loadings, blijkt slechts een vraag slecht te laden:

- ASL7_rekenen_VB_C_1F_010: 0.2658907

In IRT, meet een factor loading (ook bekend als een discriminatieparameter) hoe goed een item onderscheid maakt tussen personen met verschillende niveaus van de onderliggende eigenschap. Een hoge factor loading betekent dat het item gevoelig is voor verschillen in de onderliggende eigenschap.
Het belangrijkste verschil met factor analyse, is dat in IRT, de factor loadings ook rekening houden met de moeilijkheidsgraad van de items. Dit betekent dat items die dezelfde moeilijkheidsgraad hebben (dat wil zeggen, ze zijn even moeilijk om te beantwoorden) worden geacht dezelfde hoeveelheid informatie te geven over de onderliggende eigenschap, ongeacht hun factor loadings. Dit is niet het geval in factoranalyse, waar de factor loadings alleen de relatie tussen de geobserveerde variabelen en de onderliggende factor meten.
Tevens kan het zijn dat de factor loadings en de item parameters niet overeenkomen. Bijvoorbeeld een item kan een hoge factor loading hebben (wat aangeeft dat het goed discrimineert) maar toch een slechte fit hebben (X2-statistiek) als het bijvoorbeeld verkeerd wordt begrepen door de respondenten. Omgekeerd kan een item goed passen bij het model maar niet goed discrimineren tussen verschillende niveaus van de eigenschap.

##### Item Characteristic Curves

Hieronder bevinden zich de item characteristic curves. Hierbij een korte uitleg wat het exact betekent:

- De lijnen geven de item characteristic curves weer. Dit is de kans dat een persoon met een bepaald niveau van de onderliggende eigenschap het item correct beantwoordt. Hoe hoger de curve, hoe groter de kans dat een persoon met een bepaald niveau van de onderliggende eigenschap het item correct beantwoordt.
- De x-as van de curve vertegenwoordigt het niveau van de onderliggende eigenschap (bijvoorbeeld vaardigheid of kennis) die een persoon heeft, terwijl de y-as de kans vertegenwoordigt dat een persoon het item correct beantwoordt.
- Hoe stijler de ICC-lijn is, hoe beter het item onderscheid maakt tussen personen met verschillende niveaus van de onderliggende eigenschap. Dit betekent dat het item gevoelig is voor verschillen in de onderliggende eigenschap.
- Hoe hoger het y-axale middelpunt, hoe moeilijker het item is. 

De grafieken voor de ICC en IIC hernoemen de items op volgorde die het IRT model aanbied. Hieronder een tabel met de originele namen van de items, en de bijbehorende item nummers.

```{r vebanden - ICC, echo=FALSE, message=FALSE, warning=FALSE}
params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
item_names <- extract.mirt(fit, what = "itemnames")

df <- data.frame(
  item_number = paste0("Item ", seq_along(item_names)),
  item_names = item_names,
  item_discrimination = params$items.a,
  item_difficulty = params$items.b
)
knitr::kable(df)
```


![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/verbanden_ICC_combined.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/verbanden_ICC_separate.png)

##### Item Information Curves

Hieronder bevinden zich de item information curves. Hierbij een korte uitleg wat het exact betekent:

- De lijnen geven de item information curves weer. Dit is de hoeveelheid informatie die een item geeft over de onderliggende eigenschap. Hoe hoger de curve, hoe meer informatie het item geeft over de onderliggende eigenschap.
- Hoe verder naar rechts de piek van de curve ligt, hoe meer informatie het item geeft over hooge niveaus van de onderliggende eigenschap. 
- Hoe verder naar links de piek van de curve ligt, hoe meer informatie het item geeft over lage niveaus van de onderliggende eigenschap.

De grafieken voor de ICC en IIC hernoemen de items op volgorde die het IRT model aanbied. Hieronder een tabel met de originele namen van de items, en de bijbehorende item nummers.

```{r verbanden - IIC, echo=FALSE, message=FALSE, warning=FALSE}
params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
item_names <- extract.mirt(fit, what = "itemnames")

df <- data.frame(
  item_number = paste0("Item ", seq_along(item_names)),
  item_names = item_names,
  item_discrimination = params$items.a,
  item_difficulty = params$items.b
)
knitr::kable(df)
```


![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/verbanden_IIC_combined.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/verbanden_IIC_separate.png)

##### Item Person Map

Hieronder bevindt zich de item person map. Hierbij een korte uitleg wat het exact betekent:

- De x-as van de grafiek vertegenwoordigt het niveau van de onderliggende eigenschap (bijvoorbeeld vaardigheid of kennis) die een persoon heeft, terwijl de y-as de hoeveelheid participanten vertegenwoordigt.
- Onder de grafiek kan bekeken worden welke vraag ongeveer bij welke hoogte van onderliggende eigenschap past, en hoeveel leerlingen hier daadwerkelijk bij horen.

![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/verbanden_item_person_fit.png)

##### Test Characteristic and Information Curves

Gelijk aan de Item characteristics en information curves, maar dan voor de test als geheel. Hierbij een korte uitleg wat het exact betekent:

- De lijnen geven de test characteristic curves weer. Deze geeft aan hoe goed de test informatie geeft over de onderliggende eigenschap. Hoe hoger de curve, hoe meer informatie de test geeft over de onderliggende eigenschap op dat niveau.
- Grofweg kunnen de lijnen en de curve gezien woren als een amalgamatie van de onderliggende item characteristic en information curves.

![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/verbanden_TCC.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/verbanden_TIC.png)

### Verhoudingen

```{r verhoudingen - stat extraction, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
# load data
fit <- readRDS(paste0(here(), "/data/irt_analyses2024/irt_fit_verhoudingen.rds"))
data <- read.csv(paste0(here(), "/data/data_interrim/verhoudingen.csv"))

# Model fit
model_fit <- as.data.frame(M2(fit, na.rm = TRUE))
model_fit <- cbind(model_fit, nrow(data))
model_fit <- rename(model_fit, `N` = `nrow(data)`)
model_fit <- tidyr::pivot_longer(model_fit, cols = -N, names_to = "Fit Index", values_to = "Value")
model_fit <- dplyr::select(model_fit, `Fit Index`, Value)
model_fit$`Cut-Off` <- c("", "", "> .05", "< .05 | .05 - .1 | > .1", "", "", "< .08 | .08 - .10 | > .10", "> .95 | .90 - .95 | < .90", "> .95 | .90 - .95 | < .90")

# item fit
item_fit <- itemfit(fit, na.rm = TRUE)
colnames(item_fit) <- c("Item", "X2", "df", "RMSEA", "p")
item_fit <- item_fit[order(item_fit$X2), ]
rownames(item_fit) <- NULL

robust_item_fit <- itemfit(fit, na.rm = TRUE)
colnames(robust_item_fit) <- c("Item", "PV_Q1", "df", "RMSEA", "p")
robust_item_fit <- robust_item_fit[order(robust_item_fit$PV_Q1), ]
rownames(robust_item_fit) <- NULL


# Item parameters
params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
params <- params[, -which(colnames(params) %in% c("items.g", "items.u", "means", "F1"))]
sink("/dev/null")
sum <- suppressWarnings(suppressMessages(summary(fit, silent = TRUE)))
sink()
params <- cbind(params, sum$rotF)
params <- cbind(params, sum$h2)
params <- params[, c("F1", colnames(params)[-which(colnames(params) %in% "F1")])]
colnames(params) <- c("Factor Loading", "Discrimination Parameter", "Difficulty Parameter ", "Communalities")
params <- params[order(params$`Factor Loading`, decreasing = TRUE), ]


# # Differential Item Functionining DIF
# # Prepare the data
# gender <- data$gender
# gender <- dplyr::recode(gender, "Man" = 1, "Vrouw" = 2, "Onbekend" = 2)
# gender <- as.factor(gender)
# local_df <- data[, !grepl("package_duration_raw|student_number|student_name|birth_date|gender", colnames(data))]

# # Fit the multiple group model
# dif_fit <- mirt::multipleGroup(
#   data = local_df,
#   model = 1,
#   group = gender
# )

# # Perform DIF analysis
# dif <- mirt::DIF(dif_fit, group = gender, which.par = c("a1", "d"), na.rm = TRUE, plotdif = TRUE)

# Person fit
person_fit <- personfit(fit) %>%
  reframe(
    infit.outside = prop.table(table(z.infit > 1.96 | z.infit < -1.96)),
    outfit.outside = prop.table(table(z.outfit > 1.96 | z.outfit < -1.96))
  ) # lower row = non-fitting people
person_fit$Correct <- c("Correct", "Incorrect")
person_fit <- person_fit %>% dplyr::select(Correct, everything())
```

#### Model Fit

Hieronder bevinden zich de verschillende model fit indices.Hierbij een korte uitleg wat het exact betekent:

- M2: Dit is de M2-statistiek van Maydeu-Olivares, die een maat is voor de absolute fit van het model. Een lager getal duidt op een betere fit. Er is geen vaste cut-off waarde, maar over het algemeen wordt een lagere waarde als beter beschouwd. Als de bijbehorende p-waarde kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- df: de degrees of freedom van het model. Dit is het aantal parameters dat is geschat, minus het aantal constraints dat is opgelegd.
- p-value: de p-waarde van de Goodness of fit test. Als deze kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- RMSEA: RMSEA meet hoe goed een model de waargenomen data benadert, rekening houdend met de fout in het model. Lagere waarden duiden op een betere pasvorm, typisch onder 0.05 voor goede pasvorm en tussen 0.05 en 0.08 voor redelijke pasvorm.
  - RMSEA_5 en RMSEA_95: de 95% betrouwbaarheidsinterval van de RMSEA. Hoe kleiner dit interval rondom de RMSEA waarde, hoe zekerder we zijn van die waarde. 
- SRMSR: SRMSR staat voor "Standardized Root Mean Square Residual". Het is een maat voor de gemiddelde discrepantie tussen de waargenomen en voorspelde covarianties. Een lagere waarde duidt op een betere fit. Een SRMSR van minder dan 0.08 wordt over het algemeen als goed beschouwd.
- TLI: de Tucker-Lewis Index. TLI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.
- CFI: de Comparative Fit Index. De CFI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.
- N: het aantal respondenten in de steekproef.

```{r verhoudingen - model fit, echo=FALSE, message=FALSE, warning=FALSE}
model_fit$`Conclusion` <- c("", "", "Poor", "Good", "", "", "Good", "Good", "Good")
knitr::kable(model_fit, digits = 3)
```

Als we kijken naar de fit indices, zien dat verhoudingen een relatiev goeie fit. Alle waardes zijn goed of acceptabel.
Gezien de hoge hoeveelheid observaties (N = `r nrow(data)`), kan de p-waarde als onbetrouwbaar zijn, gezien diens gevoeligheid voor sample size. Daarom wordt de p-waarde genegeerd, en wordt er geconcludeerd dat het model goed past.

#### Person Fit

Hieronder zijn de percentages van person-item-fit indices weergegeven. Hierbij een korte uitleg wat het exact betekent:
- infit.outside: De infit (information-weighted fit) statistiek is gevoelig voor onverwacht gedrag dat invloed heeft op de antwoorden op items die de meeste informatie geven over het vaardigheidsniveau van de persoon. Met andere woorden, het is gevoeliger voor onverwachte antwoorden op items die dicht bij het vaardigheidsniveau van de persoon liggen. De infit.outside waarde in de tabel vertegenwoordigt de proportie van personen wiens infit statistiek buiten het acceptabele bereik valt, wat duidt op een slechte fit.
- outfit.outside: De outfit (outlier-sensitive fit) statistiek is gevoelig voor onverwacht gedrag dat invloed heeft op de antwoorden op items die de minste informatie geven over het vaardigheidsniveau van de persoon. Met andere woorden, het is gevoeliger voor onverwachte antwoorden op items die ver van het vaardigheidsniveau van de persoon liggen. De outfit.outside waarde in de tabel vertegenwoordigt de proportie van personen wiens outfit statistiek buiten het acceptabele bereik valt, wat duidt op een slechte fit.

```{r verhoudingen - person fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(person_fit)
```

Gezien zowel de infit en outfit statistieken rond de 95 procent vallen, is er geen reden om aan te nemen dat de personen niet goed passen in het model. De proportie van personen wiens infit statistiek buiten het acceptabele bereik valt, wat groter te zijn (~6%).

Hieronder in de grafiek is te zien welke vragen in hoeverre bijdragen aan de infit en outfit waardes:

![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/verhoudingen_item_fit.png)

#### Item Fit

Hieronder bevinden zich de verschillende item fit indices. Hierbij een korte uitleg wat het exact betekent:

- X2: Dit is de X2-statistiek van de item fit test. Een lager getal duidt op een betere fit. Er is geen vaste cut-off waarde, maar over het algemeen wordt een lagere waarde als beter beschouwd. Als de bijbehorende p-waarde kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het item niet goed past.
- df: de degrees of freedom van het model. Dit is het aantal parameters dat is geschat, minus het aantal constraints dat is opgelegd.
- p-value: de p-waarde van de Goodness of fit test. Als deze kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- RMSEA: RMSEA meet hoe goed een model de waargenomen data benadert, rekening houdend met de fout in het model. Lagere waarden duiden op een betere pasvorm, typisch onder 0.05 voor goede pasvorm en tussen 0.05 en 0.08 voor redelijke pasvorm.
 
```{r verhoudingen - item fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(item_fit)
```

Op basis van de resultaten, blijken meerdere vragen niet goed te passen (zie tabel: vragen met een p lager dan .05).

Dit betekent dat deze waardes niet goed passen in het 1-factor IRT model.Als bepaalde waarden niet goed passen in dit model, kan dit verschillende dingen betekenen.
Ten eerste kan het zijn dat de items niet allemaal dezelfde onderliggende eigenschap meten. Ten tweede kan het zijn dat sommige items verkeerd worden begrepen door de respondenten.
Ten derde kan het zijn dat de veronderstelling van lokale onafhankelijkheid wordt geschonden. Dit betekent dat de reacties op de items afhankelijk zijn van elkaar, zelfs na controle voor de onderliggende eigenschap. Bijvoorbeeld, als het beantwoorden van het ene item helpt bij het beantwoorden van een ander item, dan zullen deze items niet goed passen in het model.

Hieronder volgt een robuste variant van de X2 test, namelijk de PV_Q1 test. Hieronder volgt een korte uitleg wat het exact betekent:

De 'PV_Q1'-statistiek is een afgeleide versie van de Q1-statistiek, een chi-kwadraatstatistiek gebruikt in item respons theorie (IRT) modellen om te testen of een item goed past bij het model. 
Een hoge 'PV_Q1'-waarde duidt op een slechte overeenkomst tussen de waargenomen gegevens en voorspellingen van het model, wat suggereert dat het item mogelijk niet conform het verwachte gedrag 
volgens het IRT-model functioneert. Een niet-significante p-waarde, meestal p > 0.05, in combinatie met lage 'PV_Q1'- en RMSEA.PV_Q1-waarden, geeft aan dat het item goed past bij het IRT-model.
Daarnaast is het belangrijk op te merken dat de 'PV_Q1'-statistiek robuust is tegen invloeden van steekproefomvang en lokale afhankelijkheid, waardoor het een betrouwbaar instrument is om de pasvorm
van een item met het IRT-model te beoordelen, ongeacht de omvang van de steekproef of eventuele lokale onderlinge afhankelijkheden tussen de waarnemingen. Hieronder de resultaten:

```{r verhoudingen - robust item fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(robust_item_fit)
```

Om een beter beeld te krijgen, volgen hieronder the item parameters van de vragen.

```{r verhoudingen - item parameters, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(params)
```

Op basis van de factor loadings, blijkt slechts een vraag slecht te laden:

- ASL7_rekenen_VH_C_2F_036_pilot: -0.0085766

Hierbij wordt erop geattendeerd dat deze vraag een hele lage moeilijkheidsgraad (b = -126.8284639) bevat. Dit betekent dat deze vraag door bijna alle respondenten goed wordt beantwoord. Dit kan een reden zijn waarom deze vraag niet goed past in het model.

In IRT, meet een factor loading (ook bekend als een discriminatieparameter) hoe goed een item onderscheid maakt tussen personen met verschillende niveaus van de onderliggende eigenschap. Een hoge factor loading betekent dat het item gevoelig is voor verschillen in de onderliggende eigenschap.
Het belangrijkste verschil met factor analyse, is dat in IRT, de factor loadings ook rekening houden met de moeilijkheidsgraad van de items. Dit betekent dat items die dezelfde moeilijkheidsgraad hebben (dat wil zeggen, ze zijn even moeilijk om te beantwoorden) worden geacht dezelfde hoeveelheid informatie te geven over de onderliggende eigenschap, ongeacht hun factor loadings. Dit is niet het geval in factoranalyse, waar de factor loadings alleen de relatie tussen de geobserveerde variabelen en de onderliggende factor meten.
Tevens kan het zijn dat de factor loadings en de item parameters niet overeenkomen. Bijvoorbeeld een item kan een hoge factor loading hebben (wat aangeeft dat het goed discrimineert) maar toch een slechte fit hebben (X2-statistiek) als het bijvoorbeeld verkeerd wordt begrepen door de respondenten. Omgekeerd kan een item goed passen bij het model maar niet goed discrimineren tussen verschillende niveaus van de eigenschap.

##### Item Characteristic Curves

Hieronder bevinden zich de item characteristic curves. Hierbij een korte uitleg wat het exact betekent:

- De lijnen geven de item characteristic curves weer. Dit is de kans dat een persoon met een bepaald niveau van de onderliggende eigenschap het item correct beantwoordt. Hoe hoger de curve, hoe groter de kans dat een persoon met een bepaald niveau van de onderliggende eigenschap het item correct beantwoordt.
- De x-as van de curve vertegenwoordigt het niveau van de onderliggende eigenschap (bijvoorbeeld vaardigheid of kennis) die een persoon heeft, terwijl de y-as de kans vertegenwoordigt dat een persoon het item correct beantwoordt.
- Hoe stijler de ICC-lijn is, hoe beter het item onderscheid maakt tussen personen met verschillende niveaus van de onderliggende eigenschap. Dit betekent dat het item gevoelig is voor verschillen in de onderliggende eigenschap.
- Hoe hoger het y-axale middelpunt, hoe moeilijker het item is. 

De grafieken voor de ICC en IIC hernoemen de items op volgorde die het IRT model aanbied. Hieronder een tabel met de originele namen van de items, en de bijbehorende item nummers.

```{r verhoudingen - ICC, echo=FALSE, message=FALSE, warning=FALSE}
params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
item_names <- extract.mirt(fit, what = "itemnames")

df <- data.frame(
  item_number = paste0("Item ", seq_along(item_names)),
  item_names = item_names,
  item_discrimination = params$items.a,
  item_difficulty = params$items.b
)
knitr::kable(df)
```


![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/verhoudingen_ICC_combined.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/verhoudingen_ICC_separate.png)

##### Item Information Curves

Hieronder bevinden zich de item information curves. Hierbij een korte uitleg wat het exact betekent:

- De lijnen geven de item information curves weer. Dit is de hoeveelheid informatie die een item geeft over de onderliggende eigenschap. Hoe hoger de curve, hoe meer informatie het item geeft over de onderliggende eigenschap.
- Hoe verder naar rechts de piek van de curve ligt, hoe meer informatie het item geeft over hooge niveaus van de onderliggende eigenschap. 
- Hoe verder naar links de piek van de curve ligt, hoe meer informatie het item geeft over lage niveaus van de onderliggende eigenschap.

De grafieken voor de ICC en IIC hernoemen de items op volgorde die het IRT model aanbied. Hieronder een tabel met de originele namen van de items, en de bijbehorende item nummers.

```{r verhoudingen - IIC, echo=FALSE, message=FALSE, warning=FALSE}
params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
item_names <- extract.mirt(fit, what = "itemnames")

df <- data.frame(
  item_number = paste0("Item ", seq_along(item_names)),
  item_names = item_names,
  item_discrimination = params$items.a,
  item_difficulty = params$items.b
)
knitr::kable(df)
```


![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/verhoudingen_IIC_combined.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/verhoudingen_IIC_separate.png)

##### Item Person Map

Hieronder bevindt zich de item person map. Hierbij een korte uitleg wat het exact betekent:

- De x-as van de grafiek vertegenwoordigt het niveau van de onderliggende eigenschap (bijvoorbeeld vaardigheid of kennis) die een persoon heeft, terwijl de y-as de hoeveelheid participanten vertegenwoordigt.
- Onder de grafiek kan bekeken worden welke vraag ongeveer bij welke hoogte van onderliggende eigenschap past, en hoeveel leerlingen hier daadwerkelijk bij horen.

![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/verhoudingen_item_person_fit.png)

##### Test Characteristic and Information Curves

Gelijk aan de Item characteristics en information curves, maar dan voor de test als geheel. Hierbij een korte uitleg wat het exact betekent:

- De lijnen geven de test characteristic curves weer. Deze geeft aan hoe goed de test informatie geeft over de onderliggende eigenschap. Hoe hoger de curve, hoe meer informatie de test geeft over de onderliggende eigenschap op dat niveau.
- Grofweg kunnen de lijnen en de curve gezien woren als een amalgamatie van de onderliggende item characteristic en information curves.

![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/verhoudingen_TCC.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/verhoudingen_TIC.png)

### Verschillen

```{r verschillen - stat extraction, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
# load data
fit <- readRDS(paste0(here(), "/data/irt_analyses2024/irt_fit_verschillen.rds"))
data <- read.csv(paste0(here(), "/data/data_interrim/verschillen.csv"))

# Model fit
model_fit <- as.data.frame(M2(fit, na.rm = TRUE))
model_fit <- cbind(model_fit, nrow(data))
model_fit <- rename(model_fit, `N` = `nrow(data)`)
model_fit <- tidyr::pivot_longer(model_fit, cols = -N, names_to = "Fit Index", values_to = "Value")
model_fit <- dplyr::select(model_fit, `Fit Index`, Value)
model_fit$`Cut-Off` <- c("", "", "> .05", "< .05 | .05 - .1 | > .1", "", "", "< .08 | .08 - .10 | > .10", "> .95 | .90 - .95 | < .90", "> .95 | .90 - .95 | < .90")

# item fit
item_fit <- itemfit(fit, na.rm = TRUE)
colnames(item_fit) <- c("Item", "X2", "df", "RMSEA", "p")
item_fit <- item_fit[order(item_fit$X2), ]
rownames(item_fit) <- NULL

robust_item_fit <- itemfit(fit, na.rm = TRUE)
colnames(robust_item_fit) <- c("Item", "PV_Q1", "df", "RMSEA", "p")
robust_item_fit <- robust_item_fit[order(robust_item_fit$PV_Q1), ]
rownames(robust_item_fit) <- NULL


# Item parameters
params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
params <- params[, -which(colnames(params) %in% c("items.g", "items.u", "means", "F1"))]
sink("/dev/null")
sum <- suppressWarnings(suppressMessages(summary(fit, silent = TRUE)))
sink()
params <- cbind(params, sum$rotF)
params <- cbind(params, sum$h2)
params <- params[, c("F1", colnames(params)[-which(colnames(params) %in% "F1")])]
colnames(params) <- c("Factor Loading", "Discrimination Parameter", "Difficulty Parameter ", "Communalities")
params <- params[order(params$`Factor Loading`, decreasing = TRUE), ]


# # Differential Item Functionining DIF
# # Prepare the data
# gender <- data$gender
# gender <- dplyr::recode(gender, "Man" = 1, "Vrouw" = 2, "Onbekend" = 2)
# gender <- as.factor(gender)
# local_df <- data[, !grepl("package_duration_raw|student_number|student_name|birth_date|gender", colnames(data))]

# # Fit the multiple group model
# dif_fit <- mirt::multipleGroup(
#   data = local_df,
#   model = 1,
#   group = gender
# )

# # Perform DIF analysis
# dif <- mirt::DIF(dif_fit, group = gender, which.par = c("a1", "d"), na.rm = TRUE, plotdif = TRUE)

# Person fit
person_fit <- personfit(fit) %>%
  reframe(
    infit.outside = prop.table(table(z.infit > 1.96 | z.infit < -1.96)),
    outfit.outside = prop.table(table(z.outfit > 1.96 | z.outfit < -1.96))
  ) # lower row = non-fitting people
person_fit$Correct <- c("Correct", "Incorrect")
person_fit <- person_fit %>% dplyr::select(Correct, everything())
```

#### Model Fit

Hieronder bevinden zich de verschillende model fit indices.Hierbij een korte uitleg wat het exact betekent:

- M2: Dit is de M2-statistiek van Maydeu-Olivares, die een maat is voor de absolute fit van het model. Een lager getal duidt op een betere fit. Er is geen vaste cut-off waarde, maar over het algemeen wordt een lagere waarde als beter beschouwd. Als de bijbehorende p-waarde kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- df: de degrees of freedom van het model. Dit is het aantal parameters dat is geschat, minus het aantal constraints dat is opgelegd.
- p-value: de p-waarde van de Goodness of fit test. Als deze kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- RMSEA: RMSEA meet hoe goed een model de waargenomen data benadert, rekening houdend met de fout in het model. Lagere waarden duiden op een betere pasvorm, typisch onder 0.05 voor goede pasvorm en tussen 0.05 en 0.08 voor redelijke pasvorm.
  - RMSEA_5 en RMSEA_95: de 95% betrouwbaarheidsinterval van de RMSEA. Hoe kleiner dit interval rondom de RMSEA waarde, hoe zekerder we zijn van die waarde. 
- SRMSR: SRMSR staat voor "Standardized Root Mean Square Residual". Het is een maat voor de gemiddelde discrepantie tussen de waargenomen en voorspelde covarianties. Een lagere waarde duidt op een betere fit. Een SRMSR van minder dan 0.08 wordt over het algemeen als goed beschouwd.
- TLI: de Tucker-Lewis Index. TLI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.
- CFI: de Comparative Fit Index. De CFI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.
- N: het aantal respondenten in de steekproef.

```{r verschillen - model fit, echo=FALSE, message=FALSE, warning=FALSE}
model_fit$`Conclusion` <- c("", "", "Poor", "Good", "", "", "Good", "Good", "Good")
knitr::kable(model_fit, digits = 3)
```

Als we kijken naar de fit indices, zien dat verschillen een relatiev goeie fit. Alle waardes zijn goed of acceptabel.
Gezien de hoge hoeveelheid observaties (N = `r nrow(data)`), kan de p-waarde als onbetrouwbaar zijn, gezien diens gevoeligheid voor sample size. Daarom wordt de p-waarde genegeerd, en wordt er geconcludeerd dat het model goed past.

#### Person Fit

Hieronder zijn de percentages van person-item-fit indices weergegeven. Hierbij een korte uitleg wat het exact betekent:
- infit.outside: De infit (information-weighted fit) statistiek is gevoelig voor onverwacht gedrag dat invloed heeft op de antwoorden op items die de meeste informatie geven over het vaardigheidsniveau van de persoon. Met andere woorden, het is gevoeliger voor onverwachte antwoorden op items die dicht bij het vaardigheidsniveau van de persoon liggen. De infit.outside waarde in de tabel vertegenwoordigt de proportie van personen wiens infit statistiek buiten het acceptabele bereik valt, wat duidt op een slechte fit.
- outfit.outside: De outfit (outlier-sensitive fit) statistiek is gevoelig voor onverwacht gedrag dat invloed heeft op de antwoorden op items die de minste informatie geven over het vaardigheidsniveau van de persoon. Met andere woorden, het is gevoeliger voor onverwachte antwoorden op items die ver van het vaardigheidsniveau van de persoon liggen. De outfit.outside waarde in de tabel vertegenwoordigt de proportie van personen wiens outfit statistiek buiten het acceptabele bereik valt, wat duidt op een slechte fit.

```{r verschillen - person fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(person_fit)
```

Gezien zowel de infit en outfit statistieken rond de 95 procent vallen, is er geen reden om aan te nemen dat de personen niet goed passen in het model. De proportie van personen wiens infit statistiek buiten het acceptabele bereik valt, wat groter te zijn (~6%).

#### Item Fit

Hieronder bevinden zich de verschillende item fit indices. Hierbij een korte uitleg wat het exact betekent:

- X2: Dit is de X2-statistiek van de item fit test. Een lager getal duidt op een betere fit. Er is geen vaste cut-off waarde, maar over het algemeen wordt een lagere waarde als beter beschouwd. Als de bijbehorende p-waarde kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het item niet goed past.
- df: de degrees of freedom van het model. Dit is het aantal parameters dat is geschat, minus het aantal constraints dat is opgelegd.
- p-value: de p-waarde van de Goodness of fit test. Als deze kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- RMSEA: RMSEA meet hoe goed een model de waargenomen data benadert, rekening houdend met de fout in het model. Lagere waarden duiden op een betere pasvorm, typisch onder 0.05 voor goede pasvorm en tussen 0.05 en 0.08 voor redelijke pasvorm.
 
```{r verschillen - item fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(item_fit)
```

Op basis van de resultaten, blijken meerdere vragen niet goed te passen (zie tabel: vragen met een p lager dan .05).

Dit betekent dat deze waardes niet goed passen in het 1-factor IRT model.Als bepaalde waarden niet goed passen in dit model, kan dit verschillende dingen betekenen.
Ten eerste kan het zijn dat de items niet allemaal dezelfde onderliggende eigenschap meten. Ten tweede kan het zijn dat sommige items verkeerd worden begrepen door de respondenten.
Ten derde kan het zijn dat de veronderstelling van lokale onafhankelijkheid wordt geschonden. Dit betekent dat de reacties op de items afhankelijk zijn van elkaar, zelfs na controle voor de onderliggende eigenschap. Bijvoorbeeld, als het beantwoorden van het ene item helpt bij het beantwoorden van een ander item, dan zullen deze items niet goed passen in het model.

Hieronder volgt een robuste variant van de X2 test, namelijk de PV_Q1 test. Hieronder volgt een korte uitleg wat het exact betekent:

De 'PV_Q1'-statistiek is een afgeleide versie van de Q1-statistiek, een chi-kwadraatstatistiek gebruikt in item respons theorie (IRT) modellen om te testen of een item goed past bij het model. 
Een hoge 'PV_Q1'-waarde duidt op een slechte overeenkomst tussen de waargenomen gegevens en voorspellingen van het model, wat suggereert dat het item mogelijk niet conform het verwachte gedrag 
volgens het IRT-model functioneert. Een niet-significante p-waarde, meestal p > 0.05, in combinatie met lage 'PV_Q1'- en RMSEA.PV_Q1-waarden, geeft aan dat het item goed past bij het IRT-model.
Daarnaast is het belangrijk op te merken dat de 'PV_Q1'-statistiek robuust is tegen invloeden van steekproefomvang en lokale afhankelijkheid, waardoor het een betrouwbaar instrument is om de pasvorm
van een item met het IRT-model te beoordelen, ongeacht de omvang van de steekproef of eventuele lokale onderlinge afhankelijkheden tussen de waarnemingen. Hieronder de resultaten:

```{r verschillen - robust item fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(robust_item_fit)
```

Om een beter beeld te krijgen, volgen hieronder the item parameters van de vragen.

```{r verschillen - item parameters, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(params)
```

Op basis van de factor loadings, blijkt slechts een vraag slecht te laden:

- ASL_verschillen_017: 0.2792875

Hierbij wordt erop geattendeerd dat deze vraag een hele lage moeilijkheidsgraad (b = -126.8284639) bevat. Dit betekent dat deze vraag door bijna alle respondenten goed wordt beantwoord. Dit kan een reden zijn waarom deze vraag niet goed past in het model.

In IRT, meet een factor loading (ook bekend als een discriminatieparameter) hoe goed een item onderscheid maakt tussen personen met verschillende niveaus van de onderliggende eigenschap. Een hoge factor loading betekent dat het item gevoelig is voor verschillen in de onderliggende eigenschap.
Het belangrijkste verschil met factor analyse, is dat in IRT, de factor loadings ook rekening houden met de moeilijkheidsgraad van de items. Dit betekent dat items die dezelfde moeilijkheidsgraad hebben (dat wil zeggen, ze zijn even moeilijk om te beantwoorden) worden geacht dezelfde hoeveelheid informatie te geven over de onderliggende eigenschap, ongeacht hun factor loadings. Dit is niet het geval in factoranalyse, waar de factor loadings alleen de relatie tussen de geobserveerde variabelen en de onderliggende factor meten.
Tevens kan het zijn dat de factor loadings en de item parameters niet overeenkomen. Bijvoorbeeld een item kan een hoge factor loading hebben (wat aangeeft dat het goed discrimineert) maar toch een slechte fit hebben (X2-statistiek) als het bijvoorbeeld verkeerd wordt begrepen door de respondenten. Omgekeerd kan een item goed passen bij het model maar niet goed discrimineren tussen verschillende niveaus van de eigenschap.

##### Item Characteristic Curves

Hieronder bevinden zich de item characteristic curves. Hierbij een korte uitleg wat het exact betekent:

- De lijnen geven de item characteristic curves weer. Dit is de kans dat een persoon met een bepaald niveau van de onderliggende eigenschap het item correct beantwoordt. Hoe hoger de curve, hoe groter de kans dat een persoon met een bepaald niveau van de onderliggende eigenschap het item correct beantwoordt.
- De x-as van de curve vertegenwoordigt het niveau van de onderliggende eigenschap (bijvoorbeeld vaardigheid of kennis) die een persoon heeft, terwijl de y-as de kans vertegenwoordigt dat een persoon het item correct beantwoordt.
- Hoe stijler de ICC-lijn is, hoe beter het item onderscheid maakt tussen personen met verschillende niveaus van de onderliggende eigenschap. Dit betekent dat het item gevoelig is voor verschillen in de onderliggende eigenschap.
- Hoe hoger het y-axale middelpunt, hoe moeilijker het item is. 

De grafieken voor de ICC en IIC hernoemen de items op volgorde die het IRT model aanbied. Hieronder een tabel met de originele namen van de items, en de bijbehorende item nummers.

```{r verschillen - ICC, echo=FALSE, message=FALSE, warning=FALSE}
params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
item_names <- extract.mirt(fit, what = "itemnames")

df <- data.frame(
  item_number = paste0("Item ", seq_along(item_names)),
  item_names = item_names,
  item_discrimination = params$items.a,
  item_difficulty = params$items.b
)
knitr::kable(df)
```


![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/verschillen_ICC_combined.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/verschillen_ICC_separate.png)

##### Item Information Curves

Hieronder bevinden zich de item information curves. Hierbij een korte uitleg wat het exact betekent:

- De lijnen geven de item information curves weer. Dit is de hoeveelheid informatie die een item geeft over de onderliggende eigenschap. Hoe hoger de curve, hoe meer informatie het item geeft over de onderliggende eigenschap.
- Hoe verder naar rechts de piek van de curve ligt, hoe meer informatie het item geeft over hooge niveaus van de onderliggende eigenschap. 
- Hoe verder naar links de piek van de curve ligt, hoe meer informatie het item geeft over lage niveaus van de onderliggende eigenschap.

De grafieken voor de ICC en IIC hernoemen de items op volgorde die het IRT model aanbied. Hieronder een tabel met de originele namen van de items, en de bijbehorende item nummers.

```{r verschillen - IIC, echo=FALSE, message=FALSE, warning=FALSE}
params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
item_names <- extract.mirt(fit, what = "itemnames")

df <- data.frame(
  item_number = paste0("Item ", seq_along(item_names)),
  item_names = item_names,
  item_discrimination = params$items.a,
  item_difficulty = params$items.b
)
knitr::kable(df)
```


![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/verschillen_IIC_combined.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/verschillen_IIC_separate.png)

##### Item Person Map

Hieronder bevindt zich de item person map. Hierbij een korte uitleg wat het exact betekent:

- De x-as van de grafiek vertegenwoordigt het niveau van de onderliggende eigenschap (bijvoorbeeld vaardigheid of kennis) die een persoon heeft, terwijl de y-as de hoeveelheid participanten vertegenwoordigt.
- Onder de grafiek kan bekeken worden welke vraag ongeveer bij welke hoogte van onderliggende eigenschap past, en hoeveel leerlingen hier daadwerkelijk bij horen.

![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/verschillen_item_person_fit.png)

##### Test Characteristic and Information Curves

Gelijk aan de Item characteristics en information curves, maar dan voor de test als geheel. Hierbij een korte uitleg wat het exact betekent:

- De lijnen geven de test characteristic curves weer. Deze geeft aan hoe goed de test informatie geeft over de onderliggende eigenschap. Hoe hoger de curve, hoe meer informatie de test geeft over de onderliggende eigenschap op dat niveau.
- Grofweg kunnen de lijnen en de curve gezien woren als een amalgamatie van de onderliggende item characteristic en information curves.

![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/verschillen_TCC.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/verschillen_TIC.png)

### Werkwoordspelling

```{r werkwoordspelling - stat extraction, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
# load data
fit <- readRDS(paste0(here(), "/data/irt_analyses2024/irt_fit_werkwoordspelling.rds"))
data <- read.csv(paste0(here(), "/data/data_interrim/werkwoordspelling.csv"))

# Model fit
model_fit <- as.data.frame(M2(fit, na.rm = TRUE))
model_fit <- cbind(model_fit, nrow(data))
model_fit <- rename(model_fit, `N` = `nrow(data)`)
model_fit <- tidyr::pivot_longer(model_fit, cols = -N, names_to = "Fit Index", values_to = "Value")
model_fit <- dplyr::select(model_fit, `Fit Index`, Value)
model_fit$`Cut-Off` <- c("", "", "> .05", "< .05 | .05 - .1 | > .1", "", "", "< .08 | .08 - .10 | > .10", "> .95 | .90 - .95 | < .90", "> .95 | .90 - .95 | < .90")

# item fit
item_fit <- itemfit(fit, na.rm = TRUE)
colnames(item_fit) <- c("Item", "X2", "df", "RMSEA", "p")
item_fit <- item_fit[order(item_fit$X2), ]
rownames(item_fit) <- NULL

robust_item_fit <- itemfit(fit, na.rm = TRUE)
colnames(robust_item_fit) <- c("Item", "PV_Q1", "df", "RMSEA", "p")
robust_item_fit <- robust_item_fit[order(robust_item_fit$PV_Q1), ]
rownames(robust_item_fit) <- NULL


# Item parameters
params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
params <- params[, -which(colnames(params) %in% c("items.g", "items.u", "means", "F1"))]
sink("/dev/null")
sum <- suppressWarnings(suppressMessages(summary(fit, silent = TRUE)))
sink()
params <- cbind(params, sum$rotF)
params <- cbind(params, sum$h2)
params <- params[, c("F1", colnames(params)[-which(colnames(params) %in% "F1")])]
colnames(params) <- c("Factor Loading", "Discrimination Parameter", "Difficulty Parameter ", "Communalities")
params <- params[order(params$`Factor Loading`, decreasing = TRUE), ]


# # Differential Item Functionining DIF
# # Prepare the data
# gender <- data$gender
# gender <- dplyr::recode(gender, "Man" = 1, "Vrouw" = 2, "Onbekend" = 2)
# gender <- as.factor(gender)
# local_df <- data[, !grepl("package_duration_raw|student_number|student_name|birth_date|gender", colnames(data))]

# # Fit the multiple group model
# dif_fit <- mirt::multipleGroup(
#   data = local_df,
#   model = 1,
#   group = gender
# )

# # Perform DIF analysis
# dif <- mirt::DIF(dif_fit, group = gender, which.par = c("a1", "d"), na.rm = TRUE, plotdif = TRUE)

# Person fit
person_fit <- personfit(fit) %>%
  reframe(
    infit.outside = prop.table(table(z.infit > 1.96 | z.infit < -1.96)),
    outfit.outside = prop.table(table(z.outfit > 1.96 | z.outfit < -1.96))
  ) # lower row = non-fitting people
person_fit$Correct <- c("Correct", "Incorrect")
person_fit <- person_fit %>% dplyr::select(Correct, everything())
```

#### Model Fit

Hieronder bevinden zich de verschillende model fit indices.Hierbij een korte uitleg wat het exact betekent:

- M2: Dit is de M2-statistiek van Maydeu-Olivares, die een maat is voor de absolute fit van het model. Een lager getal duidt op een betere fit. Er is geen vaste cut-off waarde, maar over het algemeen wordt een lagere waarde als beter beschouwd. Als de bijbehorende p-waarde kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- df: de degrees of freedom van het model. Dit is het aantal parameters dat is geschat, minus het aantal constraints dat is opgelegd.
- p-value: de p-waarde van de Goodness of fit test. Als deze kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- RMSEA: RMSEA meet hoe goed een model de waargenomen data benadert, rekening houdend met de fout in het model. Lagere waarden duiden op een betere pasvorm, typisch onder 0.05 voor goede pasvorm en tussen 0.05 en 0.08 voor redelijke pasvorm.
  - RMSEA_5 en RMSEA_95: de 95% betrouwbaarheidsinterval van de RMSEA. Hoe kleiner dit interval rondom de RMSEA waarde, hoe zekerder we zijn van die waarde. 
- SRMSR: SRMSR staat voor "Standardized Root Mean Square Residual". Het is een maat voor de gemiddelde discrepantie tussen de waargenomen en voorspelde covarianties. Een lagere waarde duidt op een betere fit. Een SRMSR van minder dan 0.08 wordt over het algemeen als goed beschouwd.
- TLI: de Tucker-Lewis Index. TLI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.
- CFI: de Comparative Fit Index. De CFI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.
- N: het aantal respondenten in de steekproef.

```{r werkwoordspelling - model fit, echo=FALSE, message=FALSE, warning=FALSE}
model_fit$`Conclusion` <- c("", "", "Poor", "Acceptabel", "", "", "Good", "Poor", "Poor")
knitr::kable(model_fit, digits = 3)
```

Als we kijken naar de fit indices, zien dat werkwoordspelling een niet zo goede fit heeft, met TLI en CFI als slecht beoordeelt. 
Gezien de hoge hoeveelheid observaties (N = `r nrow(data)`), kan de p-waarde als onbetrouwbaar zijn, gezien diens gevoeligheid voor sample size. Daarom wordt de p-waarde genegeerd, en wordt er geconcludeerd dat het model goed past.

#### Person Fit

Hieronder zijn de percentages van person-item-fit indices weergegeven. Hierbij een korte uitleg wat het exact betekent:
- infit.outside: De infit (information-weighted fit) statistiek is gevoelig voor onverwacht gedrag dat invloed heeft op de antwoorden op items die de meeste informatie geven over het vaardigheidsniveau van de persoon. Met andere woorden, het is gevoeliger voor onverwachte antwoorden op items die dicht bij het vaardigheidsniveau van de persoon liggen. De infit.outside waarde in de tabel vertegenwoordigt de proportie van personen wiens infit statistiek buiten het acceptabele bereik valt, wat duidt op een slechte fit.
- outfit.outside: De outfit (outlier-sensitive fit) statistiek is gevoelig voor onverwacht gedrag dat invloed heeft op de antwoorden op items die de minste informatie geven over het vaardigheidsniveau van de persoon. Met andere woorden, het is gevoeliger voor onverwachte antwoorden op items die ver van het vaardigheidsniveau van de persoon liggen. De outfit.outside waarde in de tabel vertegenwoordigt de proportie van personen wiens outfit statistiek buiten het acceptabele bereik valt, wat duidt op een slechte fit.

```{r werkwoordspelling - person fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(person_fit)
```

Gezien zowel de infit en outfit statistieken rond de 95 procent vallen, is er geen reden om aan te nemen dat de personen niet goed passen in het model. De proportie van personen wiens infit statistiek buiten het acceptabele bereik valt, wat groter te zijn (~8%).

Hieronder in de grafiek is te zien welke vragen in hoeverre bijdragen aan de infit en outfit waardes:

![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/werkwoordspelling_item_fit.png)

#### Item Fit

Hieronder bevinden zich de verschillende item fit indices. Hierbij een korte uitleg wat het exact betekent:

- X2: Dit is de X2-statistiek van de item fit test. Een lager getal duidt op een betere fit. Er is geen vaste cut-off waarde, maar over het algemeen wordt een lagere waarde als beter beschouwd. Als de bijbehorende p-waarde kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het item niet goed past.
- df: de degrees of freedom van het model. Dit is het aantal parameters dat is geschat, minus het aantal constraints dat is opgelegd.
- p-value: de p-waarde van de Goodness of fit test. Als deze kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- RMSEA: RMSEA meet hoe goed een model de waargenomen data benadert, rekening houdend met de fout in het model. Lagere waarden duiden op een betere pasvorm, typisch onder 0.05 voor goede pasvorm en tussen 0.05 en 0.08 voor redelijke pasvorm.
 
```{r werkwoordspelling - item fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(item_fit)
```

Op basis van de resultaten, blijken meerdere vragen niet goed te passen (zie tabel: vragen met een p lager dan .05).

Dit betekent dat deze waardes niet goed passen in het 1-factor IRT model.Als bepaalde waarden niet goed passen in dit model, kan dit verschillende dingen betekenen.
Ten eerste kan het zijn dat de items niet allemaal dezelfde onderliggende eigenschap meten. Ten tweede kan het zijn dat sommige items verkeerd worden begrepen door de respondenten.
Ten derde kan het zijn dat de veronderstelling van lokale onafhankelijkheid wordt geschonden. Dit betekent dat de reacties op de items afhankelijk zijn van elkaar, zelfs na controle voor de onderliggende eigenschap. Bijvoorbeeld, als het beantwoorden van het ene item helpt bij het beantwoorden van een ander item, dan zullen deze items niet goed passen in het model.

Hieronder volgt een robuste variant van de X2 test, namelijk de PV_Q1 test. Hieronder volgt een korte uitleg wat het exact betekent:

De 'PV_Q1'-statistiek is een afgeleide versie van de Q1-statistiek, een chi-kwadraatstatistiek gebruikt in item respons theorie (IRT) modellen om te testen of een item goed past bij het model. 
Een hoge 'PV_Q1'-waarde duidt op een slechte overeenkomst tussen de waargenomen gegevens en voorspellingen van het model, wat suggereert dat het item mogelijk niet conform het verwachte gedrag 
volgens het IRT-model functioneert. Een niet-significante p-waarde, meestal p > 0.05, in combinatie met lage 'PV_Q1'- en RMSEA.PV_Q1-waarden, geeft aan dat het item goed past bij het IRT-model.
Daarnaast is het belangrijk op te merken dat de 'PV_Q1'-statistiek robuust is tegen invloeden van steekproefomvang en lokale afhankelijkheid, waardoor het een betrouwbaar instrument is om de pasvorm
van een item met het IRT-model te beoordelen, ongeacht de omvang van de steekproef of eventuele lokale onderlinge afhankelijkheden tussen de waarnemingen. Hieronder de resultaten:

```{r werkwoordspelling - robust item fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(robust_item_fit)
```

Om een beter beeld te krijgen, volgen hieronder the item parameters van de vragen.

```{r werkwoordspelling - item parameters, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(params)
```

Op basis van de factor loadings, blijkt slechts twee vraag slecht te laden:

- ASL7_taalverzorging_WS_3F_025      :  0.1967051
- ASL7_taalverzorging_WS_1F_009_pilot: -0.0666549

Hierbij wordt erop geattendeerd dat deze vraag een hele lage moeilijkheidsgraad (b = -126.8284639) bevat. Dit betekent dat deze vraag door bijna alle respondenten goed wordt beantwoord. Dit kan een reden zijn waarom deze vraag niet goed past in het model.

In IRT, meet een factor loading (ook bekend als een discriminatieparameter) hoe goed een item onderscheid maakt tussen personen met verschillende niveaus van de onderliggende eigenschap. Een hoge factor loading betekent dat het item gevoelig is voor verschillen in de onderliggende eigenschap.
Het belangrijkste verschil met factor analyse, is dat in IRT, de factor loadings ook rekening houden met de moeilijkheidsgraad van de items. Dit betekent dat items die dezelfde moeilijkheidsgraad hebben (dat wil zeggen, ze zijn even moeilijk om te beantwoorden) worden geacht dezelfde hoeveelheid informatie te geven over de onderliggende eigenschap, ongeacht hun factor loadings. Dit is niet het geval in factoranalyse, waar de factor loadings alleen de relatie tussen de geobserveerde variabelen en de onderliggende factor meten.
Tevens kan het zijn dat de factor loadings en de item parameters niet overeenkomen. Bijvoorbeeld een item kan een hoge factor loading hebben (wat aangeeft dat het goed discrimineert) maar toch een slechte fit hebben (X2-statistiek) als het bijvoorbeeld verkeerd wordt begrepen door de respondenten. Omgekeerd kan een item goed passen bij het model maar niet goed discrimineren tussen verschillende niveaus van de eigenschap.

##### Item Characteristic Curves

Hieronder bevinden zich de item characteristic curves. Hierbij een korte uitleg wat het exact betekent:

- De lijnen geven de item characteristic curves weer. Dit is de kans dat een persoon met een bepaald niveau van de onderliggende eigenschap het item correct beantwoordt. Hoe hoger de curve, hoe groter de kans dat een persoon met een bepaald niveau van de onderliggende eigenschap het item correct beantwoordt.
- De x-as van de curve vertegenwoordigt het niveau van de onderliggende eigenschap (bijvoorbeeld vaardigheid of kennis) die een persoon heeft, terwijl de y-as de kans vertegenwoordigt dat een persoon het item correct beantwoordt.
- Hoe stijler de ICC-lijn is, hoe beter het item onderscheid maakt tussen personen met verschillende niveaus van de onderliggende eigenschap. Dit betekent dat het item gevoelig is voor verschillen in de onderliggende eigenschap.
- Hoe hoger het y-axale middelpunt, hoe moeilijker het item is. 

De grafieken voor de ICC en IIC hernoemen de items op volgorde die het IRT model aanbied. Hieronder een tabel met de originele namen van de items, en de bijbehorende item nummers.

```{r werkwoordspelling - ICC, echo=FALSE, message=FALSE, warning=FALSE}
params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
item_names <- extract.mirt(fit, what = "itemnames")

df <- data.frame(
  item_number = paste0("Item ", seq_along(item_names)),
  item_names = item_names,
  item_discrimination = params$items.a,
  item_difficulty = params$items.b
)
knitr::kable(df)
```


![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/werkwoordspelling_ICC_combined.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/werkwoordspelling_ICC_separate.png)

##### Item Information Curves

Hieronder bevinden zich de item information curves. Hierbij een korte uitleg wat het exact betekent:

- De lijnen geven de item information curves weer. Dit is de hoeveelheid informatie die een item geeft over de onderliggende eigenschap. Hoe hoger de curve, hoe meer informatie het item geeft over de onderliggende eigenschap.
- Hoe verder naar rechts de piek van de curve ligt, hoe meer informatie het item geeft over hooge niveaus van de onderliggende eigenschap. 
- Hoe verder naar links de piek van de curve ligt, hoe meer informatie het item geeft over lage niveaus van de onderliggende eigenschap.

De grafieken voor de ICC en IIC hernoemen de items op volgorde die het IRT model aanbied. Hieronder een tabel met de originele namen van de items, en de bijbehorende item nummers.

```{r werkwoordspelling - IIC, echo=FALSE, message=FALSE, warning=FALSE}
params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
item_names <- extract.mirt(fit, what = "itemnames")

df <- data.frame(
  item_number = paste0("Item ", seq_along(item_names)),
  item_names = item_names,
  item_discrimination = params$items.a,
  item_difficulty = params$items.b
)
knitr::kable(df)
```


![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/werkwoordspelling_IIC_combined.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/werkwoordspelling_IIC_separate.png)

##### Item Person Map

Hieronder bevindt zich de item person map. Hierbij een korte uitleg wat het exact betekent:

- De x-as van de grafiek vertegenwoordigt het niveau van de onderliggende eigenschap (bijvoorbeeld vaardigheid of kennis) die een persoon heeft, terwijl de y-as de hoeveelheid participanten vertegenwoordigt.
- Onder de grafiek kan bekeken worden welke vraag ongeveer bij welke hoogte van onderliggende eigenschap past, en hoeveel leerlingen hier daadwerkelijk bij horen.

![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/werkwoordspelling_item_person_fit.png)

##### Test Characteristic and Information Curves

Gelijk aan de Item characteristics en information curves, maar dan voor de test als geheel. Hierbij een korte uitleg wat het exact betekent:

- De lijnen geven de test characteristic curves weer. Deze geeft aan hoe goed de test informatie geeft over de onderliggende eigenschap. Hoe hoger de curve, hoe meer informatie de test geeft over de onderliggende eigenschap op dat niveau.
- Grofweg kunnen de lijnen en de curve gezien woren als een amalgamatie van de onderliggende item characteristic en information curves.

![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/werkwoordspelling_TCC.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/werkwoordspelling_TIC.png)

### Woorden

```{r woorden - stat extraction, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
# load data
fit <- readRDS(paste0(here(), "/data/irt_analyses2024/irt_fit_woorden.rds"))
data <- read.csv(paste0(here(), "/data/data_interrim/woorden.csv"))

# Model fit
model_fit <- as.data.frame(M2(fit, na.rm = TRUE))
model_fit <- cbind(model_fit, nrow(data))
model_fit <- rename(model_fit, `N` = `nrow(data)`)
model_fit <- tidyr::pivot_longer(model_fit, cols = -N, names_to = "Fit Index", values_to = "Value")
model_fit <- dplyr::select(model_fit, `Fit Index`, Value)
model_fit$`Cut-Off` <- c("", "", "> .05", "< .05 | .05 - .1 | > .1", "", "", "< .08 | .08 - .10 | > .10", "> .95 | .90 - .95 | < .90", "> .95 | .90 - .95 | < .90")

# item fit
item_fit <- itemfit(fit, na.rm = TRUE)
colnames(item_fit) <- c("Item", "X2", "df", "RMSEA", "p")
item_fit <- item_fit[order(item_fit$X2), ]
rownames(item_fit) <- NULL

robust_item_fit <- itemfit(fit, na.rm = TRUE)
colnames(robust_item_fit) <- c("Item", "PV_Q1", "df", "RMSEA", "p")
robust_item_fit <- robust_item_fit[order(robust_item_fit$PV_Q1), ]
rownames(robust_item_fit) <- NULL


# Item parameters
params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
params <- params[, -which(colnames(params) %in% c("items.g", "items.u", "means", "F1"))]
sink("/dev/null")
sum <- suppressWarnings(suppressMessages(summary(fit, silent = TRUE)))
sink()
params <- cbind(params, sum$rotF)
params <- cbind(params, sum$h2)
params <- params[, c("F1", colnames(params)[-which(colnames(params) %in% "F1")])]
colnames(params) <- c("Factor Loading", "Discrimination Parameter", "Difficulty Parameter ", "Communalities")
params <- params[order(params$`Factor Loading`, decreasing = TRUE), ]


# # Differential Item Functionining DIF
# # Prepare the data
# gender <- data$gender
# gender <- dplyr::recode(gender, "Man" = 1, "Vrouw" = 2, "Onbekend" = 2)
# gender <- as.factor(gender)
# local_df <- data[, !grepl("package_duration_raw|student_number|student_name|birth_date|gender", colnames(data))]

# # Fit the multiple group model
# dif_fit <- mirt::multipleGroup(
#   data = local_df,
#   model = 1,
#   group = gender
# )

# # Perform DIF analysis
# dif <- mirt::DIF(dif_fit, group = gender, which.par = c("a1", "d"), na.rm = TRUE, plotdif = TRUE)

# Person fit
person_fit <- personfit(fit) %>%
  reframe(
    infit.outside = prop.table(table(z.infit > 1.96 | z.infit < -1.96)),
    outfit.outside = prop.table(table(z.outfit > 1.96 | z.outfit < -1.96))
  ) # lower row = non-fitting people
person_fit$Correct <- c("Correct", "Incorrect")
person_fit <- person_fit %>% dplyr::select(Correct, everything())
```

#### Model Fit

Hieronder bevinden zich de verschillende model fit indices.Hierbij een korte uitleg wat het exact betekent:

- M2: Dit is de M2-statistiek van Maydeu-Olivares, die een maat is voor de absolute fit van het model. Een lager getal duidt op een betere fit. Er is geen vaste cut-off waarde, maar over het algemeen wordt een lagere waarde als beter beschouwd. Als de bijbehorende p-waarde kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- df: de degrees of freedom van het model. Dit is het aantal parameters dat is geschat, minus het aantal constraints dat is opgelegd.
- p-value: de p-waarde van de Goodness of fit test. Als deze kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- RMSEA: RMSEA meet hoe goed een model de waargenomen data benadert, rekening houdend met de fout in het model. Lagere waarden duiden op een betere pasvorm, typisch onder 0.05 voor goede pasvorm en tussen 0.05 en 0.08 voor redelijke pasvorm.
  - RMSEA_5 en RMSEA_95: de 95% betrouwbaarheidsinterval van de RMSEA. Hoe kleiner dit interval rondom de RMSEA waarde, hoe zekerder we zijn van die waarde. 
- SRMSR: SRMSR staat voor "Standardized Root Mean Square Residual". Het is een maat voor de gemiddelde discrepantie tussen de waargenomen en voorspelde covarianties. Een lagere waarde duidt op een betere fit. Een SRMSR van minder dan 0.08 wordt over het algemeen als goed beschouwd.
- TLI: de Tucker-Lewis Index. TLI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.
- CFI: de Comparative Fit Index. De CFI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.
- N: het aantal respondenten in de steekproef.

```{r woorden - model fit, echo=FALSE, message=FALSE, warning=FALSE}
model_fit$`Conclusion` <- c("", "", "Poor", "Good", "", "", "Good", "Acceptabel", "Acceptabel")
knitr::kable(model_fit, digits = 3)
```

Als we kijken naar de fit indices, zien dat woorden een vrij goeie fit heeft, met alles acceptabel of goed. 
Gezien de hoge hoeveelheid observaties (N = `r nrow(data)`), kan de p-waarde als onbetrouwbaar zijn, gezien diens gevoeligheid voor sample size. Daarom wordt de p-waarde genegeerd, en wordt er geconcludeerd dat het model goed past.

#### Person Fit

Hieronder zijn de percentages van person-item-fit indices weergegeven. Hierbij een korte uitleg wat het exact betekent:
- infit.outside: De infit (information-weighted fit) statistiek is gevoelig voor onverwacht gedrag dat invloed heeft op de antwoorden op items die de meeste informatie geven over het vaardigheidsniveau van de persoon. Met andere woorden, het is gevoeliger voor onverwachte antwoorden op items die dicht bij het vaardigheidsniveau van de persoon liggen. De infit.outside waarde in de tabel vertegenwoordigt de proportie van personen wiens infit statistiek buiten het acceptabele bereik valt, wat duidt op een slechte fit.
- outfit.outside: De outfit (outlier-sensitive fit) statistiek is gevoelig voor onverwacht gedrag dat invloed heeft op de antwoorden op items die de minste informatie geven over het vaardigheidsniveau van de persoon. Met andere woorden, het is gevoeliger voor onverwachte antwoorden op items die ver van het vaardigheidsniveau van de persoon liggen. De outfit.outside waarde in de tabel vertegenwoordigt de proportie van personen wiens outfit statistiek buiten het acceptabele bereik valt, wat duidt op een slechte fit.

```{r woorden - person fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(person_fit)
```

Gezien zowel de infit en outfit statistieken rond de 95 procent vallen, is er geen reden om aan te nemen dat de personen niet goed passen in het model. De proportie van personen wiens infit statistiek buiten het acceptabele bereik valt, wat groter te zijn (~8%).

#### Item Fit

Hieronder bevinden zich de verschillende item fit indices. Hierbij een korte uitleg wat het exact betekent:

- X2: Dit is de X2-statistiek van de item fit test. Een lager getal duidt op een betere fit. Er is geen vaste cut-off waarde, maar over het algemeen wordt een lagere waarde als beter beschouwd. Als de bijbehorende p-waarde kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het item niet goed past.
- df: de degrees of freedom van het model. Dit is het aantal parameters dat is geschat, minus het aantal constraints dat is opgelegd.
- p-value: de p-waarde van de Goodness of fit test. Als deze kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- RMSEA: RMSEA meet hoe goed een model de waargenomen data benadert, rekening houdend met de fout in het model. Lagere waarden duiden op een betere pasvorm, typisch onder 0.05 voor goede pasvorm en tussen 0.05 en 0.08 voor redelijke pasvorm.
 
```{r woorden - item fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(item_fit)
```

Op basis van de resultaten, blijken meerdere vragen niet goed te passen (zie tabel: vragen met een p lager dan .05).

Dit betekent dat deze waardes niet goed passen in het 1-factor IRT model.Als bepaalde waarden niet goed passen in dit model, kan dit verschillende dingen betekenen.
Ten eerste kan het zijn dat de items niet allemaal dezelfde onderliggende eigenschap meten. Ten tweede kan het zijn dat sommige items verkeerd worden begrepen door de respondenten.
Ten derde kan het zijn dat de veronderstelling van lokale onafhankelijkheid wordt geschonden. Dit betekent dat de reacties op de items afhankelijk zijn van elkaar, zelfs na controle voor de onderliggende eigenschap. Bijvoorbeeld, als het beantwoorden van het ene item helpt bij het beantwoorden van een ander item, dan zullen deze items niet goed passen in het model.

Hieronder volgt een robuste variant van de X2 test, namelijk de PV_Q1 test. Hieronder volgt een korte uitleg wat het exact betekent:

De 'PV_Q1'-statistiek is een afgeleide versie van de Q1-statistiek, een chi-kwadraatstatistiek gebruikt in item respons theorie (IRT) modellen om te testen of een item goed past bij het model. 
Een hoge 'PV_Q1'-waarde duidt op een slechte overeenkomst tussen de waargenomen gegevens en voorspellingen van het model, wat suggereert dat het item mogelijk niet conform het verwachte gedrag 
volgens het IRT-model functioneert. Een niet-significante p-waarde, meestal p > 0.05, in combinatie met lage 'PV_Q1'- en RMSEA.PV_Q1-waarden, geeft aan dat het item goed past bij het IRT-model.
Daarnaast is het belangrijk op te merken dat de 'PV_Q1'-statistiek robuust is tegen invloeden van steekproefomvang en lokale afhankelijkheid, waardoor het een betrouwbaar instrument is om de pasvorm
van een item met het IRT-model te beoordelen, ongeacht de omvang van de steekproef of eventuele lokale onderlinge afhankelijkheden tussen de waarnemingen. Hieronder de resultaten:

```{r woorden - robust item fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(robust_item_fit)
```

Om een beter beeld te krijgen, volgen hieronder the item parameters van de vragen.

```{r woorden - item parameters, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(params)
```

Op basis van de factor loadings, blijkt slechts twee vraag slecht te laden:

- ASL_woorden_012: 0.2930172
- ASL_woorden_014: 0.2877881
- ASL_woorden_009: 0.1900700
- ASL_woorden_019: 0.1762761

Hierbij wordt erop geattendeerd dat deze vraag een hele lage moeilijkheidsgraad (b = -126.8284639) bevat. Dit betekent dat deze vraag door bijna alle respondenten goed wordt beantwoord. Dit kan een reden zijn waarom deze vraag niet goed past in het model.

In IRT, meet een factor loading (ook bekend als een discriminatieparameter) hoe goed een item onderscheid maakt tussen personen met verschillende niveaus van de onderliggende eigenschap. Een hoge factor loading betekent dat het item gevoelig is voor verschillen in de onderliggende eigenschap.
Het belangrijkste verschil met factor analyse, is dat in IRT, de factor loadings ook rekening houden met de moeilijkheidsgraad van de items. Dit betekent dat items die dezelfde moeilijkheidsgraad hebben (dat wil zeggen, ze zijn even moeilijk om te beantwoorden) worden geacht dezelfde hoeveelheid informatie te geven over de onderliggende eigenschap, ongeacht hun factor loadings. Dit is niet het geval in factoranalyse, waar de factor loadings alleen de relatie tussen de geobserveerde variabelen en de onderliggende factor meten.
Tevens kan het zijn dat de factor loadings en de item parameters niet overeenkomen. Bijvoorbeeld een item kan een hoge factor loading hebben (wat aangeeft dat het goed discrimineert) maar toch een slechte fit hebben (X2-statistiek) als het bijvoorbeeld verkeerd wordt begrepen door de respondenten. Omgekeerd kan een item goed passen bij het model maar niet goed discrimineren tussen verschillende niveaus van de eigenschap.

##### Item Characteristic Curves

Hieronder bevinden zich de item characteristic curves. Hierbij een korte uitleg wat het exact betekent:

- De lijnen geven de item characteristic curves weer. Dit is de kans dat een persoon met een bepaald niveau van de onderliggende eigenschap het item correct beantwoordt. Hoe hoger de curve, hoe groter de kans dat een persoon met een bepaald niveau van de onderliggende eigenschap het item correct beantwoordt.
- De x-as van de curve vertegenwoordigt het niveau van de onderliggende eigenschap (bijvoorbeeld vaardigheid of kennis) die een persoon heeft, terwijl de y-as de kans vertegenwoordigt dat een persoon het item correct beantwoordt.
- Hoe stijler de ICC-lijn is, hoe beter het item onderscheid maakt tussen personen met verschillende niveaus van de onderliggende eigenschap. Dit betekent dat het item gevoelig is voor verschillen in de onderliggende eigenschap.
- Hoe hoger het y-axale middelpunt, hoe moeilijker het item is. 

De grafieken voor de ICC en IIC hernoemen de items op volgorde die het IRT model aanbied. Hieronder een tabel met de originele namen van de items, en de bijbehorende item nummers.

```{r woorden - ICC, echo=FALSE, message=FALSE, warning=FALSE}
params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
item_names <- extract.mirt(fit, what = "itemnames")

df <- data.frame(
  item_number = paste0("Item ", seq_along(item_names)),
  item_names = item_names,
  item_discrimination = params$items.a,
  item_difficulty = params$items.b
)
knitr::kable(df)
```



![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/woorden_ICC_combined.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/woorden_ICC_separate.png)

##### Item Information Curves

Hieronder bevinden zich de item information curves. Hierbij een korte uitleg wat het exact betekent:

- De lijnen geven de item information curves weer. Dit is de hoeveelheid informatie die een item geeft over de onderliggende eigenschap. Hoe hoger de curve, hoe meer informatie het item geeft over de onderliggende eigenschap.
- Hoe verder naar rechts de piek van de curve ligt, hoe meer informatie het item geeft over hooge niveaus van de onderliggende eigenschap. 
- Hoe verder naar links de piek van de curve ligt, hoe meer informatie het item geeft over lage niveaus van de onderliggende eigenschap.

De grafieken voor de ICC en IIC hernoemen de items op volgorde die het IRT model aanbied. Hieronder een tabel met de originele namen van de items, en de bijbehorende item nummers.

```{r woorden - IIC, echo=FALSE, message=FALSE, warning=FALSE}
params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
item_names <- extract.mirt(fit, what = "itemnames")

df <- data.frame(
  item_number = paste0("Item ", seq_along(item_names)),
  item_names = item_names,
  item_discrimination = params$items.a,
  item_difficulty = params$items.b
)
knitr::kable(df)
```


![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/woorden_IIC_combined.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/woorden_IIC_separate.png)

##### Item Person Map

Hieronder bevindt zich de item person map. Hierbij een korte uitleg wat het exact betekent:

- De x-as van de grafiek vertegenwoordigt het niveau van de onderliggende eigenschap (bijvoorbeeld vaardigheid of kennis) die een persoon heeft, terwijl de y-as de hoeveelheid participanten vertegenwoordigt.
- Onder de grafiek kan bekeken worden welke vraag ongeveer bij welke hoogte van onderliggende eigenschap past, en hoeveel leerlingen hier daadwerkelijk bij horen.

![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/woorden_item_person_fit.png)

##### Test Characteristic and Information Curves

Gelijk aan de Item characteristics en information curves, maar dan voor de test als geheel. Hierbij een korte uitleg wat het exact betekent:

- De lijnen geven de test characteristic curves weer. Deze geeft aan hoe goed de test informatie geeft over de onderliggende eigenschap. Hoe hoger de curve, hoe meer informatie de test geeft over de onderliggende eigenschap op dat niveau.
- Grofweg kunnen de lijnen en de curve gezien woren als een amalgamatie van de onderliggende item characteristic en information curves.

![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/woorden_TCC.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/woorden_TIC.png)

### Woorden Relateren

```{r woordenrelateren - stat extraction, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
# load data
fit <- readRDS(paste0(here(), "/data/irt_analyses2024/irt_fit_woordenrelateren.rds"))
data <- read.csv(paste0(here(), "/data/data_interrim/woordenrelateren.csv"))

# Model fit
model_fit <- as.data.frame(M2(fit, na.rm = TRUE))
model_fit <- cbind(model_fit, nrow(data))
model_fit <- rename(model_fit, `N` = `nrow(data)`)
model_fit <- tidyr::pivot_longer(model_fit, cols = -N, names_to = "Fit Index", values_to = "Value")
model_fit <- dplyr::select(model_fit, `Fit Index`, Value)
model_fit$`Cut-Off` <- c("", "", "> .05", "< .05 | .05 - .1 | > .1", "", "", "< .08 | .08 - .10 | > .10", "> .95 | .90 - .95 | < .90", "> .95 | .90 - .95 | < .90")

# item fit
item_fit <- itemfit(fit, na.rm = TRUE)
colnames(item_fit) <- c("Item", "X2", "df", "RMSEA", "p")
item_fit <- item_fit[order(item_fit$X2), ]
rownames(item_fit) <- NULL

robust_item_fit <- itemfit(fit, na.rm = TRUE)
colnames(robust_item_fit) <- c("Item", "PV_Q1", "df", "RMSEA", "p")
robust_item_fit <- robust_item_fit[order(robust_item_fit$PV_Q1), ]
rownames(robust_item_fit) <- NULL


# Item parameters
params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
params <- params[, -which(colnames(params) %in% c("items.g", "items.u", "means", "F1"))]
sink("/dev/null")
sum <- suppressWarnings(suppressMessages(summary(fit, silent = TRUE)))
sink()
params <- cbind(params, sum$rotF)
params <- cbind(params, sum$h2)
params <- params[, c("F1", colnames(params)[-which(colnames(params) %in% "F1")])]
colnames(params) <- c("Factor Loading", "Discrimination Parameter", "Difficulty Parameter ", "Communalities")
params <- params[order(params$`Factor Loading`, decreasing = TRUE), ]


# # Differential Item Functionining DIF
# # Prepare the data
# gender <- data$gender
# gender <- dplyr::recode(gender, "Man" = 1, "Vrouw" = 2, "Onbekend" = 2)
# gender <- as.factor(gender)
# local_df <- data[, !grepl("package_duration_raw|student_number|student_name|birth_date|gender", colnames(data))]

# # Fit the multiple group model
# dif_fit <- mirt::multipleGroup(
#   data = local_df,
#   model = 1,
#   group = gender
# )

# # Perform DIF analysis
# dif <- mirt::DIF(dif_fit, group = gender, which.par = c("a1", "d"), na.rm = TRUE, plotdif = TRUE)

# Person fit
person_fit <- personfit(fit) %>%
  reframe(
    infit.outside = prop.table(table(z.infit > 1.96 | z.infit < -1.96)),
    outfit.outside = prop.table(table(z.outfit > 1.96 | z.outfit < -1.96))
  ) # lower row = non-fitting people
person_fit$Correct <- c("Correct", "Incorrect")
person_fit <- person_fit %>% dplyr::select(Correct, everything())
```

#### Model Fit

Hieronder bevinden zich de verschillende model fit indices.Hierbij een korte uitleg wat het exact betekent:

- M2: Dit is de M2-statistiek van Maydeu-Olivares, die een maat is voor de absolute fit van het model. Een lager getal duidt op een betere fit. Er is geen vaste cut-off waarde, maar over het algemeen wordt een lagere waarde als beter beschouwd. Als de bijbehorende p-waarde kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- df: de degrees of freedom van het model. Dit is het aantal parameters dat is geschat, minus het aantal constraints dat is opgelegd.
- p-value: de p-waarde van de Goodness of fit test. Als deze kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- RMSEA: RMSEA meet hoe goed een model de waargenomen data benadert, rekening houdend met de fout in het model. Lagere waarden duiden op een betere pasvorm, typisch onder 0.05 voor goede pasvorm en tussen 0.05 en 0.08 voor redelijke pasvorm.
  - RMSEA_5 en RMSEA_95: de 95% betrouwbaarheidsinterval van de RMSEA. Hoe kleiner dit interval rondom de RMSEA waarde, hoe zekerder we zijn van die waarde. 
- SRMSR: SRMSR staat voor "Standardized Root Mean Square Residual". Het is een maat voor de gemiddelde discrepantie tussen de waargenomen en voorspelde covarianties. Een lagere waarde duidt op een betere fit. Een SRMSR van minder dan 0.08 wordt over het algemeen als goed beschouwd.
- TLI: de Tucker-Lewis Index. TLI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.
- CFI: de Comparative Fit Index. De CFI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.
- N: het aantal respondenten in de steekproef.

```{r woordenrelateren - model fit, echo=FALSE, message=FALSE, warning=FALSE}
model_fit$`Conclusion` <- c("", "", "Poor", "Good", "", "", "Good", "Good", "Good")
knitr::kable(model_fit, digits = 3)
```

Als we kijken naar de fit indices, zien dat woordenrelateren een vrij goeie fit heeft, met alles acceptabel of goed. 
Gezien de hoge hoeveelheid observaties (N = `r nrow(data)`), kan de p-waarde als onbetrouwbaar zijn, gezien diens gevoeligheid voor sample size. Daarom wordt de p-waarde genegeerd, en wordt er geconcludeerd dat het model goed past.

#### Person Fit

Hieronder zijn de percentages van person-item-fit indices weergegeven. Hierbij een korte uitleg wat het exact betekent:
- infit.outside: De infit (information-weighted fit) statistiek is gevoelig voor onverwacht gedrag dat invloed heeft op de antwoordenrelateren op items die de meeste informatie geven over het vaardigheidsniveau van de persoon. Met andere woordenrelateren, het is gevoeliger voor onverwachte antwoordenrelateren op items die dicht bij het vaardigheidsniveau van de persoon liggen. De infit.outside waarde in de tabel vertegenwoordigt de proportie van personen wiens infit statistiek buiten het acceptabele bereik valt, wat duidt op een slechte fit.
- outfit.outside: De outfit (outlier-sensitive fit) statistiek is gevoelig voor onverwacht gedrag dat invloed heeft op de antwoordenrelateren op items die de minste informatie geven over het vaardigheidsniveau van de persoon. Met andere woordenrelateren, het is gevoeliger voor onverwachte antwoordenrelateren op items die ver van het vaardigheidsniveau van de persoon liggen. De outfit.outside waarde in de tabel vertegenwoordigt de proportie van personen wiens outfit statistiek buiten het acceptabele bereik valt, wat duidt op een slechte fit.

```{r woordenrelateren - person fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(person_fit)
```

Gezien zowel de infit en outfit statistieken rond de 95 procent vallen, is er geen reden om aan te nemen dat de personen niet goed passen in het model. De proportie van personen wiens infit statistiek buiten het acceptabele bereik valt, wat groter te zijn (~8%).

Hieronder in de grafiek is te zien welke vragen in hoeverre bijdragen aan de infit en outfit waardes:

![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/woordenrelateren_item_fit.png)

#### Item Fit

Hieronder bevinden zich de verschillende item fit indices. Hierbij een korte uitleg wat het exact betekent:

- X2: Dit is de X2-statistiek van de item fit test. Een lager getal duidt op een betere fit. Er is geen vaste cut-off waarde, maar over het algemeen wordt een lagere waarde als beter beschouwd. Als de bijbehorende p-waarde kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het item niet goed past.
- df: de degrees of freedom van het model. Dit is het aantal parameters dat is geschat, minus het aantal constraints dat is opgelegd.
- p-value: de p-waarde van de Goodness of fit test. Als deze kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- RMSEA: RMSEA meet hoe goed een model de waargenomen data benadert, rekening houdend met de fout in het model. Lagere waarden duiden op een betere pasvorm, typisch onder 0.05 voor goede pasvorm en tussen 0.05 en 0.08 voor redelijke pasvorm.
 
```{r woordenrelateren - item fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(item_fit)
```

Op basis van de resultaten, blijken meerdere vragen niet goed te passen (zie tabel: vragen met een p lager dan .05).

Dit betekent dat deze waardes niet goed passen in het 1-factor IRT model.Als bepaalde waarden niet goed passen in dit model, kan dit verschillende dingen betekenen.
Ten eerste kan het zijn dat de items niet allemaal dezelfde onderliggende eigenschap meten. Ten tweede kan het zijn dat sommige items verkeerd worden begrepen door de respondenten.
Ten derde kan het zijn dat de veronderstelling van lokale onafhankelijkheid wordt geschonden. Dit betekent dat de reacties op de items afhankelijk zijn van elkaar, zelfs na controle voor de onderliggende eigenschap. Bijvoorbeeld, als het beantwoordenrelateren van het ene item helpt bij het beantwoordenrelateren van een ander item, dan zullen deze items niet goed passen in het model.

Hieronder volgt een robuste variant van de X2 test, namelijk de PV_Q1 test. Hieronder volgt een korte uitleg wat het exact betekent:

De 'PV_Q1'-statistiek is een afgeleide versie van de Q1-statistiek, een chi-kwadraatstatistiek gebruikt in item respons theorie (IRT) modellen om te testen of een item goed past bij het model. 
Een hoge 'PV_Q1'-waarde duidt op een slechte overeenkomst tussen de waargenomen gegevens en voorspellingen van het model, wat suggereert dat het item mogelijk niet conform het verwachte gedrag 
volgens het IRT-model functioneert. Een niet-significante p-waarde, meestal p > 0.05, in combinatie met lage 'PV_Q1'- en RMSEA.PV_Q1-waarden, geeft aan dat het item goed past bij het IRT-model.
Daarnaast is het belangrijk op te merken dat de 'PV_Q1'-statistiek robuust is tegen invloeden van steekproefomvang en lokale afhankelijkheid, waardoor het een betrouwbaar instrument is om de pasvorm
van een item met het IRT-model te beoordelen, ongeacht de omvang van de steekproef of eventuele lokale onderlinge afhankelijkheden tussen de waarnemingen. Hieronder de resultaten:

```{r woordenrelateren - robust item fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(robust_item_fit)
```

Om een beter beeld te krijgen, volgen hieronder the item parameters van de vragen.

```{r woordenrelateren - item parameters, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(params)
```

Op basis van de factor loadings, blijkt slechts twee vraag slecht te laden:

- ASL_woordenrelateren_016: 0.2683514

Hierbij wordt erop geattendeerd dat deze vraag een hele lage moeilijkheidsgraad (b = -126.8284639) bevat. Dit betekent dat deze vraag door bijna alle respondenten goed wordt beantwoord. Dit kan een reden zijn waarom deze vraag niet goed past in het model.

In IRT, meet een factor loading (ook bekend als een discriminatieparameter) hoe goed een item onderscheid maakt tussen personen met verschillende niveaus van de onderliggende eigenschap. Een hoge factor loading betekent dat het item gevoelig is voor verschillen in de onderliggende eigenschap.
Het belangrijkste verschil met factor analyse, is dat in IRT, de factor loadings ook rekening houden met de moeilijkheidsgraad van de items. Dit betekent dat items die dezelfde moeilijkheidsgraad hebben (dat wil zeggen, ze zijn even moeilijk om te beantwoordenrelateren) worden geacht dezelfde hoeveelheid informatie te geven over de onderliggende eigenschap, ongeacht hun factor loadings. Dit is niet het geval in factoranalyse, waar de factor loadings alleen de relatie tussen de geobserveerde variabelen en de onderliggende factor meten.
Tevens kan het zijn dat de factor loadings en de item parameters niet overeenkomen. Bijvoorbeeld een item kan een hoge factor loading hebben (wat aangeeft dat het goed discrimineert) maar toch een slechte fit hebben (X2-statistiek) als het bijvoorbeeld verkeerd wordt begrepen door de respondenten. Omgekeerd kan een item goed passen bij het model maar niet goed discrimineren tussen verschillende niveaus van de eigenschap.

##### Item Characteristic Curves

Hieronder bevinden zich de item characteristic curves. Hierbij een korte uitleg wat het exact betekent:

- De lijnen geven de item characteristic curves weer. Dit is de kans dat een persoon met een bepaald niveau van de onderliggende eigenschap het item correct beantwoordt. Hoe hoger de curve, hoe groter de kans dat een persoon met een bepaald niveau van de onderliggende eigenschap het item correct beantwoordt.
- De x-as van de curve vertegenwoordigt het niveau van de onderliggende eigenschap (bijvoorbeeld vaardigheid of kennis) die een persoon heeft, terwijl de y-as de kans vertegenwoordigt dat een persoon het item correct beantwoordt.
- Hoe stijler de ICC-lijn is, hoe beter het item onderscheid maakt tussen personen met verschillende niveaus van de onderliggende eigenschap. Dit betekent dat het item gevoelig is voor verschillen in de onderliggende eigenschap.
- Hoe hoger het y-axale middelpunt, hoe moeilijker het item is. 

De grafieken voor de ICC en IIC hernoemen de items op volgorde die het IRT model aanbied. Hieronder een tabel met de originele namen van de items, en de bijbehorende item nummers.

```{r woordenrelateren - ICC, echo=FALSE, message=FALSE, warning=FALSE}
params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
item_names <- extract.mirt(fit, what = "itemnames")

df <- data.frame(
  item_number = paste0("Item ", seq_along(item_names)),
  item_names = item_names,
  item_discrimination = params$items.a,
  item_difficulty = params$items.b
)
knitr::kable(df)
```


![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/woordenrelateren_ICC_combined.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/woordenrelateren_ICC_separate.png)

##### Item Information Curves

Hieronder bevinden zich de item information curves. Hierbij een korte uitleg wat het exact betekent:

- De lijnen geven de item information curves weer. Dit is de hoeveelheid informatie die een item geeft over de onderliggende eigenschap. Hoe hoger de curve, hoe meer informatie het item geeft over de onderliggende eigenschap.
- Hoe verder naar rechts de piek van de curve ligt, hoe meer informatie het item geeft over hooge niveaus van de onderliggende eigenschap. 
- Hoe verder naar links de piek van de curve ligt, hoe meer informatie het item geeft over lage niveaus van de onderliggende eigenschap.

De grafieken voor de ICC en IIC hernoemen de items op volgorde die het IRT model aanbied. Hieronder een tabel met de originele namen van de items, en de bijbehorende item nummers.

```{r woordenrelateren - IIC, echo=FALSE, message=FALSE, warning=FALSE}
params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
item_names <- extract.mirt(fit, what = "itemnames")

df <- data.frame(
  item_number = paste0("Item ", seq_along(item_names)),
  item_names = item_names,
  item_discrimination = params$items.a,
  item_difficulty = params$items.b
)
knitr::kable(df)
```


![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/woordenrelateren_IIC_combined.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/woordenrelateren_IIC_separate.png)

##### Item Person Map

Hieronder bevindt zich de item person map. Hierbij een korte uitleg wat het exact betekent:

- De x-as van de grafiek vertegenwoordigt het niveau van de onderliggende eigenschap (bijvoorbeeld vaardigheid of kennis) die een persoon heeft, terwijl de y-as de hoeveelheid participanten vertegenwoordigt.
- Onder de grafiek kan bekeken worden welke vraag ongeveer bij welke hoogte van onderliggende eigenschap past, en hoeveel leerlingen hier daadwerkelijk bij horen.

![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/woordenrelateren_item_person_fit.png)

##### Test Characteristic and Information Curves

Gelijk aan de Item characteristics en information curves, maar dan voor de test als geheel. Hierbij een korte uitleg wat het exact betekent:

- De lijnen geven de test characteristic curves weer. Deze geeft aan hoe goed de test informatie geeft over de onderliggende eigenschap. Hoe hoger de curve, hoe meer informatie de test geeft over de onderliggende eigenschap op dat niveau.
- Grofweg kunnen de lijnen en de curve gezien woren als een amalgamatie van de onderliggende item characteristic en information curves.

![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/woordenrelateren_TCC.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/irt_analyses`r format(Sys.Date(), "%Y")`/irt_graphs/woordenrelateren_TIC.png)



```{r notes, echo=FALSE, message=FALSE, warning=FALSE}
# Notes:
"If you have many items with significant chi-square tests but high factor loadings, it could suggest a few things:

Model Misfit: The 2PL model might not be the most appropriate model for your data2. Other models (e.g., 3PL model, graded response model) might provide a better fit.

Local Dependence: The items might be locally dependent, meaning the responses to some items are correlated even after controlling for the latent trait4. This can inflate the chi-square statistic leading to a significant test result4.

Large Sample Size: Chi-square tests are sensitive to sample size2. With a large sample size, even small deviations from the model can lead to a significant test result2.

Multidimensionality: The test might be measuring more than one latent trait, causing the 2PL model, which is a unidimensional model, to fit poorly4."
```


