---
title: "Aansluiting Item Analyse"
author: "J.F.J. (Jesse) van Bussel"
date: "`r format(Sys.Date(), '%Y-%m-%d')`"
output:
  html_document:
    theme: readable
    toc: true # table of contents
    toc_float: true # float: stays up left
    css: ["./css/custom.css", "./css/style_documentation.css"]
---
```{r Environment set-up, echo=FALSE, message=FALSE, warning=FALSE}
# Load files
library(here)
library(tools)
library(lavaan)

library(lordif)
library(here)
library(tools)
library(lavaan)
library(dplyr)


names <- list.files(here(paste0("data/item_analyses_pers", format(Sys.Date(), "%Y"), "/")), full.names = TRUE)
names <- names[grepl(".Rdata", names)]

# Load files
for (file in names) {
  load(file, envir = .GlobalEnv)
  # Rename an object in the global environment
  assign(file_path_sans_ext(basename(file)), factor_analyses, envir = .GlobalEnv)
  rm(file)
}
rm(names)
```


# Aansluiting Item Analyse

## Introductie

Binnen dit document worden de resultaten van de item analyse voorgelegd voor de pakketten gedrag_houding (voor zowel leerling, leerkracht als ouder, gescheiden) en interesse. De analyses waran van factor-analytische en item-response-theoretische aard.
Door de natuur van de antwoordopties (likert-schaal) is het niet mogelijk om de analyses uit te voeren op de gebruikelijke 2PL manier. Hiervoor is een Graded Response Model gebruikt. 
Voor zowel de factor analyses als ook de IRT modellen was de aanname dat er 10 dan wel 6 factoren waren, voor gedrag_houding en interesse respectievelijk. Deze aanname kwam voort uit de benaming van de items.

## Resultaten

### Gedrag_houding (leerling)

#### Factor Analyse

```{r gedrag_houding$gedrag_houding - stat extraction, echo=FALSE, message=FALSE, warning=FALSE}
fit <- fitMeasures(gedrag_houding$gedrag_houding)

# null hypothesis: empirical chi-square


p <- round(fit["pvalue"][[1]], 3)
chisq <- fit["chisq"][[1]]

# model fit indices
rmsea <- fit["rmsea"][[1]] # < .06 or .08
p_rmsea <- fit["rmsea.pvalue"][[1]] # > .05
tli <- fit["tli"][[1]] # > 0.95
rms <- fit["crmr"][[1]] # correct RMS, < 0.08
cfi <- fit["cfi"][[1]] # > 0.90

# N
lavInspect(gedrag_houding$gedrag_houding, what = "nobs")

# Explained variance
r2 <- inspect(gedrag_houding$gedrag_houding, what = "r2")

# Factor loadings
loadings <- parameterEstimates(gedrag_houding$gedrag_houding)[parameterEstimates(gedrag_houding$gedrag_houding)$op == "=~", ][c("rhs", "est")]
weights <- inspect(gedrag_houding$gedrag_houding, what = "std")[["lambda"]] # These are the factor score coefficients, which are used to compute factor scores for each observation in your data.
```

##### Model Beoordeling

Hieronder volgen alle model fit van de factor analyse voor gedrag_houding. Hierbij moet rekening gehouden worden dat de factor analyses expliciet gemaakt zijn met 10 factoren,
en de onderliggende bijbehorende vragen alleen laden op de voor ingestelde factor. 

> @FLEUR: dit kan ik ook veranderen. m.a.w. dat ze wel op meerdere factoren laden. Dit geldt ook voor alle andere factor analyses. Maar ik het het nu zo gedaan
> om een wat globaler beeld te krijgen. Vaak zijn vragen die op deze manier slecht laden beter behorend bij andere factoren. Dit zou ik dan op jouw aanraden nog kunnen onderzoeken.
  
Hieronder, in tabel 2, zie je de model-fit statistieken van de CFA voor betekenissen. Hierbij een korte uitleg wat het exact betekent:

- p-value: de p-waarde van de empirische chi-square. Als deze kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- RMSEA: RMSEA meet hoe goed een model de waargenomen data benadert, rekening houdend met de fout in het model. Lagere waarden duiden op een betere pasvorm, typisch onder 0.05 voor goede pasvorm en tussen 0.05 en 0.08 voor redelijke pasvorm.
- TLI: de Tucker-Lewis Index. TLI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.
- CRMS: de Corrected Root Mean Square. Dit kwantificeert het gemiddelde verschil tussen waargenomen en voorspelde waarden in een model. Het geeft een maat voor hoe goed de voorspellingen van het model overeenkomen met de daadwerkelijke gegevens. Lagere RMRS-waarden geven een betere overeenkomst aan, wat wijst op een nauwkeuriger model.
- CFI: de Comparative Fit Index. De CFI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.


**Tabel 2: Betekenissen - Model-fit statistieken**

```{r gedrag_houding - model fit table, echo=FALSE, message=FALSE, warning=FALSE}
# Create dataframe
df <- data.frame(
  Statistic_Name = c("p-value", "RMSEA", "TLI", "CRMS", "CFI"),
  Result = c(p, rmsea, tli, rms, cfi),
  Cut_off = c("> .05", "< .05 | .05 - .1 | > .1", "> .95 | .90 - .95 | < .90", "< .08 | .08 - .10 | > .10", "> .95 | .90 - .95 | < .90"),
  Conclusion = c("Poor", "Good", "Poor", "Good", "Poor")
)

# Create table
knitr::kable(df, col.names = c("Statistic", "Result", "Cut-off", "Conclusion"))
```


De model-fit resultaten zijn niet heel positief.
Gezien de gevoeligheid van p-waardes in empirische chi-square testen, en de hoge hoeveelheid observaties, is het verstandiger
de andere fit indexen te hanteren.

Als conclusie hier is dat het model goed past, en (dus) een factor voldoende is.


##### Item Beoordeling

Hieronder staan de factor loadings, factor weights, communalities, en uniquenesses weergegeven. Hieronder een beknopte uitleg van wat deze waarden betekenen:

- Factor loadings: de factor loadings geven aan hoe sterk een item correleert met een factor. Hoe hoger de factor loading, hoe sterker de correlatie. Een factor loading van .3 of hoger wordt gezien als een goede factor loading.
- Factor weights: de factor weights geven aan hoeveel gewicht een item heeft op een factor. Deze wordt gebruikt om de factor scores te berekenen. Hoe hoger de factor weight, hoe meer gewicht een item heeft op een factor.


**Tabel 3: Betekenissen - Model-fit statistieken**

```{r gedrag_houding - Item loadings, echo=FALSE, message=FALSE, warning=FALSE}
# Create dataframe

# Create table
loadings <- loadings[order(loadings$est, decreasing = TRUE), ]
knitr::kable(loadings, col.names = c("Naam", "std. Factor Loading"))
```

Op basis van de factor loadings en de communality/uniqueness waardes, kunnen twee reccomendaties gemaakt worden.
Een factor loading van een vraag wordt als "goed" gezien zodra deze hoger ligt dan .3. Dit is bij meerdere vragen
niet het geval. Specifiek gaat dit om de volgende vragen (van hoogste naar laagste):

- ASL_pers_lln_083_open          : 0.2751208
- ASL_pers_lln_058_zelfvertrouwen: 0.2321667

Hier volgen nog de gewichten van alle vragen op diens eigen factoren:

**Tabel 4: Betekenissen - Model-fit statistieken**

```{r gedrag_houding - Weights, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(weights)
```


Zie figuur 3 voor een grafische weergave:

**Figuur 3: Factor Loadings Betekenissen**

```{r gedrag_houding - factor loadings plot, echo=FALSE, message=FALSE, warning=FALSE}
plot <- plot(loadings$est)
plot <- lines(loadings$est)
plot <- abline(h = 0.3, lty = 2) # lty = 2 makes the line dashed
plot <- title(main = "Factor Loadings Betekenissen", sub = "Item factor loadings; order based on item factor loadings order")
```


#### Item Response Analyse
```{r leerling gedrag_houding  - load data, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
# load data
irt_fit <- readRDS(paste0(here(), "/data/item_analyses_pers2024/irt_fit_gedrag_houding.rds"))
data <- read.csv(paste0(here(), "/data/data_interrim/gedrag_houding.csv"))
```

##### Betrokkenheid thuis

```{r leerling betrokkenheid_thuis - stat extraction, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
# Model fit
fit <- irt_fit$gedrag_houding_betrokkenheid_thuis_irt_fit
model_fit <- as.data.frame(M2(fit, na.rm = TRUE))
model_fit <- cbind(model_fit, nrow(data))
model_fit <- rename(model_fit, `N` = `nrow(data)`)
model_fit <- tidyr::pivot_longer(model_fit, cols = -N, names_to = "Fit Index", values_to = "Value")
model_fit <- dplyr::select(model_fit, `Fit Index`, Value)
model_fit$`Cut-Off` <- c("", "", "> .05", "< .05 | .05 - .1 | > .1", "", "", "< .08 | .08 - .10 | > .10", "> .95 | .90 - .95 | < .90", "> .95 | .90 - .95 | < .90")

# item fit
item_fit <- itemfit(fit, na.rm = TRUE, fit_stats = "PV_Q1")
colnames(item_fit) <- c("Item", "X2", "df", "RMSEA", "p")
item_fit <- item_fit[order(item_fit$X2), ]
rownames(item_fit) <- NULL

robust_item_fit <- itemfit(fit, na.rm = TRUE)
colnames(robust_item_fit) <- c("Item", "PV_Q1", "df", "RMSEA", "p")
robust_item_fit <- robust_item_fit[order(robust_item_fit$PV_Q1), ]
rownames(robust_item_fit) <- NULL

# Item parameters
params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
params <- params[, -which(colnames(params) %in% c("items.g", "items.u", "means", "F1"))]
sink("/dev/null")
sum <- suppressWarnings(suppressMessages(summary(fit, silent = TRUE)))
sink()
params <- cbind(params, sum$rotF)
params <- cbind(params, sum$h2)
params <- params[, c("F1", colnames(params)[-which(colnames(params) %in% "F1")])]
colnames(params) <- c("Factor Loading", "Discrimination Parameter", "Difficulty Parameter 1", "Difficulty Parameter 2", "Difficulty Parameter 3", "Difficulty Parameter 4", "Communalities")
params <- params[order(params$`Factor Loading`, decreasing = TRUE), ]


# # Differential Item Functionining DIF
# # Prepare the data
# gender <- data$gender
# gender <- dplyr::recode(gender, "Man" = 1, "Vrouw" = 2, "Onbekend" = 2)
# gender <- as.factor(gender)
# local_df <- data[, !grepl("package_duration_raw|student_number|student_name|birth_date|gender", colnames(data))]

# # Fit the multiple group model
# dif_fit <- mirt::multipleGroup(
#   data = local_df,
#   model = 1,
#   group = gender
# )

# # Perform DIF analysis
# dif <- mirt::DIF(dif_fit, group = gender, which.par = c("a1", "d"), na.rm = TRUE, plotdif = TRUE)

# person fit
# person_fit <- personfit(fit) %>%
#   reframe(
#     infit.outside = prop.table(table(z.infit > 1.96 | z.infit < -1.96)),
#     outfit.outside = prop.table(table(z.outfit > 1.96 | z.outfit < -1.96))
#   ) # lower row = non-fitting people
# person_fit$Correct <- c("Correct", "Incorrect")
# person_fit <- person_fit %>% dplyr::select(Correct, everything())
```

###### Model Fit

Hieronder bevinden zich de verschillende model fit indices.Hierbij een korte uitleg wat het exact betekent:

- M2: Dit is de M2-statistiek van Maydeu-Olivares, die een maat is voor de absolute fit van het model. Een lager getal duidt op een betere fit. Er is geen vaste cut-off waarde, maar over het algemeen wordt een lagere waarde als beter beschouwd. Als de bijbehorende p-waarde kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- df: de degrees of freedom van het model. Dit is het aantal parameters dat is geschat, minus het aantal constraints dat is opgelegd.
- p-value: de p-waarde van de Goodness of fit test. Als deze kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- RMSEA: RMSEA meet hoe goed een model de waargenomen data benadert, rekening houdend met de fout in het model. Lagere waarden duiden op een betere pasvorm, typisch onder 0.05 voor goede pasvorm en tussen 0.05 en 0.08 voor redelijke pasvorm.
  - RMSEA_5 en RMSEA_95: de 95% betrouwbaarheidsinterval van de RMSEA. Hoe kleiner dit interval rondom de RMSEA waarde, hoe zekerder we zijn van die waarde. 
- SRMSR: SRMSR staat voor "Standardized Root Mean Square Residual". Het is een maat voor de gemiddelde discrepantie tussen de waargenomen en voorspelde covarianties. Een lagere waarde duidt op een betere fit. Een SRMSR van minder dan 0.08 wordt over het algemeen als goed beschouwd.
- TLI: de Tucker-Lewis Index. TLI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.
- CFI: de Comparative Fit Index. De CFI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.
- N: het aantal respondenten in de steekproef.

```{r leerling betrokkenheid thuis - model fit, echo=FALSE, message=FALSE, warning=FALSE}
model_fit$`Conclusion` <- c("", "", "Poor", "Acceptable", "", "", "Good", "Poor", "Poor")
knitr::kable(model_fit)
```

###### Item Fit

Hieronder bevinden zich de verschillende item fit indices. Hierbij een korte uitleg wat het exact betekent:

- X2: Dit is de X2-statistiek van de item fit test. Een lager getal duidt op een betere fit. Er is geen vaste cut-off waarde, maar over het algemeen wordt een lagere waarde als beter beschouwd. Als de bijbehorende p-waarde kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het item niet goed past.
- df: de degrees of freedom van het model. Dit is het aantal parameters dat is geschat, minus het aantal constraints dat is opgelegd.
- p-value: de p-waarde van de Goodness of fit test. Als deze kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- RMSEA: RMSEA meet hoe goed een model de waargenomen data benadert, rekening houdend met de fout in het model. Lagere waarden duiden op een betere pasvorm, typisch onder 0.05 voor goede pasvorm en tussen 0.05 en 0.08 voor redelijke pasvorm.
 
```{r leerling betrokkenheid thuis - item fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(item_fit)
```


Als bepaalde waarden niet goed passen in dit model, kan dit verschillende dingen betekenen.
Ten eerste kan het zijn dat de items niet allemaal dezelfde onderliggende eigenschap meten. Ten tweede kan het zijn dat sommige items verkeerd worden begrepen door de respondenten.
Ten derde kan het zijn dat de veronderstelling van lokale onafhankelijkheid wordt geschonden. Dit betekent dat de reacties op de items afhankelijk zijn van elkaar, zelfs na controle voor de onderliggende eigenschap. Bijvoorbeeld, als het beantwoorden van het ene item helpt bij het beantwoorden van een ander item, dan zullen deze items niet goed passen in het model.

Hieronder volgt een robuste variant van de X2 test, namelijk de PV_Q1 test. Hieronder volgt een korte uitleg wat het exact betekent:

De 'PV_Q1'-statistiek is een afgeleide versie van de Q1-statistiek, een chi-kwadraatstatistiek gebruikt in item respons theorie (IRT) modellen om te testen of een item goed past bij het model. 
Een hoge 'PV_Q1'-waarde duidt op een slechte overeenkomst tussen de waargenomen gegevens en voorspellingen van het model, wat suggereert dat het item mogelijk niet conform het verwachte gedrag 
volgens het IRT-model functioneert. Een niet-significante p-waarde, meestal p > 0.05, in combinatie met lage 'PV_Q1'- en RMSEA.PV_Q1-waarden, geeft aan dat het item goed past bij het IRT-model.
Daarnaast is het belangrijk op te merken dat de 'PV_Q1'-statistiek robuust is tegen invloeden van steekproefomvang en lokale afhankelijkheid, waardoor het een betrouwbaar instrument is om de pasvorm
van een item met het IRT-model te beoordelen, ongeacht de omvang van de steekproef of eventuele lokale onderlinge afhankelijkheden tussen de waarnemingen. Hieronder de resultaten:

```{r leerling betrokkenheid thuis - robust item fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(robust_item_fit)
```

Om een beter beeld te krijgen, volgen hieronder the item parameters van de vragen. 
In IRT, meet een factor loading (ook bekend als een discriminatieparameter) hoe goed een item onderscheid maakt tussen personen met verschillende niveaus van de onderliggende eigenschap. Een hoge factor loading betekent dat het item gevoelig is voor verschillen in de onderliggende eigenschap.
Het belangrijkste verschil met factor analyse, is dat in IRT, de factor loadings ook rekening houden met de moeilijkheidsgraad van de items. Dit betekent dat items die dezelfde moeilijkheidsgraad hebben (dat wil zeggen, ze zijn even moeilijk om te beantwoorden) worden geacht dezelfde hoeveelheid informatie te geven over de onderliggende eigenschap, ongeacht hun factor loadings. Dit is niet het geval in factoranalyse, waar de factor loadings alleen de relatie tussen de geobserveerde variabelen en de onderliggende factor meten.
Tevens kan het zijn dat de factor loadings en de item parameters niet overeenkomen. Bijvoorbeeld een item kan een hoge factor loading hebben (wat aangeeft dat het goed discrimineert) maar toch een slechte fit hebben (X2-statistiek) als het bijvoorbeeld verkeerd wordt begrepen door de respondenten. Omgekeerd kan een item goed passen bij het model maar niet goed discrimineren tussen verschillende niveaus van de eigenschap.
In een Graded response model, zoals hier gebruikt, zijn er meerdere moeilijkheids parameters, een voor elke vrijheidsgraaad van de vraag (totaal antwoordmogelijk - 1). 

```{r leerling betrokkenheid thuis - item parameters, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(params)
params
```

##### concentratie

```{r leerling concentratie - stat extraction, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
# Model fit

fit <- irt_fit$gedrag_houding_concentratie_irt_fit
model_fit <- as.data.frame(M2(fit, na.rm = TRUE))
model_fit <- cbind(model_fit, nrow(data))
model_fit <- rename(model_fit, `N` = `nrow(data)`)
model_fit <- tidyr::pivot_longer(model_fit, cols = -N, names_to = "Fit Index", values_to = "Value")
model_fit <- dplyr::select(model_fit, `Fit Index`, Value)
model_fit$`Cut-Off` <- c("", "", "> .05", "< .05 | .05 - .1 | > .1", "", "", "< .08 | .08 - .10 | > .10", "> .95 | .90 - .95 | < .90", "> .95 | .90 - .95 | < .90")

# item fit
item_fit <- itemfit(fit, na.rm = TRUE, fit_stats = "PV_Q1")
colnames(item_fit) <- c("Item", "X2", "df", "RMSEA", "p")
item_fit <- item_fit[order(item_fit$X2), ]
rownames(item_fit) <- NULL

robust_item_fit <- itemfit(fit, na.rm = TRUE)
colnames(robust_item_fit) <- c("Item", "PV_Q1", "df", "RMSEA", "p")
robust_item_fit <- robust_item_fit[order(robust_item_fit$PV_Q1), ]
rownames(robust_item_fit) <- NULL

# Item parameters
params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
params <- params[, -which(colnames(params) %in% c("items.g", "items.u", "means", "F1"))]
sink("/dev/null")
sum <- suppressWarnings(suppressMessages(summary(fit, silent = TRUE)))
sink()
params <- cbind(params, sum$rotF)
params <- cbind(params, sum$h2)
params <- params[, c("F1", colnames(params)[-which(colnames(params) %in% "F1")])]
colnames(params) <- c("Factor Loading", "Discrimination Parameter", "Difficulty Parameter 1", "Difficulty Parameter 2", "Difficulty Parameter 3", "Difficulty Parameter 4", "Communalities")
params <- params[order(params$`Factor Loading`, decreasing = TRUE), ]


# # Differential Item Functionining DIF
# # Prepare the data
# gender <- data$gender
# gender <- dplyr::recode(gender, "Man" = 1, "Vrouw" = 2, "Onbekend" = 2)
# gender <- as.factor(gender)
# local_df <- data[, !grepl("package_duration_raw|student_number|student_name|birth_date|gender", colnames(data))]

# # Fit the multiple group model
# dif_fit <- mirt::multipleGroup(
#   data = local_df,
#   model = 1,
#   group = gender
# )

# # Perform DIF analysis
# dif <- mirt::DIF(dif_fit, group = gender, which.par = c("a1", "d"), na.rm = TRUE, plotdif = TRUE)

# person fit
# person_fit <- personfit(fit) %>%
#   reframe(
#     infit.outside = prop.table(table(z.infit > 1.96 | z.infit < -1.96)),
#     outfit.outside = prop.table(table(z.outfit > 1.96 | z.outfit < -1.96))
#   ) # lower row = non-fitting people
# person_fit$Correct <- c("Correct", "Incorrect")
# person_fit <- person_fit %>% dplyr::select(Correct, everything())
```

###### Model Fit

Hieronder bevinden zich de verschillende model fit indices.Hierbij een korte uitleg wat het exact betekent:

- M2: Dit is de M2-statistiek van Maydeu-Olivares, die een maat is voor de absolute fit van het model. Een lager getal duidt op een betere fit. Er is geen vaste cut-off waarde, maar over het algemeen wordt een lagere waarde als beter beschouwd. Als de bijbehorende p-waarde kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- df: de degrees of freedom van het model. Dit is het aantal parameters dat is geschat, minus het aantal constraints dat is opgelegd.
- p-value: de p-waarde van de Goodness of fit test. Als deze kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- RMSEA: RMSEA meet hoe goed een model de waargenomen data benadert, rekening houdend met de fout in het model. Lagere waarden duiden op een betere pasvorm, typisch onder 0.05 voor goede pasvorm en tussen 0.05 en 0.08 voor redelijke pasvorm.
  - RMSEA_5 en RMSEA_95: de 95% betrouwbaarheidsinterval van de RMSEA. Hoe kleiner dit interval rondom de RMSEA waarde, hoe zekerder we zijn van die waarde. 
- SRMSR: SRMSR staat voor "Standardized Root Mean Square Residual". Het is een maat voor de gemiddelde discrepantie tussen de waargenomen en voorspelde covarianties. Een lagere waarde duidt op een betere fit. Een SRMSR van minder dan 0.08 wordt over het algemeen als goed beschouwd.
- TLI: de Tucker-Lewis Index. TLI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.
- CFI: de Comparative Fit Index. De CFI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.
- N: het aantal respondenten in de steekproef.

```{r leerling concentratie - model fit, echo=FALSE, message=FALSE, warning=FALSE}
model_fit$`Conclusion` <- c("", "", "Poor", "Acceptable", "", "", "Good", "Poor", "Poor")
knitr::kable(model_fit)
```

###### Item Fit

Hieronder bevinden zich de verschillende item fit indices. Hierbij een korte uitleg wat het exact betekent:

- X2: Dit is de X2-statistiek van de item fit test. Een lager getal duidt op een betere fit. Er is geen vaste cut-off waarde, maar over het algemeen wordt een lagere waarde als beter beschouwd. Als de bijbehorende p-waarde kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het item niet goed past.
- df: de degrees of freedom van het model. Dit is het aantal parameters dat is geschat, minus het aantal constraints dat is opgelegd.
- p-value: de p-waarde van de Goodness of fit test. Als deze kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- RMSEA: RMSEA meet hoe goed een model de waargenomen data benadert, rekening houdend met de fout in het model. Lagere waarden duiden op een betere pasvorm, typisch onder 0.05 voor goede pasvorm en tussen 0.05 en 0.08 voor redelijke pasvorm.
 
```{r leerling concentratie - item fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(item_fit)
```


Als bepaalde waarden niet goed passen in dit model, kan dit verschillende dingen betekenen.
Ten eerste kan het zijn dat de items niet allemaal dezelfde onderliggende eigenschap meten. Ten tweede kan het zijn dat sommige items verkeerd worden begrepen door de respondenten.
Ten derde kan het zijn dat de veronderstelling van lokale onafhankelijkheid wordt geschonden. Dit betekent dat de reacties op de items afhankelijk zijn van elkaar, zelfs na controle voor de onderliggende eigenschap. Bijvoorbeeld, als het beantwoorden van het ene item helpt bij het beantwoorden van een ander item, dan zullen deze items niet goed passen in het model.

Hieronder volgt een robuste variant van de X2 test, namelijk de PV_Q1 test. Hieronder volgt een korte uitleg wat het exact betekent:

De 'PV_Q1'-statistiek is een afgeleide versie van de Q1-statistiek, een chi-kwadraatstatistiek gebruikt in item respons theorie (IRT) modellen om te testen of een item goed past bij het model. 
Een hoge 'PV_Q1'-waarde duidt op een slechte overeenkomst tussen de waargenomen gegevens en voorspellingen van het model, wat suggereert dat het item mogelijk niet conform het verwachte gedrag 
volgens het IRT-model functioneert. Een niet-significante p-waarde, meestal p > 0.05, in combinatie met lage 'PV_Q1'- en RMSEA.PV_Q1-waarden, geeft aan dat het item goed past bij het IRT-model.
Daarnaast is het belangrijk op te merken dat de 'PV_Q1'-statistiek robuust is tegen invloeden van steekproefomvang en lokale afhankelijkheid, waardoor het een betrouwbaar instrument is om de pasvorm
van een item met het IRT-model te beoordelen, ongeacht de omvang van de steekproef of eventuele lokale onderlinge afhankelijkheden tussen de waarnemingen. Hieronder de resultaten:

```{r leerling concentratie - robust item fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(robust_item_fit)
```

Om een beter beeld te krijgen, volgen hieronder the item parameters van de vragen. 
In IRT, meet een factor loading (ook bekend als een discriminatieparameter) hoe goed een item onderscheid maakt tussen personen met verschillende niveaus van de onderliggende eigenschap. Een hoge factor loading betekent dat het item gevoelig is voor verschillen in de onderliggende eigenschap.
Het belangrijkste verschil met factor analyse, is dat in IRT, de factor loadings ook rekening houden met de moeilijkheidsgraad van de items. Dit betekent dat items die dezelfde moeilijkheidsgraad hebben (dat wil zeggen, ze zijn even moeilijk om te beantwoorden) worden geacht dezelfde hoeveelheid informatie te geven over de onderliggende eigenschap, ongeacht hun factor loadings. Dit is niet het geval in factoranalyse, waar de factor loadings alleen de relatie tussen de geobserveerde variabelen en de onderliggende factor meten.
Tevens kan het zijn dat de factor loadings en de item parameters niet overeenkomen. Bijvoorbeeld een item kan een hoge factor loading hebben (wat aangeeft dat het goed discrimineert) maar toch een slechte fit hebben (X2-statistiek) als het bijvoorbeeld verkeerd wordt begrepen door de respondenten. Omgekeerd kan een item goed passen bij het model maar niet goed discrimineren tussen verschillende niveaus van de eigenschap.
In een Graded response model, zoals hier gebruikt, zijn er meerdere moeilijkheids parameters, een voor elke vrijheidsgraaad van de vraag (totaal antwoordmogelijk - 1). 

```{r leerling concentratie - item parameters, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(params)
params
```

##### contact met leeftijdsgenoten

```{r leerling contact met leeftijdsgenoten - stat extraction, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
# Model fit

fit <- irt_fit$gedrag_houding_contact_met_leeftijdsgenoten_irt_fit
# model_fit <- as.data.frame(M2(fit, na.rm = TRUE))
# model_fit <- cbind(model_fit, nrow(data))
# model_fit <- rename(model_fit, `N` = `nrow(data)`)
# model_fit <- tidyr::pivot_longer(model_fit, cols = -N, names_to = "Fit Index", values_to = "Value")
# model_fit <- dplyr::select(model_fit, `Fit Index`, Value)
# model_fit$`Cut-Off` <- c("", "", "> .05", "< .05 | .05 - .1 | > .1", "", "", "< .08 | .08 - .10 | > .10", "> .95 | .90 - .95 | < .90", "> .95 | .90 - .95 | < .90")

# item fit
item_fit <- itemfit(fit, na.rm = TRUE, fit_stats = "PV_Q1")
colnames(item_fit) <- c("Item", "X2", "df", "RMSEA", "p")
item_fit <- item_fit[order(item_fit$X2), ]
rownames(item_fit) <- NULL

robust_item_fit <- itemfit(fit, na.rm = TRUE)
colnames(robust_item_fit) <- c("Item", "PV_Q1", "df", "RMSEA", "p")
robust_item_fit <- robust_item_fit[order(robust_item_fit$PV_Q1), ]
rownames(robust_item_fit) <- NULL

# Item parameters
params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
params <- params[, -which(colnames(params) %in% c("items.g", "items.u", "means", "F1"))]
sink("/dev/null")
sum <- suppressWarnings(suppressMessages(summary(fit, silent = TRUE)))
sink()
params <- cbind(params, sum$rotF)
params <- cbind(params, sum$h2)
params <- params[, c("F1", colnames(params)[-which(colnames(params) %in% "F1")])]
colnames(params) <- c("Factor Loading", "Discrimination Parameter", "Difficulty Parameter 1", "Difficulty Parameter 2", "Difficulty Parameter 3", "Difficulty Parameter 4", "Communalities")
params <- params[order(params$`Factor Loading`, decreasing = TRUE), ]


# # Differential Item Functionining DIF
# # Prepare the data
# gender <- data$gender
# gender <- dplyr::recode(gender, "Man" = 1, "Vrouw" = 2, "Onbekend" = 2)
# gender <- as.factor(gender)
# local_df <- data[, !grepl("package_duration_raw|student_number|student_name|birth_date|gender", colnames(data))]

# # Fit the multiple group model
# dif_fit <- mirt::multipleGroup(
#   data = local_df,
#   model = 1,
#   group = gender
# )

# # Perform DIF analysis
# dif <- mirt::DIF(dif_fit, group = gender, which.par = c("a1", "d"), na.rm = TRUE, plotdif = TRUE)

# person fit
# person_fit <- personfit(fit) %>%
#   reframe(
#     infit.outside = prop.table(table(z.infit > 1.96 | z.infit < -1.96)),
#     outfit.outside = prop.table(table(z.outfit > 1.96 | z.outfit < -1.96))
#   ) # lower row = non-fitting people
# person_fit$Correct <- c("Correct", "Incorrect")
# person_fit <- person_fit %>% dplyr::select(Correct, everything())
```

###### Model Fit

Hieronder bevinden zich de verschillende model fit indices.Hierbij een korte uitleg wat het exact betekent:

- M2: Dit is de M2-statistiek van Maydeu-Olivares, die een maat is voor de absolute fit van het model. Een lager getal duidt op een betere fit. Er is geen vaste cut-off waarde, maar over het algemeen wordt een lagere waarde als beter beschouwd. Als de bijbehorende p-waarde kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- df: de degrees of freedom van het model. Dit is het aantal parameters dat is geschat, minus het aantal constraints dat is opgelegd.
- p-value: de p-waarde van de Goodness of fit test. Als deze kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- RMSEA: RMSEA meet hoe goed een model de waargenomen data benadert, rekening houdend met de fout in het model. Lagere waarden duiden op een betere pasvorm, typisch onder 0.05 voor goede pasvorm en tussen 0.05 en 0.08 voor redelijke pasvorm.
  - RMSEA_5 en RMSEA_95: de 95% betrouwbaarheidsinterval van de RMSEA. Hoe kleiner dit interval rondom de RMSEA waarde, hoe zekerder we zijn van die waarde. 
- SRMSR: SRMSR staat voor "Standardized Root Mean Square Residual". Het is een maat voor de gemiddelde discrepantie tussen de waargenomen en voorspelde covarianties. Een lagere waarde duidt op een betere fit. Een SRMSR van minder dan 0.08 wordt over het algemeen als goed beschouwd.
- TLI: de Tucker-Lewis Index. TLI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.
- CFI: de Comparative Fit Index. De CFI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.
- N: het aantal respondenten in de steekproef.

```{r leerling contact met leeftijdsgenoten - model fit, echo=FALSE, message=FALSE, warning=FALSE}
# model_fit$`Conclusion` <- c("", "", "Poor", "Acceptable", "", "", "Good", "Poor", "Poor")
# knitr::kable(model_fit)
```

###### Item Fit

Hieronder bevinden zich de verschillende item fit indices. Hierbij een korte uitleg wat het exact betekent:

- X2: Dit is de X2-statistiek van de item fit test. Een lager getal duidt op een betere fit. Er is geen vaste cut-off waarde, maar over het algemeen wordt een lagere waarde als beter beschouwd. Als de bijbehorende p-waarde kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het item niet goed past.
- df: de degrees of freedom van het model. Dit is het aantal parameters dat is geschat, minus het aantal constraints dat is opgelegd.
- p-value: de p-waarde van de Goodness of fit test. Als deze kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- RMSEA: RMSEA meet hoe goed een model de waargenomen data benadert, rekening houdend met de fout in het model. Lagere waarden duiden op een betere pasvorm, typisch onder 0.05 voor goede pasvorm en tussen 0.05 en 0.08 voor redelijke pasvorm.
 
```{r leerling contact met leeftijdsgenoten - item fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(item_fit)
```


Als bepaalde waarden niet goed passen in dit model, kan dit verschillende dingen betekenen.
Ten eerste kan het zijn dat de items niet allemaal dezelfde onderliggende eigenschap meten. Ten tweede kan het zijn dat sommige items verkeerd worden begrepen door de respondenten.
Ten derde kan het zijn dat de veronderstelling van lokale onafhankelijkheid wordt geschonden. Dit betekent dat de reacties op de items afhankelijk zijn van elkaar, zelfs na controle voor de onderliggende eigenschap. Bijvoorbeeld, als het beantwoorden van het ene item helpt bij het beantwoorden van een ander item, dan zullen deze items niet goed passen in het model.

Hieronder volgt een robuste variant van de X2 test, namelijk de PV_Q1 test. Hieronder volgt een korte uitleg wat het exact betekent:

De 'PV_Q1'-statistiek is een afgeleide versie van de Q1-statistiek, een chi-kwadraatstatistiek gebruikt in item respons theorie (IRT) modellen om te testen of een item goed past bij het model. 
Een hoge 'PV_Q1'-waarde duidt op een slechte overeenkomst tussen de waargenomen gegevens en voorspellingen van het model, wat suggereert dat het item mogelijk niet conform het verwachte gedrag 
volgens het IRT-model functioneert. Een niet-significante p-waarde, meestal p > 0.05, in combinatie met lage 'PV_Q1'- en RMSEA.PV_Q1-waarden, geeft aan dat het item goed past bij het IRT-model.
Daarnaast is het belangrijk op te merken dat de 'PV_Q1'-statistiek robuust is tegen invloeden van steekproefomvang en lokale afhankelijkheid, waardoor het een betrouwbaar instrument is om de pasvorm
van een item met het IRT-model te beoordelen, ongeacht de omvang van de steekproef of eventuele lokale onderlinge afhankelijkheden tussen de waarnemingen. Hieronder de resultaten:

```{r leerling contact met leeftijdsgenoten - robust item fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(robust_item_fit)
```

Om een beter beeld te krijgen, volgen hieronder the item parameters van de vragen. 
In IRT, meet een factor loading (ook bekend als een discriminatieparameter) hoe goed een item onderscheid maakt tussen personen met verschillende niveaus van de onderliggende eigenschap. Een hoge factor loading betekent dat het item gevoelig is voor verschillen in de onderliggende eigenschap.
Het belangrijkste verschil met factor analyse, is dat in IRT, de factor loadings ook rekening houden met de moeilijkheidsgraad van de items. Dit betekent dat items die dezelfde moeilijkheidsgraad hebben (dat wil zeggen, ze zijn even moeilijk om te beantwoorden) worden geacht dezelfde hoeveelheid informatie te geven over de onderliggende eigenschap, ongeacht hun factor loadings. Dit is niet het geval in factoranalyse, waar de factor loadings alleen de relatie tussen de geobserveerde variabelen en de onderliggende factor meten.
Tevens kan het zijn dat de factor loadings en de item parameters niet overeenkomen. Bijvoorbeeld een item kan een hoge factor loading hebben (wat aangeeft dat het goed discrimineert) maar toch een slechte fit hebben (X2-statistiek) als het bijvoorbeeld verkeerd wordt begrepen door de respondenten. Omgekeerd kan een item goed passen bij het model maar niet goed discrimineren tussen verschillende niveaus van de eigenschap.
In een Graded response model, zoals hier gebruikt, zijn er meerdere moeilijkheids parameters, een voor elke vrijheidsgraaad van de vraag (totaal antwoordmogelijk - 1). 

```{r leerling contact met leeftijdsgenoten - item parameters, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(params)
params
```


##### interesse in school

```{r leerling interesse in school - stat extraction, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
# Model fit

fit <- irt_fit$gedrag_houding_interesse_in_school_irt_fit
model_fit <- as.data.frame(M2(fit, na.rm = TRUE))
model_fit <- cbind(model_fit, nrow(data))
model_fit <- rename(model_fit, `N` = `nrow(data)`)
model_fit <- tidyr::pivot_longer(model_fit, cols = -N, names_to = "Fit Index", values_to = "Value")
model_fit <- dplyr::select(model_fit, `Fit Index`, Value)
model_fit$`Cut-Off` <- c("", "", "> .05", "< .05 | .05 - .1 | > .1", "", "", "< .08 | .08 - .10 | > .10", "> .95 | .90 - .95 | < .90", "> .95 | .90 - .95 | < .90")

# item fit
item_fit <- itemfit(fit, na.rm = TRUE, fit_stats = "PV_Q1")
colnames(item_fit) <- c("Item", "X2", "df", "RMSEA", "p")
item_fit <- item_fit[order(item_fit$X2), ]
rownames(item_fit) <- NULL

robust_item_fit <- itemfit(fit, na.rm = TRUE)
colnames(robust_item_fit) <- c("Item", "PV_Q1", "df", "RMSEA", "p")
robust_item_fit <- robust_item_fit[order(robust_item_fit$PV_Q1), ]
rownames(robust_item_fit) <- NULL

# Item parameters
params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
params <- params[, -which(colnames(params) %in% c("items.g", "items.u", "means", "F1"))]
sink("/dev/null")
sum <- suppressWarnings(suppressMessages(summary(fit, silent = TRUE)))
sink()
params <- cbind(params, sum$rotF)
params <- cbind(params, sum$h2)
params <- params[, c("F1", colnames(params)[-which(colnames(params) %in% "F1")])]
colnames(params) <- c("Factor Loading", "Discrimination Parameter", "Difficulty Parameter 1", "Difficulty Parameter 2", "Difficulty Parameter 3", "Difficulty Parameter 4", "Communalities")
params <- params[order(params$`Factor Loading`, decreasing = TRUE), ]


# # Differential Item Functionining DIF
# # Prepare the data
# gender <- data$gender
# gender <- dplyr::recode(gender, "Man" = 1, "Vrouw" = 2, "Onbekend" = 2)
# gender <- as.factor(gender)
# local_df <- data[, !grepl("package_duration_raw|student_number|student_name|birth_date|gender", colnames(data))]

# # Fit the multiple group model
# dif_fit <- mirt::multipleGroup(
#   data = local_df,
#   model = 1,
#   group = gender
# )

# # Perform DIF analysis
# dif <- mirt::DIF(dif_fit, group = gender, which.par = c("a1", "d"), na.rm = TRUE, plotdif = TRUE)

# person fit
# person_fit <- personfit(fit) %>%
#   reframe(
#     infit.outside = prop.table(table(z.infit > 1.96 | z.infit < -1.96)),
#     outfit.outside = prop.table(table(z.outfit > 1.96 | z.outfit < -1.96))
#   ) # lower row = non-fitting people
# person_fit$Correct <- c("Correct", "Incorrect")
# person_fit <- person_fit %>% dplyr::select(Correct, everything())
```

###### Model Fit

Hieronder bevinden zich de verschillende model fit indices.Hierbij een korte uitleg wat het exact betekent:

- M2: Dit is de M2-statistiek van Maydeu-Olivares, die een maat is voor de absolute fit van het model. Een lager getal duidt op een betere fit. Er is geen vaste cut-off waarde, maar over het algemeen wordt een lagere waarde als beter beschouwd. Als de bijbehorende p-waarde kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- df: de degrees of freedom van het model. Dit is het aantal parameters dat is geschat, minus het aantal constraints dat is opgelegd.
- p-value: de p-waarde van de Goodness of fit test. Als deze kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- RMSEA: RMSEA meet hoe goed een model de waargenomen data benadert, rekening houdend met de fout in het model. Lagere waarden duiden op een betere pasvorm, typisch onder 0.05 voor goede pasvorm en tussen 0.05 en 0.08 voor redelijke pasvorm.
  - RMSEA_5 en RMSEA_95: de 95% betrouwbaarheidsinterval van de RMSEA. Hoe kleiner dit interval rondom de RMSEA waarde, hoe zekerder we zijn van die waarde. 
- SRMSR: SRMSR staat voor "Standardized Root Mean Square Residual". Het is een maat voor de gemiddelde discrepantie tussen de waargenomen en voorspelde covarianties. Een lagere waarde duidt op een betere fit. Een SRMSR van minder dan 0.08 wordt over het algemeen als goed beschouwd.
- TLI: de Tucker-Lewis Index. TLI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.
- CFI: de Comparative Fit Index. De CFI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.
- N: het aantal respondenten in de steekproef.

```{r leerling interesse in school - model fit, echo=FALSE, message=FALSE, warning=FALSE}
model_fit$`Conclusion` <- c("", "", "Poor", "Acceptable", "", "", "Good", "Poor", "Poor")
knitr::kable(model_fit)
```

###### Item Fit

Hieronder bevinden zich de verschillende item fit indices. Hierbij een korte uitleg wat het exact betekent:

- X2: Dit is de X2-statistiek van de item fit test. Een lager getal duidt op een betere fit. Er is geen vaste cut-off waarde, maar over het algemeen wordt een lagere waarde als beter beschouwd. Als de bijbehorende p-waarde kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het item niet goed past.
- df: de degrees of freedom van het model. Dit is het aantal parameters dat is geschat, minus het aantal constraints dat is opgelegd.
- p-value: de p-waarde van de Goodness of fit test. Als deze kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- RMSEA: RMSEA meet hoe goed een model de waargenomen data benadert, rekening houdend met de fout in het model. Lagere waarden duiden op een betere pasvorm, typisch onder 0.05 voor goede pasvorm en tussen 0.05 en 0.08 voor redelijke pasvorm.
 
```{r leerling interesse in school - item fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(item_fit)
```


Als bepaalde waarden niet goed passen in dit model, kan dit verschillende dingen betekenen.
Ten eerste kan het zijn dat de items niet allemaal dezelfde onderliggende eigenschap meten. Ten tweede kan het zijn dat sommige items verkeerd worden begrepen door de respondenten.
Ten derde kan het zijn dat de veronderstelling van lokale onafhankelijkheid wordt geschonden. Dit betekent dat de reacties op de items afhankelijk zijn van elkaar, zelfs na controle voor de onderliggende eigenschap. Bijvoorbeeld, als het beantwoorden van het ene item helpt bij het beantwoorden van een ander item, dan zullen deze items niet goed passen in het model.

Hieronder volgt een robuste variant van de X2 test, namelijk de PV_Q1 test. Hieronder volgt een korte uitleg wat het exact betekent:

De 'PV_Q1'-statistiek is een afgeleide versie van de Q1-statistiek, een chi-kwadraatstatistiek gebruikt in item respons theorie (IRT) modellen om te testen of een item goed past bij het model. 
Een hoge 'PV_Q1'-waarde duidt op een slechte overeenkomst tussen de waargenomen gegevens en voorspellingen van het model, wat suggereert dat het item mogelijk niet conform het verwachte gedrag 
volgens het IRT-model functioneert. Een niet-significante p-waarde, meestal p > 0.05, in combinatie met lage 'PV_Q1'- en RMSEA.PV_Q1-waarden, geeft aan dat het item goed past bij het IRT-model.
Daarnaast is het belangrijk op te merken dat de 'PV_Q1'-statistiek robuust is tegen invloeden van steekproefomvang en lokale afhankelijkheid, waardoor het een betrouwbaar instrument is om de pasvorm
van een item met het IRT-model te beoordelen, ongeacht de omvang van de steekproef of eventuele lokale onderlinge afhankelijkheden tussen de waarnemingen. Hieronder de resultaten:

```{r leerling interesse in school - robust item fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(robust_item_fit)
```

Om een beter beeld te krijgen, volgen hieronder the item parameters van de vragen. 
In IRT, meet een factor loading (ook bekend als een discriminatieparameter) hoe goed een item onderscheid maakt tussen personen met verschillende niveaus van de onderliggende eigenschap. Een hoge factor loading betekent dat het item gevoelig is voor verschillen in de onderliggende eigenschap.
Het belangrijkste verschil met factor analyse, is dat in IRT, de factor loadings ook rekening houden met de moeilijkheidsgraad van de items. Dit betekent dat items die dezelfde moeilijkheidsgraad hebben (dat wil zeggen, ze zijn even moeilijk om te beantwoorden) worden geacht dezelfde hoeveelheid informatie te geven over de onderliggende eigenschap, ongeacht hun factor loadings. Dit is niet het geval in factoranalyse, waar de factor loadings alleen de relatie tussen de geobserveerde variabelen en de onderliggende factor meten.
Tevens kan het zijn dat de factor loadings en de item parameters niet overeenkomen. Bijvoorbeeld een item kan een hoge factor loading hebben (wat aangeeft dat het goed discrimineert) maar toch een slechte fit hebben (X2-statistiek) als het bijvoorbeeld verkeerd wordt begrepen door de respondenten. Omgekeerd kan een item goed passen bij het model maar niet goed discrimineren tussen verschillende niveaus van de eigenschap.
In een Graded response model, zoals hier gebruikt, zijn er meerdere moeilijkheids parameters, een voor elke vrijheidsgraaad van de vraag (totaal antwoordmogelijk - 1). 

```{r leerling interesse in school - item parameters, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(params)
params
```

##### motivatie om te presteren

```{r leerling motivatie om te presteren - stat extraction, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
# Model fit

fit <- irt_fit$gedrag_houding_motivatie_om_te_presteren_irt_fit
model_fit <- as.data.frame(M2(fit, na.rm = TRUE))
model_fit <- cbind(model_fit, nrow(data))
model_fit <- rename(model_fit, `N` = `nrow(data)`)
model_fit <- tidyr::pivot_longer(model_fit, cols = -N, names_to = "Fit Index", values_to = "Value")
model_fit <- dplyr::select(model_fit, `Fit Index`, Value)
model_fit$`Cut-Off` <- c("", "", "> .05", "< .05 | .05 - .1 | > .1", "", "", "< .08 | .08 - .10 | > .10", "> .95 | .90 - .95 | < .90", "> .95 | .90 - .95 | < .90")

# item fit
item_fit <- itemfit(fit, na.rm = TRUE, fit_stats = "PV_Q1")
colnames(item_fit) <- c("Item", "X2", "df", "RMSEA", "p")
item_fit <- item_fit[order(item_fit$X2), ]
rownames(item_fit) <- NULL

robust_item_fit <- itemfit(fit, na.rm = TRUE)
colnames(robust_item_fit) <- c("Item", "PV_Q1", "df", "RMSEA", "p")
robust_item_fit <- robust_item_fit[order(robust_item_fit$PV_Q1), ]
rownames(robust_item_fit) <- NULL

# Item parameters
params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
params <- params[, -which(colnames(params) %in% c("items.g", "items.u", "means", "F1"))]
sink("/dev/null")
sum <- suppressWarnings(suppressMessages(summary(fit, silent = TRUE)))
sink()
params <- cbind(params, sum$rotF)
params <- cbind(params, sum$h2)
params <- params[, c("F1", colnames(params)[-which(colnames(params) %in% "F1")])]
colnames(params) <- c("Factor Loading", "Discrimination Parameter", "Difficulty Parameter 1", "Difficulty Parameter 2", "Difficulty Parameter 3", "Difficulty Parameter 4", "Communalities")
params <- params[order(params$`Factor Loading`, decreasing = TRUE), ]


# # Differential Item Functionining DIF
# # Prepare the data
# gender <- data$gender
# gender <- dplyr::recode(gender, "Man" = 1, "Vrouw" = 2, "Onbekend" = 2)
# gender <- as.factor(gender)
# local_df <- data[, !grepl("package_duration_raw|student_number|student_name|birth_date|gender", colnames(data))]

# # Fit the multiple group model
# dif_fit <- mirt::multipleGroup(
#   data = local_df,
#   model = 1,
#   group = gender
# )

# # Perform DIF analysis
# dif <- mirt::DIF(dif_fit, group = gender, which.par = c("a1", "d"), na.rm = TRUE, plotdif = TRUE)

# person fit
# person_fit <- personfit(fit) %>%
#   reframe(
#     infit.outside = prop.table(table(z.infit > 1.96 | z.infit < -1.96)),
#     outfit.outside = prop.table(table(z.outfit > 1.96 | z.outfit < -1.96))
#   ) # lower row = non-fitting people
# person_fit$Correct <- c("Correct", "Incorrect")
# person_fit <- person_fit %>% dplyr::select(Correct, everything())
```

###### Model Fit

Hieronder bevinden zich de verschillende model fit indices.Hierbij een korte uitleg wat het exact betekent:

- M2: Dit is de M2-statistiek van Maydeu-Olivares, die een maat is voor de absolute fit van het model. Een lager getal duidt op een betere fit. Er is geen vaste cut-off waarde, maar over het algemeen wordt een lagere waarde als beter beschouwd. Als de bijbehorende p-waarde kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- df: de degrees of freedom van het model. Dit is het aantal parameters dat is geschat, minus het aantal constraints dat is opgelegd.
- p-value: de p-waarde van de Goodness of fit test. Als deze kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- RMSEA: RMSEA meet hoe goed een model de waargenomen data benadert, rekening houdend met de fout in het model. Lagere waarden duiden op een betere pasvorm, typisch onder 0.05 voor goede pasvorm en tussen 0.05 en 0.08 voor redelijke pasvorm.
  - RMSEA_5 en RMSEA_95: de 95% betrouwbaarheidsinterval van de RMSEA. Hoe kleiner dit interval rondom de RMSEA waarde, hoe zekerder we zijn van die waarde. 
- SRMSR: SRMSR staat voor "Standardized Root Mean Square Residual". Het is een maat voor de gemiddelde discrepantie tussen de waargenomen en voorspelde covarianties. Een lagere waarde duidt op een betere fit. Een SRMSR van minder dan 0.08 wordt over het algemeen als goed beschouwd.
- TLI: de Tucker-Lewis Index. TLI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.
- CFI: de Comparative Fit Index. De CFI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.
- N: het aantal respondenten in de steekproef.

```{r leerling motivatie om te presteren - model fit, echo=FALSE, message=FALSE, warning=FALSE}
model_fit$`Conclusion` <- c("", "", "Poor", "Acceptable", "", "", "Good", "Poor", "Poor")
knitr::kable(model_fit)
```

###### Item Fit

Hieronder bevinden zich de verschillende item fit indices. Hierbij een korte uitleg wat het exact betekent:

- X2: Dit is de X2-statistiek van de item fit test. Een lager getal duidt op een betere fit. Er is geen vaste cut-off waarde, maar over het algemeen wordt een lagere waarde als beter beschouwd. Als de bijbehorende p-waarde kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het item niet goed past.
- df: de degrees of freedom van het model. Dit is het aantal parameters dat is geschat, minus het aantal constraints dat is opgelegd.
- p-value: de p-waarde van de Goodness of fit test. Als deze kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- RMSEA: RMSEA meet hoe goed een model de waargenomen data benadert, rekening houdend met de fout in het model. Lagere waarden duiden op een betere pasvorm, typisch onder 0.05 voor goede pasvorm en tussen 0.05 en 0.08 voor redelijke pasvorm.
 
```{r leerling motivatie om te presteren - item fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(item_fit)
```


Als bepaalde waarden niet goed passen in dit model, kan dit verschillende dingen betekenen.
Ten eerste kan het zijn dat de items niet allemaal dezelfde onderliggende eigenschap meten. Ten tweede kan het zijn dat sommige items verkeerd worden begrepen door de respondenten.
Ten derde kan het zijn dat de veronderstelling van lokale onafhankelijkheid wordt geschonden. Dit betekent dat de reacties op de items afhankelijk zijn van elkaar, zelfs na controle voor de onderliggende eigenschap. Bijvoorbeeld, als het beantwoorden van het ene item helpt bij het beantwoorden van een ander item, dan zullen deze items niet goed passen in het model.

Hieronder volgt een robuste variant van de X2 test, namelijk de PV_Q1 test. Hieronder volgt een korte uitleg wat het exact betekent:

De 'PV_Q1'-statistiek is een afgeleide versie van de Q1-statistiek, een chi-kwadraatstatistiek gebruikt in item respons theorie (IRT) modellen om te testen of een item goed past bij het model. 
Een hoge 'PV_Q1'-waarde duidt op een slechte overeenkomst tussen de waargenomen gegevens en voorspellingen van het model, wat suggereert dat het item mogelijk niet conform het verwachte gedrag 
volgens het IRT-model functioneert. Een niet-significante p-waarde, meestal p > 0.05, in combinatie met lage 'PV_Q1'- en RMSEA.PV_Q1-waarden, geeft aan dat het item goed past bij het IRT-model.
Daarnaast is het belangrijk op te merken dat de 'PV_Q1'-statistiek robuust is tegen invloeden van steekproefomvang en lokale afhankelijkheid, waardoor het een betrouwbaar instrument is om de pasvorm
van een item met het IRT-model te beoordelen, ongeacht de omvang van de steekproef of eventuele lokale onderlinge afhankelijkheden tussen de waarnemingen. Hieronder de resultaten:

```{r leerling motivatie om te presteren - robust item fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(robust_item_fit)
```

Om een beter beeld te krijgen, volgen hieronder the item parameters van de vragen. 
In IRT, meet een factor loading (ook bekend als een discriminatieparameter) hoe goed een item onderscheid maakt tussen personen met verschillende niveaus van de onderliggende eigenschap. Een hoge factor loading betekent dat het item gevoelig is voor verschillen in de onderliggende eigenschap.
Het belangrijkste verschil met factor analyse, is dat in IRT, de factor loadings ook rekening houden met de moeilijkheidsgraad van de items. Dit betekent dat items die dezelfde moeilijkheidsgraad hebben (dat wil zeggen, ze zijn even moeilijk om te beantwoorden) worden geacht dezelfde hoeveelheid informatie te geven over de onderliggende eigenschap, ongeacht hun factor loadings. Dit is niet het geval in factoranalyse, waar de factor loadings alleen de relatie tussen de geobserveerde variabelen en de onderliggende factor meten.
Tevens kan het zijn dat de factor loadings en de item parameters niet overeenkomen. Bijvoorbeeld een item kan een hoge factor loading hebben (wat aangeeft dat het goed discrimineert) maar toch een slechte fit hebben (X2-statistiek) als het bijvoorbeeld verkeerd wordt begrepen door de respondenten. Omgekeerd kan een item goed passen bij het model maar niet goed discrimineren tussen verschillende niveaus van de eigenschap.
In een Graded response model, zoals hier gebruikt, zijn er meerdere moeilijkheids parameters, een voor elke vrijheidsgraaad van de vraag (totaal antwoordmogelijk - 1). 

```{r leerling motivatie om te presteren - item parameters, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(params)
params
```

##### samenwerken

```{r leerling samenwerken - stat extraction, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
# Model fit

fit <- irt_fit$gedrag_houding_samenwerken_irt_fit
model_fit <- as.data.frame(M2(fit, na.rm = TRUE))
model_fit <- cbind(model_fit, nrow(data))
model_fit <- rename(model_fit, `N` = `nrow(data)`)
model_fit <- tidyr::pivot_longer(model_fit, cols = -N, names_to = "Fit Index", values_to = "Value")
model_fit <- dplyr::select(model_fit, `Fit Index`, Value)
model_fit$`Cut-Off` <- c("", "", "> .05", "< .05 | .05 - .1 | > .1", "", "", "< .08 | .08 - .10 | > .10", "> .95 | .90 - .95 | < .90", "> .95 | .90 - .95 | < .90")

# item fit
item_fit <- itemfit(fit, na.rm = TRUE, fit_stats = "PV_Q1")
colnames(item_fit) <- c("Item", "X2", "df", "RMSEA", "p")
item_fit <- item_fit[order(item_fit$X2), ]
rownames(item_fit) <- NULL

robust_item_fit <- itemfit(fit, na.rm = TRUE)
colnames(robust_item_fit) <- c("Item", "PV_Q1", "df", "RMSEA", "p")
robust_item_fit <- robust_item_fit[order(robust_item_fit$PV_Q1), ]
rownames(robust_item_fit) <- NULL

# Item parameters
params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
params <- params[, -which(colnames(params) %in% c("items.g", "items.u", "means", "F1"))]
sink("/dev/null")
sum <- suppressWarnings(suppressMessages(summary(fit, silent = TRUE)))
sink()
params <- cbind(params, sum$rotF)
params <- cbind(params, sum$h2)
params <- params[, c("F1", colnames(params)[-which(colnames(params) %in% "F1")])]
colnames(params) <- c("Factor Loading", "Discrimination Parameter", "Difficulty Parameter 1", "Difficulty Parameter 2", "Difficulty Parameter 3", "Difficulty Parameter 4", "Communalities")
params <- params[order(params$`Factor Loading`, decreasing = TRUE), ]


# # Differential Item Functionining DIF
# # Prepare the data
# gender <- data$gender
# gender <- dplyr::recode(gender, "Man" = 1, "Vrouw" = 2, "Onbekend" = 2)
# gender <- as.factor(gender)
# local_df <- data[, !grepl("package_duration_raw|student_number|student_name|birth_date|gender", colnames(data))]

# # Fit the multiple group model
# dif_fit <- mirt::multipleGroup(
#   data = local_df,
#   model = 1,
#   group = gender
# )

# # Perform DIF analysis
# dif <- mirt::DIF(dif_fit, group = gender, which.par = c("a1", "d"), na.rm = TRUE, plotdif = TRUE)

# person fit
# person_fit <- personfit(fit) %>%
#   reframe(
#     infit.outside = prop.table(table(z.infit > 1.96 | z.infit < -1.96)),
#     outfit.outside = prop.table(table(z.outfit > 1.96 | z.outfit < -1.96))
#   ) # lower row = non-fitting people
# person_fit$Correct <- c("Correct", "Incorrect")
# person_fit <- person_fit %>% dplyr::select(Correct, everything())
```

###### Model Fit

Hieronder bevinden zich de verschillende model fit indices.Hierbij een korte uitleg wat het exact betekent:

- M2: Dit is de M2-statistiek van Maydeu-Olivares, die een maat is voor de absolute fit van het model. Een lager getal duidt op een betere fit. Er is geen vaste cut-off waarde, maar over het algemeen wordt een lagere waarde als beter beschouwd. Als de bijbehorende p-waarde kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- df: de degrees of freedom van het model. Dit is het aantal parameters dat is geschat, minus het aantal constraints dat is opgelegd.
- p-value: de p-waarde van de Goodness of fit test. Als deze kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- RMSEA: RMSEA meet hoe goed een model de waargenomen data benadert, rekening houdend met de fout in het model. Lagere waarden duiden op een betere pasvorm, typisch onder 0.05 voor goede pasvorm en tussen 0.05 en 0.08 voor redelijke pasvorm.
  - RMSEA_5 en RMSEA_95: de 95% betrouwbaarheidsinterval van de RMSEA. Hoe kleiner dit interval rondom de RMSEA waarde, hoe zekerder we zijn van die waarde. 
- SRMSR: SRMSR staat voor "Standardized Root Mean Square Residual". Het is een maat voor de gemiddelde discrepantie tussen de waargenomen en voorspelde covarianties. Een lagere waarde duidt op een betere fit. Een SRMSR van minder dan 0.08 wordt over het algemeen als goed beschouwd.
- TLI: de Tucker-Lewis Index. TLI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.
- CFI: de Comparative Fit Index. De CFI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.
- N: het aantal respondenten in de steekproef.

```{r leerling samenwerken - model fit, echo=FALSE, message=FALSE, warning=FALSE}
model_fit$`Conclusion` <- c("", "", "Poor", "Acceptable", "", "", "Good", "Poor", "Poor")
knitr::kable(model_fit)
```

###### Item Fit

Hieronder bevinden zich de verschillende item fit indices. Hierbij een korte uitleg wat het exact betekent:

- X2: Dit is de X2-statistiek van de item fit test. Een lager getal duidt op een betere fit. Er is geen vaste cut-off waarde, maar over het algemeen wordt een lagere waarde als beter beschouwd. Als de bijbehorende p-waarde kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het item niet goed past.
- df: de degrees of freedom van het model. Dit is het aantal parameters dat is geschat, minus het aantal constraints dat is opgelegd.
- p-value: de p-waarde van de Goodness of fit test. Als deze kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- RMSEA: RMSEA meet hoe goed een model de waargenomen data benadert, rekening houdend met de fout in het model. Lagere waarden duiden op een betere pasvorm, typisch onder 0.05 voor goede pasvorm en tussen 0.05 en 0.08 voor redelijke pasvorm.
 
```{r leerling samenwerken - item fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(item_fit)
```


Als bepaalde waarden niet goed passen in dit model, kan dit verschillende dingen betekenen.
Ten eerste kan het zijn dat de items niet allemaal dezelfde onderliggende eigenschap meten. Ten tweede kan het zijn dat sommige items verkeerd worden begrepen door de respondenten.
Ten derde kan het zijn dat de veronderstelling van lokale onafhankelijkheid wordt geschonden. Dit betekent dat de reacties op de items afhankelijk zijn van elkaar, zelfs na controle voor de onderliggende eigenschap. Bijvoorbeeld, als het beantwoorden van het ene item helpt bij het beantwoorden van een ander item, dan zullen deze items niet goed passen in het model.

Hieronder volgt een robuste variant van de X2 test, namelijk de PV_Q1 test. Hieronder volgt een korte uitleg wat het exact betekent:

De 'PV_Q1'-statistiek is een afgeleide versie van de Q1-statistiek, een chi-kwadraatstatistiek gebruikt in item respons theorie (IRT) modellen om te testen of een item goed past bij het model. 
Een hoge 'PV_Q1'-waarde duidt op een slechte overeenkomst tussen de waargenomen gegevens en voorspellingen van het model, wat suggereert dat het item mogelijk niet conform het verwachte gedrag 
volgens het IRT-model functioneert. Een niet-significante p-waarde, meestal p > 0.05, in combinatie met lage 'PV_Q1'- en RMSEA.PV_Q1-waarden, geeft aan dat het item goed past bij het IRT-model.
Daarnaast is het belangrijk op te merken dat de 'PV_Q1'-statistiek robuust is tegen invloeden van steekproefomvang en lokale afhankelijkheid, waardoor het een betrouwbaar instrument is om de pasvorm
van een item met het IRT-model te beoordelen, ongeacht de omvang van de steekproef of eventuele lokale onderlinge afhankelijkheden tussen de waarnemingen. Hieronder de resultaten:

```{r leerling samenwerken - robust item fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(robust_item_fit)
```

Om een beter beeld te krijgen, volgen hieronder the item parameters van de vragen. 
In IRT, meet een factor loading (ook bekend als een discriminatieparameter) hoe goed een item onderscheid maakt tussen personen met verschillende niveaus van de onderliggende eigenschap. Een hoge factor loading betekent dat het item gevoelig is voor verschillen in de onderliggende eigenschap.
Het belangrijkste verschil met factor analyse, is dat in IRT, de factor loadings ook rekening houden met de moeilijkheidsgraad van de items. Dit betekent dat items die dezelfde moeilijkheidsgraad hebben (dat wil zeggen, ze zijn even moeilijk om te beantwoorden) worden geacht dezelfde hoeveelheid informatie te geven over de onderliggende eigenschap, ongeacht hun factor loadings. Dit is niet het geval in factoranalyse, waar de factor loadings alleen de relatie tussen de geobserveerde variabelen en de onderliggende factor meten.
Tevens kan het zijn dat de factor loadings en de item parameters niet overeenkomen. Bijvoorbeeld een item kan een hoge factor loading hebben (wat aangeeft dat het goed discrimineert) maar toch een slechte fit hebben (X2-statistiek) als het bijvoorbeeld verkeerd wordt begrepen door de respondenten. Omgekeerd kan een item goed passen bij het model maar niet goed discrimineren tussen verschillende niveaus van de eigenschap.
In een Graded response model, zoals hier gebruikt, zijn er meerdere moeilijkheids parameters, een voor elke vrijheidsgraaad van de vraag (totaal antwoordmogelijk - 1). 

```{r leerling samenwerken - item parameters, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(params)
params
```


##### zelfvertrouwen

```{r leerling zelfvertrouwen - stat extraction, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
# Model fit

fit <- irt_fit$gedrag_houding_zelfvertrouwen_irt_fit
model_fit <- as.data.frame(M2(fit, na.rm = TRUE))
model_fit <- cbind(model_fit, nrow(data))
model_fit <- rename(model_fit, `N` = `nrow(data)`)
model_fit <- tidyr::pivot_longer(model_fit, cols = -N, names_to = "Fit Index", values_to = "Value")
model_fit <- dplyr::select(model_fit, `Fit Index`, Value)
model_fit$`Cut-Off` <- c("", "", "> .05", "< .05 | .05 - .1 | > .1", "", "", "< .08 | .08 - .10 | > .10", "> .95 | .90 - .95 | < .90", "> .95 | .90 - .95 | < .90")

# item fit
item_fit <- itemfit(fit, na.rm = TRUE, fit_stats = "PV_Q1")
colnames(item_fit) <- c("Item", "X2", "df", "RMSEA", "p")
item_fit <- item_fit[order(item_fit$X2), ]
rownames(item_fit) <- NULL

robust_item_fit <- itemfit(fit, na.rm = TRUE)
colnames(robust_item_fit) <- c("Item", "PV_Q1", "df", "RMSEA", "p")
robust_item_fit <- robust_item_fit[order(robust_item_fit$PV_Q1), ]
rownames(robust_item_fit) <- NULL

# Item parameters
params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
params <- params[, -which(colnames(params) %in% c("items.g", "items.u", "means", "F1"))]
sink("/dev/null")
sum <- suppressWarnings(suppressMessages(summary(fit, silent = TRUE)))
sink()
params <- cbind(params, sum$rotF)
params <- cbind(params, sum$h2)
params <- params[, c("F1", colnames(params)[-which(colnames(params) %in% "F1")])]
colnames(params) <- c("Factor Loading", "Discrimination Parameter", "Difficulty Parameter 1", "Difficulty Parameter 2", "Difficulty Parameter 3", "Difficulty Parameter 4", "Communalities")
params <- params[order(params$`Factor Loading`, decreasing = TRUE), ]


# # Differential Item Functionining DIF
# # Prepare the data
# gender <- data$gender
# gender <- dplyr::recode(gender, "Man" = 1, "Vrouw" = 2, "Onbekend" = 2)
# gender <- as.factor(gender)
# local_df <- data[, !grepl("package_duration_raw|student_number|student_name|birth_date|gender", colnames(data))]

# # Fit the multiple group model
# dif_fit <- mirt::multipleGroup(
#   data = local_df,
#   model = 1,
#   group = gender
# )

# # Perform DIF analysis
# dif <- mirt::DIF(dif_fit, group = gender, which.par = c("a1", "d"), na.rm = TRUE, plotdif = TRUE)

# person fit
# person_fit <- personfit(fit) %>%
#   reframe(
#     infit.outside = prop.table(table(z.infit > 1.96 | z.infit < -1.96)),
#     outfit.outside = prop.table(table(z.outfit > 1.96 | z.outfit < -1.96))
#   ) # lower row = non-fitting people
# person_fit$Correct <- c("Correct", "Incorrect")
# person_fit <- person_fit %>% dplyr::select(Correct, everything())
```

###### Model Fit

Hieronder bevinden zich de verschillende model fit indices.Hierbij een korte uitleg wat het exact betekent:

- M2: Dit is de M2-statistiek van Maydeu-Olivares, die een maat is voor de absolute fit van het model. Een lager getal duidt op een betere fit. Er is geen vaste cut-off waarde, maar over het algemeen wordt een lagere waarde als beter beschouwd. Als de bijbehorende p-waarde kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- df: de degrees of freedom van het model. Dit is het aantal parameters dat is geschat, minus het aantal constraints dat is opgelegd.
- p-value: de p-waarde van de Goodness of fit test. Als deze kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- RMSEA: RMSEA meet hoe goed een model de waargenomen data benadert, rekening houdend met de fout in het model. Lagere waarden duiden op een betere pasvorm, typisch onder 0.05 voor goede pasvorm en tussen 0.05 en 0.08 voor redelijke pasvorm.
  - RMSEA_5 en RMSEA_95: de 95% betrouwbaarheidsinterval van de RMSEA. Hoe kleiner dit interval rondom de RMSEA waarde, hoe zekerder we zijn van die waarde. 
- SRMSR: SRMSR staat voor "Standardized Root Mean Square Residual". Het is een maat voor de gemiddelde discrepantie tussen de waargenomen en voorspelde covarianties. Een lagere waarde duidt op een betere fit. Een SRMSR van minder dan 0.08 wordt over het algemeen als goed beschouwd.
- TLI: de Tucker-Lewis Index. TLI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.
- CFI: de Comparative Fit Index. De CFI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.
- N: het aantal respondenten in de steekproef.

```{r leerling zelfvertrouwen - model fit, echo=FALSE, message=FALSE, warning=FALSE}
model_fit$`Conclusion` <- c("", "", "Poor", "Acceptable", "", "", "Good", "Poor", "Poor")
knitr::kable(model_fit)
```

###### Item Fit

Hieronder bevinden zich de verschillende item fit indices. Hierbij een korte uitleg wat het exact betekent:

- X2: Dit is de X2-statistiek van de item fit test. Een lager getal duidt op een betere fit. Er is geen vaste cut-off waarde, maar over het algemeen wordt een lagere waarde als beter beschouwd. Als de bijbehorende p-waarde kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het item niet goed past.
- df: de degrees of freedom van het model. Dit is het aantal parameters dat is geschat, minus het aantal constraints dat is opgelegd.
- p-value: de p-waarde van de Goodness of fit test. Als deze kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- RMSEA: RMSEA meet hoe goed een model de waargenomen data benadert, rekening houdend met de fout in het model. Lagere waarden duiden op een betere pasvorm, typisch onder 0.05 voor goede pasvorm en tussen 0.05 en 0.08 voor redelijke pasvorm.
 
```{r leerling zelfvertrouwen - item fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(item_fit)
```


Als bepaalde waarden niet goed passen in dit model, kan dit verschillende dingen betekenen.
Ten eerste kan het zijn dat de items niet allemaal dezelfde onderliggende eigenschap meten. Ten tweede kan het zijn dat sommige items verkeerd worden begrepen door de respondenten.
Ten derde kan het zijn dat de veronderstelling van lokale onafhankelijkheid wordt geschonden. Dit betekent dat de reacties op de items afhankelijk zijn van elkaar, zelfs na controle voor de onderliggende eigenschap. Bijvoorbeeld, als het beantwoorden van het ene item helpt bij het beantwoorden van een ander item, dan zullen deze items niet goed passen in het model.

Hieronder volgt een robuste variant van de X2 test, namelijk de PV_Q1 test. Hieronder volgt een korte uitleg wat het exact betekent:

De 'PV_Q1'-statistiek is een afgeleide versie van de Q1-statistiek, een chi-kwadraatstatistiek gebruikt in item respons theorie (IRT) modellen om te testen of een item goed past bij het model. 
Een hoge 'PV_Q1'-waarde duidt op een slechte overeenkomst tussen de waargenomen gegevens en voorspellingen van het model, wat suggereert dat het item mogelijk niet conform het verwachte gedrag 
volgens het IRT-model functioneert. Een niet-significante p-waarde, meestal p > 0.05, in combinatie met lage 'PV_Q1'- en RMSEA.PV_Q1-waarden, geeft aan dat het item goed past bij het IRT-model.
Daarnaast is het belangrijk op te merken dat de 'PV_Q1'-statistiek robuust is tegen invloeden van steekproefomvang en lokale afhankelijkheid, waardoor het een betrouwbaar instrument is om de pasvorm
van een item met het IRT-model te beoordelen, ongeacht de omvang van de steekproef of eventuele lokale onderlinge afhankelijkheden tussen de waarnemingen. Hieronder de resultaten:

```{r leerling zelfvertrouwen - robust item fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(robust_item_fit)
```

Om een beter beeld te krijgen, volgen hieronder the item parameters van de vragen. 
In IRT, meet een factor loading (ook bekend als een discriminatieparameter) hoe goed een item onderscheid maakt tussen personen met verschillende niveaus van de onderliggende eigenschap. Een hoge factor loading betekent dat het item gevoelig is voor verschillen in de onderliggende eigenschap.
Het belangrijkste verschil met factor analyse, is dat in IRT, de factor loadings ook rekening houden met de moeilijkheidsgraad van de items. Dit betekent dat items die dezelfde moeilijkheidsgraad hebben (dat wil zeggen, ze zijn even moeilijk om te beantwoorden) worden geacht dezelfde hoeveelheid informatie te geven over de onderliggende eigenschap, ongeacht hun factor loadings. Dit is niet het geval in factoranalyse, waar de factor loadings alleen de relatie tussen de geobserveerde variabelen en de onderliggende factor meten.
Tevens kan het zijn dat de factor loadings en de item parameters niet overeenkomen. Bijvoorbeeld een item kan een hoge factor loading hebben (wat aangeeft dat het goed discrimineert) maar toch een slechte fit hebben (X2-statistiek) als het bijvoorbeeld verkeerd wordt begrepen door de respondenten. Omgekeerd kan een item goed passen bij het model maar niet goed discrimineren tussen verschillende niveaus van de eigenschap.
In een Graded response model, zoals hier gebruikt, zijn er meerdere moeilijkheids parameters, een voor elke vrijheidsgraaad van de vraag (totaal antwoordmogelijk - 1). 

```{r leerling zelfvertrouwen - item parameters, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(params)
params
```


##### stabiel

```{r leerling stabiel - stat extraction, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
# Model fit

fit <- irt_fit$gedrag_houding_stabiel_irt_fit
model_fit <- as.data.frame(M2(fit, na.rm = TRUE))
model_fit <- cbind(model_fit, nrow(data))
model_fit <- rename(model_fit, `N` = `nrow(data)`)
model_fit <- tidyr::pivot_longer(model_fit, cols = -N, names_to = "Fit Index", values_to = "Value")
model_fit <- dplyr::select(model_fit, `Fit Index`, Value)
model_fit$`Cut-Off` <- c("", "", "> .05", "< .05 | .05 - .1 | > .1", "", "", "< .08 | .08 - .10 | > .10", "> .95 | .90 - .95 | < .90", "> .95 | .90 - .95 | < .90")

# item fit
item_fit <- itemfit(fit, na.rm = TRUE, fit_stats = "PV_Q1")
colnames(item_fit) <- c("Item", "X2", "df", "RMSEA", "p")
item_fit <- item_fit[order(item_fit$X2), ]
rownames(item_fit) <- NULL

robust_item_fit <- itemfit(fit, na.rm = TRUE)
colnames(robust_item_fit) <- c("Item", "PV_Q1", "df", "RMSEA", "p")
robust_item_fit <- robust_item_fit[order(robust_item_fit$PV_Q1), ]
rownames(robust_item_fit) <- NULL

# Item parameters
params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
params <- params[, -which(colnames(params) %in% c("items.g", "items.u", "means", "F1"))]
sink("/dev/null")
sum <- suppressWarnings(suppressMessages(summary(fit, silent = TRUE)))
sink()
params <- cbind(params, sum$rotF)
params <- cbind(params, sum$h2)
params <- params[, c("F1", colnames(params)[-which(colnames(params) %in% "F1")])]
colnames(params) <- c("Factor Loading", "Discrimination Parameter", "Difficulty Parameter 1", "Difficulty Parameter 2", "Difficulty Parameter 3", "Difficulty Parameter 4", "Communalities")
params <- params[order(params$`Factor Loading`, decreasing = TRUE), ]


# # Differential Item Functionining DIF
# # Prepare the data
# gender <- data$gender
# gender <- dplyr::recode(gender, "Man" = 1, "Vrouw" = 2, "Onbekend" = 2)
# gender <- as.factor(gender)
# local_df <- data[, !grepl("package_duration_raw|student_number|student_name|birth_date|gender", colnames(data))]

# # Fit the multiple group model
# dif_fit <- mirt::multipleGroup(
#   data = local_df,
#   model = 1,
#   group = gender
# )

# # Perform DIF analysis
# dif <- mirt::DIF(dif_fit, group = gender, which.par = c("a1", "d"), na.rm = TRUE, plotdif = TRUE)

# person fit
# person_fit <- personfit(fit) %>%
#   reframe(
#     infit.outside = prop.table(table(z.infit > 1.96 | z.infit < -1.96)),
#     outfit.outside = prop.table(table(z.outfit > 1.96 | z.outfit < -1.96))
#   ) # lower row = non-fitting people
# person_fit$Correct <- c("Correct", "Incorrect")
# person_fit <- person_fit %>% dplyr::select(Correct, everything())
```

###### Model Fit

Hieronder bevinden zich de verschillende model fit indices.Hierbij een korte uitleg wat het exact betekent:

- M2: Dit is de M2-statistiek van Maydeu-Olivares, die een maat is voor de absolute fit van het model. Een lager getal duidt op een betere fit. Er is geen vaste cut-off waarde, maar over het algemeen wordt een lagere waarde als beter beschouwd. Als de bijbehorende p-waarde kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- df: de degrees of freedom van het model. Dit is het aantal parameters dat is geschat, minus het aantal constraints dat is opgelegd.
- p-value: de p-waarde van de Goodness of fit test. Als deze kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- RMSEA: RMSEA meet hoe goed een model de waargenomen data benadert, rekening houdend met de fout in het model. Lagere waarden duiden op een betere pasvorm, typisch onder 0.05 voor goede pasvorm en tussen 0.05 en 0.08 voor redelijke pasvorm.
  - RMSEA_5 en RMSEA_95: de 95% betrouwbaarheidsinterval van de RMSEA. Hoe kleiner dit interval rondom de RMSEA waarde, hoe zekerder we zijn van die waarde. 
- SRMSR: SRMSR staat voor "Standardized Root Mean Square Residual". Het is een maat voor de gemiddelde discrepantie tussen de waargenomen en voorspelde covarianties. Een lagere waarde duidt op een betere fit. Een SRMSR van minder dan 0.08 wordt over het algemeen als goed beschouwd.
- TLI: de Tucker-Lewis Index. TLI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.
- CFI: de Comparative Fit Index. De CFI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.
- N: het aantal respondenten in de steekproef.

```{r leerling stabiel - model fit, echo=FALSE, message=FALSE, warning=FALSE}
model_fit$`Conclusion` <- c("", "", "Poor", "Acceptable", "", "", "Good", "Poor", "Poor")
knitr::kable(model_fit)
```

###### Item Fit

Hieronder bevinden zich de verschillende item fit indices. Hierbij een korte uitleg wat het exact betekent:

- X2: Dit is de X2-statistiek van de item fit test. Een lager getal duidt op een betere fit. Er is geen vaste cut-off waarde, maar over het algemeen wordt een lagere waarde als beter beschouwd. Als de bijbehorende p-waarde kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het item niet goed past.
- df: de degrees of freedom van het model. Dit is het aantal parameters dat is geschat, minus het aantal constraints dat is opgelegd.
- p-value: de p-waarde van de Goodness of fit test. Als deze kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- RMSEA: RMSEA meet hoe goed een model de waargenomen data benadert, rekening houdend met de fout in het model. Lagere waarden duiden op een betere pasvorm, typisch onder 0.05 voor goede pasvorm en tussen 0.05 en 0.08 voor redelijke pasvorm.
 
```{r leerling stabiel - item fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(item_fit)
```


Als bepaalde waarden niet goed passen in dit model, kan dit verschillende dingen betekenen.
Ten eerste kan het zijn dat de items niet allemaal dezelfde onderliggende eigenschap meten. Ten tweede kan het zijn dat sommige items verkeerd worden begrepen door de respondenten.
Ten derde kan het zijn dat de veronderstelling van lokale onafhankelijkheid wordt geschonden. Dit betekent dat de reacties op de items afhankelijk zijn van elkaar, zelfs na controle voor de onderliggende eigenschap. Bijvoorbeeld, als het beantwoorden van het ene item helpt bij het beantwoorden van een ander item, dan zullen deze items niet goed passen in het model.

Hieronder volgt een robuste variant van de X2 test, namelijk de PV_Q1 test. Hieronder volgt een korte uitleg wat het exact betekent:

De 'PV_Q1'-statistiek is een afgeleide versie van de Q1-statistiek, een chi-kwadraatstatistiek gebruikt in item respons theorie (IRT) modellen om te testen of een item goed past bij het model. 
Een hoge 'PV_Q1'-waarde duidt op een slechte overeenkomst tussen de waargenomen gegevens en voorspellingen van het model, wat suggereert dat het item mogelijk niet conform het verwachte gedrag 
volgens het IRT-model functioneert. Een niet-significante p-waarde, meestal p > 0.05, in combinatie met lage 'PV_Q1'- en RMSEA.PV_Q1-waarden, geeft aan dat het item goed past bij het IRT-model.
Daarnaast is het belangrijk op te merken dat de 'PV_Q1'-statistiek robuust is tegen invloeden van steekproefomvang en lokale afhankelijkheid, waardoor het een betrouwbaar instrument is om de pasvorm
van een item met het IRT-model te beoordelen, ongeacht de omvang van de steekproef of eventuele lokale onderlinge afhankelijkheden tussen de waarnemingen. Hieronder de resultaten:

```{r leerling stabiel - robust item fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(robust_item_fit)
```

Om een beter beeld te krijgen, volgen hieronder the item parameters van de vragen. 
In IRT, meet een factor loading (ook bekend als een discriminatieparameter) hoe goed een item onderscheid maakt tussen personen met verschillende niveaus van de onderliggende eigenschap. Een hoge factor loading betekent dat het item gevoelig is voor verschillen in de onderliggende eigenschap.
Het belangrijkste verschil met factor analyse, is dat in IRT, de factor loadings ook rekening houden met de moeilijkheidsgraad van de items. Dit betekent dat items die dezelfde moeilijkheidsgraad hebben (dat wil zeggen, ze zijn even moeilijk om te beantwoorden) worden geacht dezelfde hoeveelheid informatie te geven over de onderliggende eigenschap, ongeacht hun factor loadings. Dit is niet het geval in factoranalyse, waar de factor loadings alleen de relatie tussen de geobserveerde variabelen en de onderliggende factor meten.
Tevens kan het zijn dat de factor loadings en de item parameters niet overeenkomen. Bijvoorbeeld een item kan een hoge factor loading hebben (wat aangeeft dat het goed discrimineert) maar toch een slechte fit hebben (X2-statistiek) als het bijvoorbeeld verkeerd wordt begrepen door de respondenten. Omgekeerd kan een item goed passen bij het model maar niet goed discrimineren tussen verschillende niveaus van de eigenschap.
In een Graded response model, zoals hier gebruikt, zijn er meerdere moeilijkheids parameters, een voor elke vrijheidsgraaad van de vraag (totaal antwoordmogelijk - 1). 

```{r leerling stabiel - item parameters, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(params)
params
```

##### zorgvuldig

```{r leerling zorgvuldig - stat extraction, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
# Model fit

fit <- irt_fit$gedrag_houding_zorgvuldig_irt_fit
model_fit <- as.data.frame(M2(fit, na.rm = TRUE))
model_fit <- cbind(model_fit, nrow(data))
model_fit <- rename(model_fit, `N` = `nrow(data)`)
model_fit <- tidyr::pivot_longer(model_fit, cols = -N, names_to = "Fit Index", values_to = "Value")
model_fit <- dplyr::select(model_fit, `Fit Index`, Value)
model_fit$`Cut-Off` <- c("", "", "> .05", "< .05 | .05 - .1 | > .1", "", "", "< .08 | .08 - .10 | > .10", "> .95 | .90 - .95 | < .90", "> .95 | .90 - .95 | < .90")

# item fit
item_fit <- itemfit(fit, na.rm = TRUE, fit_stats = "PV_Q1")
colnames(item_fit) <- c("Item", "X2", "df", "RMSEA", "p")
item_fit <- item_fit[order(item_fit$X2), ]
rownames(item_fit) <- NULL

robust_item_fit <- itemfit(fit, na.rm = TRUE)
colnames(robust_item_fit) <- c("Item", "PV_Q1", "df", "RMSEA", "p")
robust_item_fit <- robust_item_fit[order(robust_item_fit$PV_Q1), ]
rownames(robust_item_fit) <- NULL

# Item parameters
params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
params <- params[, -which(colnames(params) %in% c("items.g", "items.u", "means", "F1"))]
sink("/dev/null")
sum <- suppressWarnings(suppressMessages(summary(fit, silent = TRUE)))
sink()
params <- cbind(params, sum$rotF)
params <- cbind(params, sum$h2)
params <- params[, c("F1", colnames(params)[-which(colnames(params) %in% "F1")])]
colnames(params) <- c("Factor Loading", "Discrimination Parameter", "Difficulty Parameter 1", "Difficulty Parameter 2", "Difficulty Parameter 3", "Difficulty Parameter 4", "Communalities")
params <- params[order(params$`Factor Loading`, decreasing = TRUE), ]


# # Differential Item Functionining DIF
# # Prepare the data
# gender <- data$gender
# gender <- dplyr::recode(gender, "Man" = 1, "Vrouw" = 2, "Onbekend" = 2)
# gender <- as.factor(gender)
# local_df <- data[, !grepl("package_duration_raw|student_number|student_name|birth_date|gender", colnames(data))]

# # Fit the multiple group model
# dif_fit <- mirt::multipleGroup(
#   data = local_df,
#   model = 1,
#   group = gender
# )

# # Perform DIF analysis
# dif <- mirt::DIF(dif_fit, group = gender, which.par = c("a1", "d"), na.rm = TRUE, plotdif = TRUE)

# person fit
# person_fit <- personfit(fit) %>%
#   reframe(
#     infit.outside = prop.table(table(z.infit > 1.96 | z.infit < -1.96)),
#     outfit.outside = prop.table(table(z.outfit > 1.96 | z.outfit < -1.96))
#   ) # lower row = non-fitting people
# person_fit$Correct <- c("Correct", "Incorrect")
# person_fit <- person_fit %>% dplyr::select(Correct, everything())
```

###### Model Fit

Hieronder bevinden zich de verschillende model fit indices.Hierbij een korte uitleg wat het exact betekent:

- M2: Dit is de M2-statistiek van Maydeu-Olivares, die een maat is voor de absolute fit van het model. Een lager getal duidt op een betere fit. Er is geen vaste cut-off waarde, maar over het algemeen wordt een lagere waarde als beter beschouwd. Als de bijbehorende p-waarde kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- df: de degrees of freedom van het model. Dit is het aantal parameters dat is geschat, minus het aantal constraints dat is opgelegd.
- p-value: de p-waarde van de Goodness of fit test. Als deze kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- RMSEA: RMSEA meet hoe goed een model de waargenomen data benadert, rekening houdend met de fout in het model. Lagere waarden duiden op een betere pasvorm, typisch onder 0.05 voor goede pasvorm en tussen 0.05 en 0.08 voor redelijke pasvorm.
  - RMSEA_5 en RMSEA_95: de 95% betrouwbaarheidsinterval van de RMSEA. Hoe kleiner dit interval rondom de RMSEA waarde, hoe zekerder we zijn van die waarde. 
- SRMSR: SRMSR staat voor "Standardized Root Mean Square Residual". Het is een maat voor de gemiddelde discrepantie tussen de waargenomen en voorspelde covarianties. Een lagere waarde duidt op een betere fit. Een SRMSR van minder dan 0.08 wordt over het algemeen als goed beschouwd.
- TLI: de Tucker-Lewis Index. TLI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.
- CFI: de Comparative Fit Index. De CFI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.
- N: het aantal respondenten in de steekproef.

```{r leerling zorgvuldig - model fit, echo=FALSE, message=FALSE, warning=FALSE}
model_fit$`Conclusion` <- c("", "", "Poor", "Acceptable", "", "", "Good", "Poor", "Poor")
knitr::kable(model_fit)
```

###### Item Fit

Hieronder bevinden zich de verschillende item fit indices. Hierbij een korte uitleg wat het exact betekent:

- X2: Dit is de X2-statistiek van de item fit test. Een lager getal duidt op een betere fit. Er is geen vaste cut-off waarde, maar over het algemeen wordt een lagere waarde als beter beschouwd. Als de bijbehorende p-waarde kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het item niet goed past.
- df: de degrees of freedom van het model. Dit is het aantal parameters dat is geschat, minus het aantal constraints dat is opgelegd.
- p-value: de p-waarde van de Goodness of fit test. Als deze kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- RMSEA: RMSEA meet hoe goed een model de waargenomen data benadert, rekening houdend met de fout in het model. Lagere waarden duiden op een betere pasvorm, typisch onder 0.05 voor goede pasvorm en tussen 0.05 en 0.08 voor redelijke pasvorm.
 
```{r leerling zorgvuldig - item fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(item_fit)
```


Als bepaalde waarden niet goed passen in dit model, kan dit verschillende dingen betekenen.
Ten eerste kan het zijn dat de items niet allemaal dezelfde onderliggende eigenschap meten. Ten tweede kan het zijn dat sommige items verkeerd worden begrepen door de respondenten.
Ten derde kan het zijn dat de veronderstelling van lokale onafhankelijkheid wordt geschonden. Dit betekent dat de reacties op de items afhankelijk zijn van elkaar, zelfs na controle voor de onderliggende eigenschap. Bijvoorbeeld, als het beantwoorden van het ene item helpt bij het beantwoorden van een ander item, dan zullen deze items niet goed passen in het model.

Hieronder volgt een robuste variant van de X2 test, namelijk de PV_Q1 test. Hieronder volgt een korte uitleg wat het exact betekent:

De 'PV_Q1'-statistiek is een afgeleide versie van de Q1-statistiek, een chi-kwadraatstatistiek gebruikt in item respons theorie (IRT) modellen om te testen of een item goed past bij het model. 
Een hoge 'PV_Q1'-waarde duidt op een slechte overeenkomst tussen de waargenomen gegevens en voorspellingen van het model, wat suggereert dat het item mogelijk niet conform het verwachte gedrag 
volgens het IRT-model functioneert. Een niet-significante p-waarde, meestal p > 0.05, in combinatie met lage 'PV_Q1'- en RMSEA.PV_Q1-waarden, geeft aan dat het item goed past bij het IRT-model.
Daarnaast is het belangrijk op te merken dat de 'PV_Q1'-statistiek robuust is tegen invloeden van steekproefomvang en lokale afhankelijkheid, waardoor het een betrouwbaar instrument is om de pasvorm
van een item met het IRT-model te beoordelen, ongeacht de omvang van de steekproef of eventuele lokale onderlinge afhankelijkheden tussen de waarnemingen. Hieronder de resultaten:

```{r leerling zorgvuldig - robust item fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(robust_item_fit)
```

Om een beter beeld te krijgen, volgen hieronder the item parameters van de vragen. 
In IRT, meet een factor loading (ook bekend als een discriminatieparameter) hoe goed een item onderscheid maakt tussen personen met verschillende niveaus van de onderliggende eigenschap. Een hoge factor loading betekent dat het item gevoelig is voor verschillen in de onderliggende eigenschap.
Het belangrijkste verschil met factor analyse, is dat in IRT, de factor loadings ook rekening houden met de moeilijkheidsgraad van de items. Dit betekent dat items die dezelfde moeilijkheidsgraad hebben (dat wil zeggen, ze zijn even moeilijk om te beantwoorden) worden geacht dezelfde hoeveelheid informatie te geven over de onderliggende eigenschap, ongeacht hun factor loadings. Dit is niet het geval in factoranalyse, waar de factor loadings alleen de relatie tussen de geobserveerde variabelen en de onderliggende factor meten.
Tevens kan het zijn dat de factor loadings en de item parameters niet overeenkomen. Bijvoorbeeld een item kan een hoge factor loading hebben (wat aangeeft dat het goed discrimineert) maar toch een slechte fit hebben (X2-statistiek) als het bijvoorbeeld verkeerd wordt begrepen door de respondenten. Omgekeerd kan een item goed passen bij het model maar niet goed discrimineren tussen verschillende niveaus van de eigenschap.
In een Graded response model, zoals hier gebruikt, zijn er meerdere moeilijkheids parameters, een voor elke vrijheidsgraaad van de vraag (totaal antwoordmogelijk - 1). 

```{r leerling zorgvuldig - item parameters, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(params)
params
```


##### open

```{r leerling open - stat extraction, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
# Model fit

fit <- irt_fit$gedrag_houding_open_irt_fit
model_fit <- as.data.frame(M2(fit, na.rm = TRUE))
model_fit <- cbind(model_fit, nrow(data))
model_fit <- rename(model_fit, `N` = `nrow(data)`)
model_fit <- tidyr::pivot_longer(model_fit, cols = -N, names_to = "Fit Index", values_to = "Value")
model_fit <- dplyr::select(model_fit, `Fit Index`, Value)
model_fit$`Cut-Off` <- c("", "", "> .05", "< .05 | .05 - .1 | > .1", "", "", "< .08 | .08 - .10 | > .10", "> .95 | .90 - .95 | < .90", "> .95 | .90 - .95 | < .90")

# item fit
item_fit <- itemfit(fit, na.rm = TRUE, fit_stats = "PV_Q1")
colnames(item_fit) <- c("Item", "X2", "df", "RMSEA", "p")
item_fit <- item_fit[order(item_fit$X2), ]
rownames(item_fit) <- NULL

robust_item_fit <- itemfit(fit, na.rm = TRUE)
colnames(robust_item_fit) <- c("Item", "PV_Q1", "df", "RMSEA", "p")
robust_item_fit <- robust_item_fit[order(robust_item_fit$PV_Q1), ]
rownames(robust_item_fit) <- NULL

# Item parameters
params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
params <- params[, -which(colnames(params) %in% c("items.g", "items.u", "means", "F1"))]
sink("/dev/null")
sum <- suppressWarnings(suppressMessages(summary(fit, silent = TRUE)))
sink()
params <- cbind(params, sum$rotF)
params <- cbind(params, sum$h2)
params <- params[, c("F1", colnames(params)[-which(colnames(params) %in% "F1")])]
colnames(params) <- c("Factor Loading", "Discrimination Parameter", "Difficulty Parameter 1", "Difficulty Parameter 2", "Difficulty Parameter 3", "Difficulty Parameter 4", "Communalities")
params <- params[order(params$`Factor Loading`, decreasing = TRUE), ]


# # Differential Item Functionining DIF
# # Prepare the data
# gender <- data$gender
# gender <- dplyr::recode(gender, "Man" = 1, "Vrouw" = 2, "Onbekend" = 2)
# gender <- as.factor(gender)
# local_df <- data[, !grepl("package_duration_raw|student_number|student_name|birth_date|gender", colnames(data))]

# # Fit the multiple group model
# dif_fit <- mirt::multipleGroup(
#   data = local_df,
#   model = 1,
#   group = gender
# )

# # Perform DIF analysis
# dif <- mirt::DIF(dif_fit, group = gender, which.par = c("a1", "d"), na.rm = TRUE, plotdif = TRUE)

# person fit
# person_fit <- personfit(fit) %>%
#   reframe(
#     infit.outside = prop.table(table(z.infit > 1.96 | z.infit < -1.96)),
#     outfit.outside = prop.table(table(z.outfit > 1.96 | z.outfit < -1.96))
#   ) # lower row = non-fitting people
# person_fit$Correct <- c("Correct", "Incorrect")
# person_fit <- person_fit %>% dplyr::select(Correct, everything())
```

###### Model Fit

Hieronder bevinden zich de verschillende model fit indices.Hierbij een korte uitleg wat het exact betekent:

- M2: Dit is de M2-statistiek van Maydeu-Olivares, die een maat is voor de absolute fit van het model. Een lager getal duidt op een betere fit. Er is geen vaste cut-off waarde, maar over het algemeen wordt een lagere waarde als beter beschouwd. Als de bijbehorende p-waarde kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- df: de degrees of freedom van het model. Dit is het aantal parameters dat is geschat, minus het aantal constraints dat is opgelegd.
- p-value: de p-waarde van de Goodness of fit test. Als deze kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- RMSEA: RMSEA meet hoe goed een model de waargenomen data benadert, rekening houdend met de fout in het model. Lagere waarden duiden op een betere pasvorm, typisch onder 0.05 voor goede pasvorm en tussen 0.05 en 0.08 voor redelijke pasvorm.
  - RMSEA_5 en RMSEA_95: de 95% betrouwbaarheidsinterval van de RMSEA. Hoe kleiner dit interval rondom de RMSEA waarde, hoe zekerder we zijn van die waarde. 
- SRMSR: SRMSR staat voor "Standardized Root Mean Square Residual". Het is een maat voor de gemiddelde discrepantie tussen de waargenomen en voorspelde covarianties. Een lagere waarde duidt op een betere fit. Een SRMSR van minder dan 0.08 wordt over het algemeen als goed beschouwd.
- TLI: de Tucker-Lewis Index. TLI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.
- CFI: de Comparative Fit Index. De CFI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.
- N: het aantal respondenten in de steekproef.

```{r leerling open - model fit, echo=FALSE, message=FALSE, warning=FALSE}
model_fit$`Conclusion` <- c("", "", "Poor", "Acceptable", "", "", "Good", "Poor", "Poor")
knitr::kable(model_fit)
```

###### Item Fit

Hieronder bevinden zich de verschillende item fit indices. Hierbij een korte uitleg wat het exact betekent:

- X2: Dit is de X2-statistiek van de item fit test. Een lager getal duidt op een betere fit. Er is geen vaste cut-off waarde, maar over het algemeen wordt een lagere waarde als beter beschouwd. Als de bijbehorende p-waarde kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het item niet goed past.
- df: de degrees of freedom van het model. Dit is het aantal parameters dat is geschat, minus het aantal constraints dat is opgelegd.
- p-value: de p-waarde van de Goodness of fit test. Als deze kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- RMSEA: RMSEA meet hoe goed een model de waargenomen data benadert, rekening houdend met de fout in het model. Lagere waarden duiden op een betere pasvorm, typisch onder 0.05 voor goede pasvorm en tussen 0.05 en 0.08 voor redelijke pasvorm.
 
```{r leerling open - item fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(item_fit)
```


Als bepaalde waarden niet goed passen in dit model, kan dit verschillende dingen betekenen.
Ten eerste kan het zijn dat de items niet allemaal dezelfde onderliggende eigenschap meten. Ten tweede kan het zijn dat sommige items verkeerd worden begrepen door de respondenten.
Ten derde kan het zijn dat de veronderstelling van lokale onafhankelijkheid wordt geschonden. Dit betekent dat de reacties op de items afhankelijk zijn van elkaar, zelfs na controle voor de onderliggende eigenschap. Bijvoorbeeld, als het beantwoorden van het ene item helpt bij het beantwoorden van een ander item, dan zullen deze items niet goed passen in het model.

Hieronder volgt een robuste variant van de X2 test, namelijk de PV_Q1 test. Hieronder volgt een korte uitleg wat het exact betekent:

De 'PV_Q1'-statistiek is een afgeleide versie van de Q1-statistiek, een chi-kwadraatstatistiek gebruikt in item respons theorie (IRT) modellen om te testen of een item goed past bij het model. 
Een hoge 'PV_Q1'-waarde duidt op een slechte overeenkomst tussen de waargenomen gegevens en voorspellingen van het model, wat suggereert dat het item mogelijk niet conform het verwachte gedrag 
volgens het IRT-model functioneert. Een niet-significante p-waarde, meestal p > 0.05, in combinatie met lage 'PV_Q1'- en RMSEA.PV_Q1-waarden, geeft aan dat het item goed past bij het IRT-model.
Daarnaast is het belangrijk op te merken dat de 'PV_Q1'-statistiek robuust is tegen invloeden van steekproefomvang en lokale afhankelijkheid, waardoor het een betrouwbaar instrument is om de pasvorm
van een item met het IRT-model te beoordelen, ongeacht de omvang van de steekproef of eventuele lokale onderlinge afhankelijkheden tussen de waarnemingen. Hieronder de resultaten:

```{r leerling open - robust item fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(robust_item_fit)
```

Om een beter beeld te krijgen, volgen hieronder the item parameters van de vragen. 
In IRT, meet een factor loading (ook bekend als een discriminatieparameter) hoe goed een item onderscheid maakt tussen personen met verschillende niveaus van de onderliggende eigenschap. Een hoge factor loading betekent dat het item gevoelig is voor verschillen in de onderliggende eigenschap.
Het belangrijkste verschil met factor analyse, is dat in IRT, de factor loadings ook rekening houden met de moeilijkheidsgraad van de items. Dit betekent dat items die dezelfde moeilijkheidsgraad hebben (dat wil zeggen, ze zijn even moeilijk om te beantwoorden) worden geacht dezelfde hoeveelheid informatie te geven over de onderliggende eigenschap, ongeacht hun factor loadings. Dit is niet het geval in factoranalyse, waar de factor loadings alleen de relatie tussen de geobserveerde variabelen en de onderliggende factor meten.
Tevens kan het zijn dat de factor loadings en de item parameters niet overeenkomen. Bijvoorbeeld een item kan een hoge factor loading hebben (wat aangeeft dat het goed discrimineert) maar toch een slechte fit hebben (X2-statistiek) als het bijvoorbeeld verkeerd wordt begrepen door de respondenten. Omgekeerd kan een item goed passen bij het model maar niet goed discrimineren tussen verschillende niveaus van de eigenschap.
In een Graded response model, zoals hier gebruikt, zijn er meerdere moeilijkheids parameters, een voor elke vrijheidsgraaad van de vraag (totaal antwoordmogelijk - 1). 

```{r leerling open - item parameters, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(params)
params
```


### Gedrag_houding (leerkracht)

#### Factor Analyse

```{r gedrag_houding_leerkracht$gedrag_houding_leerkracht - stat extraction, echo=FALSE, message=FALSE, warning=FALSE}
fit <- fitMeasures(gedrag_houding_leerkracht$gedrag_houding_leerkracht)

# null hypothesis: empirical chi-square


p <- round(fit["pvalue"][[1]], 3)
chisq <- fit["chisq"][[1]]

# model fit indices
rmsea <- fit["rmsea"][[1]] # < .06 or .08
p_rmsea <- fit["rmsea.pvalue"][[1]] # > .05
tli <- fit["tli"][[1]] # > 0.95
rms <- fit["crmr"][[1]] # correct RMS, < 0.08
cfi <- fit["cfi"][[1]] # > 0.90

# N
lavInspect(gedrag_houding_leerkracht$gedrag_houding_leerkracht, what = "nobs")

# Explained variance
r2 <- inspect(gedrag_houding_leerkracht$gedrag_houding_leerkracht, what = "r2")

# Factor loadings
loadings <- parameterEstimates(gedrag_houding_leerkracht$gedrag_houding_leerkracht)[parameterEstimates(gedrag_houding_leerkracht$gedrag_houding_leerkracht)$op == "=~", ][c("rhs", "est")]
weights <- inspect(gedrag_houding_leerkracht$gedrag_houding_leerkracht, what = "std")[["lambda"]] # These are the factor score coefficients, which are used to compute factor scores for each observation in your data.
```

##### Model Beoordeling

Hieronder volgen alle model fit van de factor analyse voor gedrag_houding_leerkracht. Hierbij moet rekening gehouden worden dat de factor analyses expliciet gemaakt zijn met 10 factoren,
en de onderliggende bijbehorende vragen alleen laden op de voor ingestelde factor. 

> @FLEUR: dit kan ik ook veranderen. m.a.w. dat ze wel op meerdere factoren laden. Dit geldt ook voor alle andere factor analyses. Maar ik het het nu zo gedaan
> om een wat globaler beeld te krijgen. Vaak zijn vragen die op deze manier slecht laden beter behorend bij andere factoren. Dit zou ik dan op jouw aanraden nog kunnen onderzoeken.
  
Hieronder, in tabel 2, zie je de model-fit statistieken van de CFA voor betekenissen. Hierbij een korte uitleg wat het exact betekent:

- p-value: de p-waarde van de empirische chi-square. Als deze kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- RMSEA: RMSEA meet hoe goed een model de waargenomen data benadert, rekening houdend met de fout in het model. Lagere waarden duiden op een betere pasvorm, typisch onder 0.05 voor goede pasvorm en tussen 0.05 en 0.08 voor redelijke pasvorm.
- TLI: de Tucker-Lewis Index. TLI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.
- CRMS: de Corrected Root Mean Square. Dit kwantificeert het gemiddelde verschil tussen waargenomen en voorspelde waarden in een model. Het geeft een maat voor hoe goed de voorspellingen van het model overeenkomen met de daadwerkelijke gegevens. Lagere RMRS-waarden geven een betere overeenkomst aan, wat wijst op een nauwkeuriger model.
- CFI: de Comparative Fit Index. De CFI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.


**Tabel 2: Betekenissen - Model-fit statistieken**

```{r gedrag_houding_leerkracht - model fit table, echo=FALSE, message=FALSE, warning=FALSE}
# Create dataframe
df <- data.frame(
  Statistic_Name = c("p-value", "RMSEA", "TLI", "CRMS", "CFI"),
  Result = c(p, rmsea, tli, rms, cfi),
  Cut_off = c("> .05", "< .05 | .05 - .1 | > .1", "> .95 | .90 - .95 | < .90", "< .08 | .08 - .10 | > .10", "> .95 | .90 - .95 | < .90"),
  Conclusion = c("Poor", "Good", "Poor", "Good", "Poor")
)

# Create table
knitr::kable(df, col.names = c("Statistic", "Result", "Cut-off", "Conclusion"))
```


De model-fit resultaten zijn niet heel positief.
Gezien de gevoeligheid van p-waardes in empirische chi-square testen, en de hoge hoeveelheid observaties, is het verstandiger
de andere fit indexen te hanteren.

Als conclusie hier is dat het model goed past, en (dus) een factor voldoende is.


##### Item Beoordeling

Hieronder staan de factor loadings, factor weights, communalities, en uniquenesses weergegeven. Hieronder een beknopte uitleg van wat deze waarden betekenen:

- Factor loadings: de factor loadings geven aan hoe sterk een item correleert met een factor. Hoe hoger de factor loading, hoe sterker de correlatie. Een factor loading van .3 of hoger wordt gezien als een goede factor loading.
- Factor weights: de factor weights geven aan hoeveel gewicht een item heeft op een factor. Deze wordt gebruikt om de factor scores te berekenen. Hoe hoger de factor weight, hoe meer gewicht een item heeft op een factor.


**Tabel 3: Betekenissen - Model-fit statistieken**

```{r gedrag_houding_leerkracht - Item loadings, echo=FALSE, message=FALSE, warning=FALSE}
# Create dataframe

# Create table
loadings <- loadings[order(loadings$est, decreasing = TRUE), ]
knitr::kable(loadings, col.names = c("Naam", "std. Factor Loading"))
```

Op basis van de factor loadings en de communality/uniqueness waardes, kunnen twee reccomendaties gemaakt worden.
Een factor loading van een vraag wordt als "goed" gezien zodra deze hoger ligt dan .3. Dit is bij meerdere vragen
niet het geval. Specifiek gaat dit om de volgende vragen (van hoogste naar laagste):

- ASL_pers_lln_083_open          : 0.2751208
- ASL_pers_lln_058_zelfvertrouwen: 0.2321667

Hier volgen nog de gewichten van alle vragen op diens eigen factoren:

**Tabel 4: Betekenissen - Model-fit statistieken**

```{r gedrag_houding_leerkracht - Weights, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(weights)
```


Zie figuur 3 voor een grafische weergave:

**Figuur 3: Factor Loadings Betekenissen**

```{r gedrag_houding_leerkracht - factor loadings plot, echo=FALSE, message=FALSE, warning=FALSE}
plot <- plot(loadings$est)
plot <- lines(loadings$est)
plot <- abline(h = 0.3, lty = 2) # lty = 2 makes the line dashed
plot <- title(main = "Factor Loadings Betekenissen", sub = "Item factor loadings; order based on item factor loadings order")
```




#### Item Response Analyse
```{r leerkracht gedrag_houding  - load data, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
# load data
irt_fit <- readRDS(paste0(here(), "/data/item_analyses_pers2024/irt_fit_gedrag_houding_leerkracht.rds"))
data <- read.csv(paste0(here(), "/data/data_interrim/gedrag_houding.csv"))
```

##### Betrokkenheid thuis

```{r leerkracht betrokkenheid_thuis - stat extraction, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
# Model fit
fit <- irt_fit$gedrag_houding_betrokkenheid_thuis_irt_fit
model_fit <- as.data.frame(M2(fit, na.rm = TRUE))
model_fit <- cbind(model_fit, nrow(data))
model_fit <- rename(model_fit, `N` = `nrow(data)`)
model_fit <- tidyr::pivot_longer(model_fit, cols = -N, names_to = "Fit Index", values_to = "Value")
model_fit <- dplyr::select(model_fit, `Fit Index`, Value)
model_fit$`Cut-Off` <- c("", "", "> .05", "< .05 | .05 - .1 | > .1", "", "", "< .08 | .08 - .10 | > .10", "> .95 | .90 - .95 | < .90", "> .95 | .90 - .95 | < .90")

# item fit
item_fit <- itemfit(fit, na.rm = TRUE, fit_stats = "PV_Q1")
colnames(item_fit) <- c("Item", "X2", "df", "RMSEA", "p")
item_fit <- item_fit[order(item_fit$X2), ]
rownames(item_fit) <- NULL

robust_item_fit <- itemfit(fit, na.rm = TRUE)
colnames(robust_item_fit) <- c("Item", "PV_Q1", "df", "RMSEA", "p")
robust_item_fit <- robust_item_fit[order(robust_item_fit$PV_Q1), ]
rownames(robust_item_fit) <- NULL

# Item parameters
params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
params <- params[, -which(colnames(params) %in% c("items.g", "items.u", "means", "F1"))]
sink("/dev/null")
sum <- suppressWarnings(suppressMessages(summary(fit, silent = TRUE)))
sink()
params <- cbind(params, sum$rotF)
params <- cbind(params, sum$h2)
params <- params[, c("F1", colnames(params)[-which(colnames(params) %in% "F1")])]
colnames(params) <- c("Factor Loading", "Discrimination Parameter", "Difficulty Parameter 1", "Difficulty Parameter 2", "Difficulty Parameter 3", "Difficulty Parameter 4", "Communalities")
params <- params[order(params$`Factor Loading`, decreasing = TRUE), ]


# # Differential Item Functionining DIF
# # Prepare the data
# gender <- data$gender
# gender <- dplyr::recode(gender, "Man" = 1, "Vrouw" = 2, "Onbekend" = 2)
# gender <- as.factor(gender)
# local_df <- data[, !grepl("package_duration_raw|student_number|student_name|birth_date|gender", colnames(data))]

# # Fit the multiple group model
# dif_fit <- mirt::multipleGroup(
#   data = local_df,
#   model = 1,
#   group = gender
# )

# # Perform DIF analysis
# dif <- mirt::DIF(dif_fit, group = gender, which.par = c("a1", "d"), na.rm = TRUE, plotdif = TRUE)

# person fit
# person_fit <- personfit(fit) %>%
#   reframe(
#     infit.outside = prop.table(table(z.infit > 1.96 | z.infit < -1.96)),
#     outfit.outside = prop.table(table(z.outfit > 1.96 | z.outfit < -1.96))
#   ) # lower row = non-fitting people
# person_fit$Correct <- c("Correct", "Incorrect")
# person_fit <- person_fit %>% dplyr::select(Correct, everything())
```

###### Model Fit

Hieronder bevinden zich de verschillende model fit indices.Hierbij een korte uitleg wat het exact betekent:

- M2: Dit is de M2-statistiek van Maydeu-Olivares, die een maat is voor de absolute fit van het model. Een lager getal duidt op een betere fit. Er is geen vaste cut-off waarde, maar over het algemeen wordt een lagere waarde als beter beschouwd. Als de bijbehorende p-waarde kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- df: de degrees of freedom van het model. Dit is het aantal parameters dat is geschat, minus het aantal constraints dat is opgelegd.
- p-value: de p-waarde van de Goodness of fit test. Als deze kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- RMSEA: RMSEA meet hoe goed een model de waargenomen data benadert, rekening houdend met de fout in het model. Lagere waarden duiden op een betere pasvorm, typisch onder 0.05 voor goede pasvorm en tussen 0.05 en 0.08 voor redelijke pasvorm.
  - RMSEA_5 en RMSEA_95: de 95% betrouwbaarheidsinterval van de RMSEA. Hoe kleiner dit interval rondom de RMSEA waarde, hoe zekerder we zijn van die waarde. 
- SRMSR: SRMSR staat voor "Standardized Root Mean Square Residual". Het is een maat voor de gemiddelde discrepantie tussen de waargenomen en voorspelde covarianties. Een lagere waarde duidt op een betere fit. Een SRMSR van minder dan 0.08 wordt over het algemeen als goed beschouwd.
- TLI: de Tucker-Lewis Index. TLI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.
- CFI: de Comparative Fit Index. De CFI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.
- N: het aantal respondenten in de steekproef.

```{r leerkracht betrokkenheid thuis - model fit, echo=FALSE, message=FALSE, warning=FALSE}
model_fit$`Conclusion` <- c("", "", "Poor", "Acceptable", "", "", "Good", "Poor", "Poor")
knitr::kable(model_fit)
```

###### Item Fit

Hieronder bevinden zich de verschillende item fit indices. Hierbij een korte uitleg wat het exact betekent:

- X2: Dit is de X2-statistiek van de item fit test. Een lager getal duidt op een betere fit. Er is geen vaste cut-off waarde, maar over het algemeen wordt een lagere waarde als beter beschouwd. Als de bijbehorende p-waarde kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het item niet goed past.
- df: de degrees of freedom van het model. Dit is het aantal parameters dat is geschat, minus het aantal constraints dat is opgelegd.
- p-value: de p-waarde van de Goodness of fit test. Als deze kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- RMSEA: RMSEA meet hoe goed een model de waargenomen data benadert, rekening houdend met de fout in het model. Lagere waarden duiden op een betere pasvorm, typisch onder 0.05 voor goede pasvorm en tussen 0.05 en 0.08 voor redelijke pasvorm.
 
```{r leerkracht betrokkenheid thuis - item fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(item_fit)
```


Als bepaalde waarden niet goed passen in dit model, kan dit verschillende dingen betekenen.
Ten eerste kan het zijn dat de items niet allemaal dezelfde onderliggende eigenschap meten. Ten tweede kan het zijn dat sommige items verkeerd worden begrepen door de respondenten.
Ten derde kan het zijn dat de veronderstelling van lokale onafhankelijkheid wordt geschonden. Dit betekent dat de reacties op de items afhankelijk zijn van elkaar, zelfs na controle voor de onderliggende eigenschap. Bijvoorbeeld, als het beantwoorden van het ene item helpt bij het beantwoorden van een ander item, dan zullen deze items niet goed passen in het model.

Hieronder volgt een robuste variant van de X2 test, namelijk de PV_Q1 test. Hieronder volgt een korte uitleg wat het exact betekent:

De 'PV_Q1'-statistiek is een afgeleide versie van de Q1-statistiek, een chi-kwadraatstatistiek gebruikt in item respons theorie (IRT) modellen om te testen of een item goed past bij het model. 
Een hoge 'PV_Q1'-waarde duidt op een slechte overeenkomst tussen de waargenomen gegevens en voorspellingen van het model, wat suggereert dat het item mogelijk niet conform het verwachte gedrag 
volgens het IRT-model functioneert. Een niet-significante p-waarde, meestal p > 0.05, in combinatie met lage 'PV_Q1'- en RMSEA.PV_Q1-waarden, geeft aan dat het item goed past bij het IRT-model.
Daarnaast is het belangrijk op te merken dat de 'PV_Q1'-statistiek robuust is tegen invloeden van steekproefomvang en lokale afhankelijkheid, waardoor het een betrouwbaar instrument is om de pasvorm
van een item met het IRT-model te beoordelen, ongeacht de omvang van de steekproef of eventuele lokale onderlinge afhankelijkheden tussen de waarnemingen. Hieronder de resultaten:

```{r leerkracht betrokkenheid thuis - robust item fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(robust_item_fit)
```

Om een beter beeld te krijgen, volgen hieronder the item parameters van de vragen. 
In IRT, meet een factor loading (ook bekend als een discriminatieparameter) hoe goed een item onderscheid maakt tussen personen met verschillende niveaus van de onderliggende eigenschap. Een hoge factor loading betekent dat het item gevoelig is voor verschillen in de onderliggende eigenschap.
Het belangrijkste verschil met factor analyse, is dat in IRT, de factor loadings ook rekening houden met de moeilijkheidsgraad van de items. Dit betekent dat items die dezelfde moeilijkheidsgraad hebben (dat wil zeggen, ze zijn even moeilijk om te beantwoorden) worden geacht dezelfde hoeveelheid informatie te geven over de onderliggende eigenschap, ongeacht hun factor loadings. Dit is niet het geval in factoranalyse, waar de factor loadings alleen de relatie tussen de geobserveerde variabelen en de onderliggende factor meten.
Tevens kan het zijn dat de factor loadings en de item parameters niet overeenkomen. Bijvoorbeeld een item kan een hoge factor loading hebben (wat aangeeft dat het goed discrimineert) maar toch een slechte fit hebben (X2-statistiek) als het bijvoorbeeld verkeerd wordt begrepen door de respondenten. Omgekeerd kan een item goed passen bij het model maar niet goed discrimineren tussen verschillende niveaus van de eigenschap.
In een Graded response model, zoals hier gebruikt, zijn er meerdere moeilijkheids parameters, een voor elke vrijheidsgraaad van de vraag (totaal antwoordmogelijk - 1). 

```{r leerkracht betrokkenheid thuis - item parameters, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(params)
params
```

##### concentratie

```{r leerkracht concentratie - stat extraction, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
# Model fit

fit <- irt_fit$gedrag_houding_concentratie_irt_fit
model_fit <- as.data.frame(M2(fit, na.rm = TRUE))
model_fit <- cbind(model_fit, nrow(data))
model_fit <- rename(model_fit, `N` = `nrow(data)`)
model_fit <- tidyr::pivot_longer(model_fit, cols = -N, names_to = "Fit Index", values_to = "Value")
model_fit <- dplyr::select(model_fit, `Fit Index`, Value)
model_fit$`Cut-Off` <- c("", "", "> .05", "< .05 | .05 - .1 | > .1", "", "", "< .08 | .08 - .10 | > .10", "> .95 | .90 - .95 | < .90", "> .95 | .90 - .95 | < .90")

# item fit
item_fit <- itemfit(fit, na.rm = TRUE, fit_stats = "PV_Q1")
colnames(item_fit) <- c("Item", "X2", "df", "RMSEA", "p")
item_fit <- item_fit[order(item_fit$X2), ]
rownames(item_fit) <- NULL

robust_item_fit <- itemfit(fit, na.rm = TRUE)
colnames(robust_item_fit) <- c("Item", "PV_Q1", "df", "RMSEA", "p")
robust_item_fit <- robust_item_fit[order(robust_item_fit$PV_Q1), ]
rownames(robust_item_fit) <- NULL

# Item parameters
params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
params <- params[, -which(colnames(params) %in% c("items.g", "items.u", "means", "F1"))]
sink("/dev/null")
sum <- suppressWarnings(suppressMessages(summary(fit, silent = TRUE)))
sink()
params <- cbind(params, sum$rotF)
params <- cbind(params, sum$h2)
params <- params[, c("F1", colnames(params)[-which(colnames(params) %in% "F1")])]
colnames(params) <- c("Factor Loading", "Discrimination Parameter", "Difficulty Parameter 1", "Difficulty Parameter 2", "Difficulty Parameter 3", "Difficulty Parameter 4", "Communalities")
params <- params[order(params$`Factor Loading`, decreasing = TRUE), ]


# # Differential Item Functionining DIF
# # Prepare the data
# gender <- data$gender
# gender <- dplyr::recode(gender, "Man" = 1, "Vrouw" = 2, "Onbekend" = 2)
# gender <- as.factor(gender)
# local_df <- data[, !grepl("package_duration_raw|student_number|student_name|birth_date|gender", colnames(data))]

# # Fit the multiple group model
# dif_fit <- mirt::multipleGroup(
#   data = local_df,
#   model = 1,
#   group = gender
# )

# # Perform DIF analysis
# dif <- mirt::DIF(dif_fit, group = gender, which.par = c("a1", "d"), na.rm = TRUE, plotdif = TRUE)

# person fit
# person_fit <- personfit(fit) %>%
#   reframe(
#     infit.outside = prop.table(table(z.infit > 1.96 | z.infit < -1.96)),
#     outfit.outside = prop.table(table(z.outfit > 1.96 | z.outfit < -1.96))
#   ) # lower row = non-fitting people
# person_fit$Correct <- c("Correct", "Incorrect")
# person_fit <- person_fit %>% dplyr::select(Correct, everything())
```

###### Model Fit

Hieronder bevinden zich de verschillende model fit indices.Hierbij een korte uitleg wat het exact betekent:

- M2: Dit is de M2-statistiek van Maydeu-Olivares, die een maat is voor de absolute fit van het model. Een lager getal duidt op een betere fit. Er is geen vaste cut-off waarde, maar over het algemeen wordt een lagere waarde als beter beschouwd. Als de bijbehorende p-waarde kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- df: de degrees of freedom van het model. Dit is het aantal parameters dat is geschat, minus het aantal constraints dat is opgelegd.
- p-value: de p-waarde van de Goodness of fit test. Als deze kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- RMSEA: RMSEA meet hoe goed een model de waargenomen data benadert, rekening houdend met de fout in het model. Lagere waarden duiden op een betere pasvorm, typisch onder 0.05 voor goede pasvorm en tussen 0.05 en 0.08 voor redelijke pasvorm.
  - RMSEA_5 en RMSEA_95: de 95% betrouwbaarheidsinterval van de RMSEA. Hoe kleiner dit interval rondom de RMSEA waarde, hoe zekerder we zijn van die waarde. 
- SRMSR: SRMSR staat voor "Standardized Root Mean Square Residual". Het is een maat voor de gemiddelde discrepantie tussen de waargenomen en voorspelde covarianties. Een lagere waarde duidt op een betere fit. Een SRMSR van minder dan 0.08 wordt over het algemeen als goed beschouwd.
- TLI: de Tucker-Lewis Index. TLI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.
- CFI: de Comparative Fit Index. De CFI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.
- N: het aantal respondenten in de steekproef.

```{r leerkracht concentratie - model fit, echo=FALSE, message=FALSE, warning=FALSE}
model_fit$`Conclusion` <- c("", "", "Poor", "Acceptable", "", "", "Good", "Poor", "Poor")
knitr::kable(model_fit)
```

###### Item Fit

Hieronder bevinden zich de verschillende item fit indices. Hierbij een korte uitleg wat het exact betekent:

- X2: Dit is de X2-statistiek van de item fit test. Een lager getal duidt op een betere fit. Er is geen vaste cut-off waarde, maar over het algemeen wordt een lagere waarde als beter beschouwd. Als de bijbehorende p-waarde kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het item niet goed past.
- df: de degrees of freedom van het model. Dit is het aantal parameters dat is geschat, minus het aantal constraints dat is opgelegd.
- p-value: de p-waarde van de Goodness of fit test. Als deze kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- RMSEA: RMSEA meet hoe goed een model de waargenomen data benadert, rekening houdend met de fout in het model. Lagere waarden duiden op een betere pasvorm, typisch onder 0.05 voor goede pasvorm en tussen 0.05 en 0.08 voor redelijke pasvorm.
 
```{r leerkracht concentratie - item fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(item_fit)
```


Als bepaalde waarden niet goed passen in dit model, kan dit verschillende dingen betekenen.
Ten eerste kan het zijn dat de items niet allemaal dezelfde onderliggende eigenschap meten. Ten tweede kan het zijn dat sommige items verkeerd worden begrepen door de respondenten.
Ten derde kan het zijn dat de veronderstelling van lokale onafhankelijkheid wordt geschonden. Dit betekent dat de reacties op de items afhankelijk zijn van elkaar, zelfs na controle voor de onderliggende eigenschap. Bijvoorbeeld, als het beantwoorden van het ene item helpt bij het beantwoorden van een ander item, dan zullen deze items niet goed passen in het model.

Hieronder volgt een robuste variant van de X2 test, namelijk de PV_Q1 test. Hieronder volgt een korte uitleg wat het exact betekent:

De 'PV_Q1'-statistiek is een afgeleide versie van de Q1-statistiek, een chi-kwadraatstatistiek gebruikt in item respons theorie (IRT) modellen om te testen of een item goed past bij het model. 
Een hoge 'PV_Q1'-waarde duidt op een slechte overeenkomst tussen de waargenomen gegevens en voorspellingen van het model, wat suggereert dat het item mogelijk niet conform het verwachte gedrag 
volgens het IRT-model functioneert. Een niet-significante p-waarde, meestal p > 0.05, in combinatie met lage 'PV_Q1'- en RMSEA.PV_Q1-waarden, geeft aan dat het item goed past bij het IRT-model.
Daarnaast is het belangrijk op te merken dat de 'PV_Q1'-statistiek robuust is tegen invloeden van steekproefomvang en lokale afhankelijkheid, waardoor het een betrouwbaar instrument is om de pasvorm
van een item met het IRT-model te beoordelen, ongeacht de omvang van de steekproef of eventuele lokale onderlinge afhankelijkheden tussen de waarnemingen. Hieronder de resultaten:

```{r leerkracht concentratie - robust item fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(robust_item_fit)
```

Om een beter beeld te krijgen, volgen hieronder the item parameters van de vragen. 
In IRT, meet een factor loading (ook bekend als een discriminatieparameter) hoe goed een item onderscheid maakt tussen personen met verschillende niveaus van de onderliggende eigenschap. Een hoge factor loading betekent dat het item gevoelig is voor verschillen in de onderliggende eigenschap.
Het belangrijkste verschil met factor analyse, is dat in IRT, de factor loadings ook rekening houden met de moeilijkheidsgraad van de items. Dit betekent dat items die dezelfde moeilijkheidsgraad hebben (dat wil zeggen, ze zijn even moeilijk om te beantwoorden) worden geacht dezelfde hoeveelheid informatie te geven over de onderliggende eigenschap, ongeacht hun factor loadings. Dit is niet het geval in factoranalyse, waar de factor loadings alleen de relatie tussen de geobserveerde variabelen en de onderliggende factor meten.
Tevens kan het zijn dat de factor loadings en de item parameters niet overeenkomen. Bijvoorbeeld een item kan een hoge factor loading hebben (wat aangeeft dat het goed discrimineert) maar toch een slechte fit hebben (X2-statistiek) als het bijvoorbeeld verkeerd wordt begrepen door de respondenten. Omgekeerd kan een item goed passen bij het model maar niet goed discrimineren tussen verschillende niveaus van de eigenschap.
In een Graded response model, zoals hier gebruikt, zijn er meerdere moeilijkheids parameters, een voor elke vrijheidsgraaad van de vraag (totaal antwoordmogelijk - 1). 

```{r leerkracht concentratie - item parameters, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(params)
params
```

##### contact met leeftijdsgenoten

```{r leerkracht contact met leeftijdsgenoten - stat extraction, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
# Model fit

fit <- irt_fit$gedrag_houding_contact_met_leeftijdsgenoten_irt_fit
# model_fit <- as.data.frame(M2(fit, na.rm = TRUE))
# model_fit <- cbind(model_fit, nrow(data))
# model_fit <- rename(model_fit, `N` = `nrow(data)`)
# model_fit <- tidyr::pivot_longer(model_fit, cols = -N, names_to = "Fit Index", values_to = "Value")
# model_fit <- dplyr::select(model_fit, `Fit Index`, Value)
# model_fit$`Cut-Off` <- c("", "", "> .05", "< .05 | .05 - .1 | > .1", "", "", "< .08 | .08 - .10 | > .10", "> .95 | .90 - .95 | < .90", "> .95 | .90 - .95 | < .90")

# item fit
item_fit <- itemfit(fit, na.rm = TRUE, fit_stats = "PV_Q1")
colnames(item_fit) <- c("Item", "X2", "df", "RMSEA", "p")
item_fit <- item_fit[order(item_fit$X2), ]
rownames(item_fit) <- NULL

robust_item_fit <- itemfit(fit, na.rm = TRUE)
colnames(robust_item_fit) <- c("Item", "PV_Q1", "df", "RMSEA", "p")
robust_item_fit <- robust_item_fit[order(robust_item_fit$PV_Q1), ]
rownames(robust_item_fit) <- NULL

# Item parameters
params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
params <- params[, -which(colnames(params) %in% c("items.g", "items.u", "means", "F1"))]
sink("/dev/null")
sum <- suppressWarnings(suppressMessages(summary(fit, silent = TRUE)))
sink()
params <- cbind(params, sum$rotF)
params <- cbind(params, sum$h2)
params <- params[, c("F1", colnames(params)[-which(colnames(params) %in% "F1")])]
colnames(params) <- c("Factor Loading", "Discrimination Parameter", "Difficulty Parameter 1", "Difficulty Parameter 2", "Difficulty Parameter 3", "Difficulty Parameter 4", "Communalities")
params <- params[order(params$`Factor Loading`, decreasing = TRUE), ]


# # Differential Item Functionining DIF
# # Prepare the data
# gender <- data$gender
# gender <- dplyr::recode(gender, "Man" = 1, "Vrouw" = 2, "Onbekend" = 2)
# gender <- as.factor(gender)
# local_df <- data[, !grepl("package_duration_raw|student_number|student_name|birth_date|gender", colnames(data))]

# # Fit the multiple group model
# dif_fit <- mirt::multipleGroup(
#   data = local_df,
#   model = 1,
#   group = gender
# )

# # Perform DIF analysis
# dif <- mirt::DIF(dif_fit, group = gender, which.par = c("a1", "d"), na.rm = TRUE, plotdif = TRUE)

# person fit
# person_fit <- personfit(fit) %>%
#   reframe(
#     infit.outside = prop.table(table(z.infit > 1.96 | z.infit < -1.96)),
#     outfit.outside = prop.table(table(z.outfit > 1.96 | z.outfit < -1.96))
#   ) # lower row = non-fitting people
# person_fit$Correct <- c("Correct", "Incorrect")
# person_fit <- person_fit %>% dplyr::select(Correct, everything())
```

###### Model Fit

Hieronder bevinden zich de verschillende model fit indices.Hierbij een korte uitleg wat het exact betekent:

- M2: Dit is de M2-statistiek van Maydeu-Olivares, die een maat is voor de absolute fit van het model. Een lager getal duidt op een betere fit. Er is geen vaste cut-off waarde, maar over het algemeen wordt een lagere waarde als beter beschouwd. Als de bijbehorende p-waarde kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- df: de degrees of freedom van het model. Dit is het aantal parameters dat is geschat, minus het aantal constraints dat is opgelegd.
- p-value: de p-waarde van de Goodness of fit test. Als deze kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- RMSEA: RMSEA meet hoe goed een model de waargenomen data benadert, rekening houdend met de fout in het model. Lagere waarden duiden op een betere pasvorm, typisch onder 0.05 voor goede pasvorm en tussen 0.05 en 0.08 voor redelijke pasvorm.
  - RMSEA_5 en RMSEA_95: de 95% betrouwbaarheidsinterval van de RMSEA. Hoe kleiner dit interval rondom de RMSEA waarde, hoe zekerder we zijn van die waarde. 
- SRMSR: SRMSR staat voor "Standardized Root Mean Square Residual". Het is een maat voor de gemiddelde discrepantie tussen de waargenomen en voorspelde covarianties. Een lagere waarde duidt op een betere fit. Een SRMSR van minder dan 0.08 wordt over het algemeen als goed beschouwd.
- TLI: de Tucker-Lewis Index. TLI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.
- CFI: de Comparative Fit Index. De CFI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.
- N: het aantal respondenten in de steekproef.

```{r leerkracht contact met leeftijdsgenoten - model fit, echo=FALSE, message=FALSE, warning=FALSE}
# model_fit$`Conclusion` <- c("", "", "Poor", "Acceptable", "", "", "Good", "Poor", "Poor")
# knitr::kable(model_fit)
```

###### Item Fit

Hieronder bevinden zich de verschillende item fit indices. Hierbij een korte uitleg wat het exact betekent:

- X2: Dit is de X2-statistiek van de item fit test. Een lager getal duidt op een betere fit. Er is geen vaste cut-off waarde, maar over het algemeen wordt een lagere waarde als beter beschouwd. Als de bijbehorende p-waarde kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het item niet goed past.
- df: de degrees of freedom van het model. Dit is het aantal parameters dat is geschat, minus het aantal constraints dat is opgelegd.
- p-value: de p-waarde van de Goodness of fit test. Als deze kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- RMSEA: RMSEA meet hoe goed een model de waargenomen data benadert, rekening houdend met de fout in het model. Lagere waarden duiden op een betere pasvorm, typisch onder 0.05 voor goede pasvorm en tussen 0.05 en 0.08 voor redelijke pasvorm.
 
```{r leerkracht contact met leeftijdsgenoten - item fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(item_fit)
```


Als bepaalde waarden niet goed passen in dit model, kan dit verschillende dingen betekenen.
Ten eerste kan het zijn dat de items niet allemaal dezelfde onderliggende eigenschap meten. Ten tweede kan het zijn dat sommige items verkeerd worden begrepen door de respondenten.
Ten derde kan het zijn dat de veronderstelling van lokale onafhankelijkheid wordt geschonden. Dit betekent dat de reacties op de items afhankelijk zijn van elkaar, zelfs na controle voor de onderliggende eigenschap. Bijvoorbeeld, als het beantwoorden van het ene item helpt bij het beantwoorden van een ander item, dan zullen deze items niet goed passen in het model.

Hieronder volgt een robuste variant van de X2 test, namelijk de PV_Q1 test. Hieronder volgt een korte uitleg wat het exact betekent:

De 'PV_Q1'-statistiek is een afgeleide versie van de Q1-statistiek, een chi-kwadraatstatistiek gebruikt in item respons theorie (IRT) modellen om te testen of een item goed past bij het model. 
Een hoge 'PV_Q1'-waarde duidt op een slechte overeenkomst tussen de waargenomen gegevens en voorspellingen van het model, wat suggereert dat het item mogelijk niet conform het verwachte gedrag 
volgens het IRT-model functioneert. Een niet-significante p-waarde, meestal p > 0.05, in combinatie met lage 'PV_Q1'- en RMSEA.PV_Q1-waarden, geeft aan dat het item goed past bij het IRT-model.
Daarnaast is het belangrijk op te merken dat de 'PV_Q1'-statistiek robuust is tegen invloeden van steekproefomvang en lokale afhankelijkheid, waardoor het een betrouwbaar instrument is om de pasvorm
van een item met het IRT-model te beoordelen, ongeacht de omvang van de steekproef of eventuele lokale onderlinge afhankelijkheden tussen de waarnemingen. Hieronder de resultaten:

```{r leerkracht contact met leeftijdsgenoten - robust item fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(robust_item_fit)
```

Om een beter beeld te krijgen, volgen hieronder the item parameters van de vragen. 
In IRT, meet een factor loading (ook bekend als een discriminatieparameter) hoe goed een item onderscheid maakt tussen personen met verschillende niveaus van de onderliggende eigenschap. Een hoge factor loading betekent dat het item gevoelig is voor verschillen in de onderliggende eigenschap.
Het belangrijkste verschil met factor analyse, is dat in IRT, de factor loadings ook rekening houden met de moeilijkheidsgraad van de items. Dit betekent dat items die dezelfde moeilijkheidsgraad hebben (dat wil zeggen, ze zijn even moeilijk om te beantwoorden) worden geacht dezelfde hoeveelheid informatie te geven over de onderliggende eigenschap, ongeacht hun factor loadings. Dit is niet het geval in factoranalyse, waar de factor loadings alleen de relatie tussen de geobserveerde variabelen en de onderliggende factor meten.
Tevens kan het zijn dat de factor loadings en de item parameters niet overeenkomen. Bijvoorbeeld een item kan een hoge factor loading hebben (wat aangeeft dat het goed discrimineert) maar toch een slechte fit hebben (X2-statistiek) als het bijvoorbeeld verkeerd wordt begrepen door de respondenten. Omgekeerd kan een item goed passen bij het model maar niet goed discrimineren tussen verschillende niveaus van de eigenschap.
In een Graded response model, zoals hier gebruikt, zijn er meerdere moeilijkheids parameters, een voor elke vrijheidsgraaad van de vraag (totaal antwoordmogelijk - 1). 

```{r leerkracht contact met leeftijdsgenoten - item parameters, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(params)
params
```


##### interesse in school

```{r leerkracht interesse in school - stat extraction, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
# Model fit

fit <- irt_fit$gedrag_houding_interesse_in_school_irt_fit
model_fit <- as.data.frame(M2(fit, na.rm = TRUE))
model_fit <- cbind(model_fit, nrow(data))
model_fit <- rename(model_fit, `N` = `nrow(data)`)
model_fit <- tidyr::pivot_longer(model_fit, cols = -N, names_to = "Fit Index", values_to = "Value")
model_fit <- dplyr::select(model_fit, `Fit Index`, Value)
model_fit$`Cut-Off` <- c("", "", "> .05", "< .05 | .05 - .1 | > .1", "", "", "< .08 | .08 - .10 | > .10", "> .95 | .90 - .95 | < .90", "> .95 | .90 - .95 | < .90")

# item fit
item_fit <- itemfit(fit, na.rm = TRUE, fit_stats = "PV_Q1")
colnames(item_fit) <- c("Item", "X2", "df", "RMSEA", "p")
item_fit <- item_fit[order(item_fit$X2), ]
rownames(item_fit) <- NULL

robust_item_fit <- itemfit(fit, na.rm = TRUE)
colnames(robust_item_fit) <- c("Item", "PV_Q1", "df", "RMSEA", "p")
robust_item_fit <- robust_item_fit[order(robust_item_fit$PV_Q1), ]
rownames(robust_item_fit) <- NULL

# Item parameters
params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
params <- params[, -which(colnames(params) %in% c("items.g", "items.u", "means", "F1"))]
sink("/dev/null")
sum <- suppressWarnings(suppressMessages(summary(fit, silent = TRUE)))
sink()
params <- cbind(params, sum$rotF)
params <- cbind(params, sum$h2)
params <- params[, c("F1", colnames(params)[-which(colnames(params) %in% "F1")])]
colnames(params) <- c("Factor Loading", "Discrimination Parameter", "Difficulty Parameter 1", "Difficulty Parameter 2", "Difficulty Parameter 3", "Difficulty Parameter 4", "Communalities")
params <- params[order(params$`Factor Loading`, decreasing = TRUE), ]


# # Differential Item Functionining DIF
# # Prepare the data
# gender <- data$gender
# gender <- dplyr::recode(gender, "Man" = 1, "Vrouw" = 2, "Onbekend" = 2)
# gender <- as.factor(gender)
# local_df <- data[, !grepl("package_duration_raw|student_number|student_name|birth_date|gender", colnames(data))]

# # Fit the multiple group model
# dif_fit <- mirt::multipleGroup(
#   data = local_df,
#   model = 1,
#   group = gender
# )

# # Perform DIF analysis
# dif <- mirt::DIF(dif_fit, group = gender, which.par = c("a1", "d"), na.rm = TRUE, plotdif = TRUE)

# person fit
# person_fit <- personfit(fit) %>%
#   reframe(
#     infit.outside = prop.table(table(z.infit > 1.96 | z.infit < -1.96)),
#     outfit.outside = prop.table(table(z.outfit > 1.96 | z.outfit < -1.96))
#   ) # lower row = non-fitting people
# person_fit$Correct <- c("Correct", "Incorrect")
# person_fit <- person_fit %>% dplyr::select(Correct, everything())
```

###### Model Fit

Hieronder bevinden zich de verschillende model fit indices.Hierbij een korte uitleg wat het exact betekent:

- M2: Dit is de M2-statistiek van Maydeu-Olivares, die een maat is voor de absolute fit van het model. Een lager getal duidt op een betere fit. Er is geen vaste cut-off waarde, maar over het algemeen wordt een lagere waarde als beter beschouwd. Als de bijbehorende p-waarde kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- df: de degrees of freedom van het model. Dit is het aantal parameters dat is geschat, minus het aantal constraints dat is opgelegd.
- p-value: de p-waarde van de Goodness of fit test. Als deze kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- RMSEA: RMSEA meet hoe goed een model de waargenomen data benadert, rekening houdend met de fout in het model. Lagere waarden duiden op een betere pasvorm, typisch onder 0.05 voor goede pasvorm en tussen 0.05 en 0.08 voor redelijke pasvorm.
  - RMSEA_5 en RMSEA_95: de 95% betrouwbaarheidsinterval van de RMSEA. Hoe kleiner dit interval rondom de RMSEA waarde, hoe zekerder we zijn van die waarde. 
- SRMSR: SRMSR staat voor "Standardized Root Mean Square Residual". Het is een maat voor de gemiddelde discrepantie tussen de waargenomen en voorspelde covarianties. Een lagere waarde duidt op een betere fit. Een SRMSR van minder dan 0.08 wordt over het algemeen als goed beschouwd.
- TLI: de Tucker-Lewis Index. TLI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.
- CFI: de Comparative Fit Index. De CFI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.
- N: het aantal respondenten in de steekproef.

```{r leerkracht interesse in school - model fit, echo=FALSE, message=FALSE, warning=FALSE}
model_fit$`Conclusion` <- c("", "", "Poor", "Acceptable", "", "", "Good", "Poor", "Poor")
knitr::kable(model_fit)
```

###### Item Fit

Hieronder bevinden zich de verschillende item fit indices. Hierbij een korte uitleg wat het exact betekent:

- X2: Dit is de X2-statistiek van de item fit test. Een lager getal duidt op een betere fit. Er is geen vaste cut-off waarde, maar over het algemeen wordt een lagere waarde als beter beschouwd. Als de bijbehorende p-waarde kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het item niet goed past.
- df: de degrees of freedom van het model. Dit is het aantal parameters dat is geschat, minus het aantal constraints dat is opgelegd.
- p-value: de p-waarde van de Goodness of fit test. Als deze kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- RMSEA: RMSEA meet hoe goed een model de waargenomen data benadert, rekening houdend met de fout in het model. Lagere waarden duiden op een betere pasvorm, typisch onder 0.05 voor goede pasvorm en tussen 0.05 en 0.08 voor redelijke pasvorm.
 
```{r leerkracht interesse in school - item fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(item_fit)
```


Als bepaalde waarden niet goed passen in dit model, kan dit verschillende dingen betekenen.
Ten eerste kan het zijn dat de items niet allemaal dezelfde onderliggende eigenschap meten. Ten tweede kan het zijn dat sommige items verkeerd worden begrepen door de respondenten.
Ten derde kan het zijn dat de veronderstelling van lokale onafhankelijkheid wordt geschonden. Dit betekent dat de reacties op de items afhankelijk zijn van elkaar, zelfs na controle voor de onderliggende eigenschap. Bijvoorbeeld, als het beantwoorden van het ene item helpt bij het beantwoorden van een ander item, dan zullen deze items niet goed passen in het model.

Hieronder volgt een robuste variant van de X2 test, namelijk de PV_Q1 test. Hieronder volgt een korte uitleg wat het exact betekent:

De 'PV_Q1'-statistiek is een afgeleide versie van de Q1-statistiek, een chi-kwadraatstatistiek gebruikt in item respons theorie (IRT) modellen om te testen of een item goed past bij het model. 
Een hoge 'PV_Q1'-waarde duidt op een slechte overeenkomst tussen de waargenomen gegevens en voorspellingen van het model, wat suggereert dat het item mogelijk niet conform het verwachte gedrag 
volgens het IRT-model functioneert. Een niet-significante p-waarde, meestal p > 0.05, in combinatie met lage 'PV_Q1'- en RMSEA.PV_Q1-waarden, geeft aan dat het item goed past bij het IRT-model.
Daarnaast is het belangrijk op te merken dat de 'PV_Q1'-statistiek robuust is tegen invloeden van steekproefomvang en lokale afhankelijkheid, waardoor het een betrouwbaar instrument is om de pasvorm
van een item met het IRT-model te beoordelen, ongeacht de omvang van de steekproef of eventuele lokale onderlinge afhankelijkheden tussen de waarnemingen. Hieronder de resultaten:

```{r leerkracht interesse in school - robust item fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(robust_item_fit)
```

Om een beter beeld te krijgen, volgen hieronder the item parameters van de vragen. 
In IRT, meet een factor loading (ook bekend als een discriminatieparameter) hoe goed een item onderscheid maakt tussen personen met verschillende niveaus van de onderliggende eigenschap. Een hoge factor loading betekent dat het item gevoelig is voor verschillen in de onderliggende eigenschap.
Het belangrijkste verschil met factor analyse, is dat in IRT, de factor loadings ook rekening houden met de moeilijkheidsgraad van de items. Dit betekent dat items die dezelfde moeilijkheidsgraad hebben (dat wil zeggen, ze zijn even moeilijk om te beantwoorden) worden geacht dezelfde hoeveelheid informatie te geven over de onderliggende eigenschap, ongeacht hun factor loadings. Dit is niet het geval in factoranalyse, waar de factor loadings alleen de relatie tussen de geobserveerde variabelen en de onderliggende factor meten.
Tevens kan het zijn dat de factor loadings en de item parameters niet overeenkomen. Bijvoorbeeld een item kan een hoge factor loading hebben (wat aangeeft dat het goed discrimineert) maar toch een slechte fit hebben (X2-statistiek) als het bijvoorbeeld verkeerd wordt begrepen door de respondenten. Omgekeerd kan een item goed passen bij het model maar niet goed discrimineren tussen verschillende niveaus van de eigenschap.
In een Graded response model, zoals hier gebruikt, zijn er meerdere moeilijkheids parameters, een voor elke vrijheidsgraaad van de vraag (totaal antwoordmogelijk - 1). 

```{r leerkracht interesse in school - item parameters, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(params)
params
```

##### motivatie om te presteren

```{r leerkracht motivatie om te presteren - stat extraction, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
# Model fit

fit <- irt_fit$gedrag_houding_motivatie_om_te_presteren_irt_fit
model_fit <- as.data.frame(M2(fit, na.rm = TRUE))
model_fit <- cbind(model_fit, nrow(data))
model_fit <- rename(model_fit, `N` = `nrow(data)`)
model_fit <- tidyr::pivot_longer(model_fit, cols = -N, names_to = "Fit Index", values_to = "Value")
model_fit <- dplyr::select(model_fit, `Fit Index`, Value)
model_fit$`Cut-Off` <- c("", "", "> .05", "< .05 | .05 - .1 | > .1", "", "", "< .08 | .08 - .10 | > .10", "> .95 | .90 - .95 | < .90", "> .95 | .90 - .95 | < .90")

# item fit
item_fit <- itemfit(fit, na.rm = TRUE, fit_stats = "PV_Q1")
colnames(item_fit) <- c("Item", "X2", "df", "RMSEA", "p")
item_fit <- item_fit[order(item_fit$X2), ]
rownames(item_fit) <- NULL

robust_item_fit <- itemfit(fit, na.rm = TRUE)
colnames(robust_item_fit) <- c("Item", "PV_Q1", "df", "RMSEA", "p")
robust_item_fit <- robust_item_fit[order(robust_item_fit$PV_Q1), ]
rownames(robust_item_fit) <- NULL

# Item parameters
params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
params <- params[, -which(colnames(params) %in% c("items.g", "items.u", "means", "F1"))]
sink("/dev/null")
sum <- suppressWarnings(suppressMessages(summary(fit, silent = TRUE)))
sink()
params <- cbind(params, sum$rotF)
params <- cbind(params, sum$h2)
params <- params[, c("F1", colnames(params)[-which(colnames(params) %in% "F1")])]
colnames(params) <- c("Factor Loading", "Discrimination Parameter", "Difficulty Parameter 1", "Difficulty Parameter 2", "Difficulty Parameter 3", "Difficulty Parameter 4", "Communalities")
params <- params[order(params$`Factor Loading`, decreasing = TRUE), ]


# # Differential Item Functionining DIF
# # Prepare the data
# gender <- data$gender
# gender <- dplyr::recode(gender, "Man" = 1, "Vrouw" = 2, "Onbekend" = 2)
# gender <- as.factor(gender)
# local_df <- data[, !grepl("package_duration_raw|student_number|student_name|birth_date|gender", colnames(data))]

# # Fit the multiple group model
# dif_fit <- mirt::multipleGroup(
#   data = local_df,
#   model = 1,
#   group = gender
# )

# # Perform DIF analysis
# dif <- mirt::DIF(dif_fit, group = gender, which.par = c("a1", "d"), na.rm = TRUE, plotdif = TRUE)

# person fit
# person_fit <- personfit(fit) %>%
#   reframe(
#     infit.outside = prop.table(table(z.infit > 1.96 | z.infit < -1.96)),
#     outfit.outside = prop.table(table(z.outfit > 1.96 | z.outfit < -1.96))
#   ) # lower row = non-fitting people
# person_fit$Correct <- c("Correct", "Incorrect")
# person_fit <- person_fit %>% dplyr::select(Correct, everything())
```

###### Model Fit

Hieronder bevinden zich de verschillende model fit indices.Hierbij een korte uitleg wat het exact betekent:

- M2: Dit is de M2-statistiek van Maydeu-Olivares, die een maat is voor de absolute fit van het model. Een lager getal duidt op een betere fit. Er is geen vaste cut-off waarde, maar over het algemeen wordt een lagere waarde als beter beschouwd. Als de bijbehorende p-waarde kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- df: de degrees of freedom van het model. Dit is het aantal parameters dat is geschat, minus het aantal constraints dat is opgelegd.
- p-value: de p-waarde van de Goodness of fit test. Als deze kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- RMSEA: RMSEA meet hoe goed een model de waargenomen data benadert, rekening houdend met de fout in het model. Lagere waarden duiden op een betere pasvorm, typisch onder 0.05 voor goede pasvorm en tussen 0.05 en 0.08 voor redelijke pasvorm.
  - RMSEA_5 en RMSEA_95: de 95% betrouwbaarheidsinterval van de RMSEA. Hoe kleiner dit interval rondom de RMSEA waarde, hoe zekerder we zijn van die waarde. 
- SRMSR: SRMSR staat voor "Standardized Root Mean Square Residual". Het is een maat voor de gemiddelde discrepantie tussen de waargenomen en voorspelde covarianties. Een lagere waarde duidt op een betere fit. Een SRMSR van minder dan 0.08 wordt over het algemeen als goed beschouwd.
- TLI: de Tucker-Lewis Index. TLI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.
- CFI: de Comparative Fit Index. De CFI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.
- N: het aantal respondenten in de steekproef.

```{r leerkracht motivatie om te presteren - model fit, echo=FALSE, message=FALSE, warning=FALSE}
model_fit$`Conclusion` <- c("", "", "Poor", "Acceptable", "", "", "Good", "Poor", "Poor")
knitr::kable(model_fit)
```

###### Item Fit

Hieronder bevinden zich de verschillende item fit indices. Hierbij een korte uitleg wat het exact betekent:

- X2: Dit is de X2-statistiek van de item fit test. Een lager getal duidt op een betere fit. Er is geen vaste cut-off waarde, maar over het algemeen wordt een lagere waarde als beter beschouwd. Als de bijbehorende p-waarde kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het item niet goed past.
- df: de degrees of freedom van het model. Dit is het aantal parameters dat is geschat, minus het aantal constraints dat is opgelegd.
- p-value: de p-waarde van de Goodness of fit test. Als deze kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- RMSEA: RMSEA meet hoe goed een model de waargenomen data benadert, rekening houdend met de fout in het model. Lagere waarden duiden op een betere pasvorm, typisch onder 0.05 voor goede pasvorm en tussen 0.05 en 0.08 voor redelijke pasvorm.
 
```{r leerkracht motivatie om te presteren - item fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(item_fit)
```


Als bepaalde waarden niet goed passen in dit model, kan dit verschillende dingen betekenen.
Ten eerste kan het zijn dat de items niet allemaal dezelfde onderliggende eigenschap meten. Ten tweede kan het zijn dat sommige items verkeerd worden begrepen door de respondenten.
Ten derde kan het zijn dat de veronderstelling van lokale onafhankelijkheid wordt geschonden. Dit betekent dat de reacties op de items afhankelijk zijn van elkaar, zelfs na controle voor de onderliggende eigenschap. Bijvoorbeeld, als het beantwoorden van het ene item helpt bij het beantwoorden van een ander item, dan zullen deze items niet goed passen in het model.

Hieronder volgt een robuste variant van de X2 test, namelijk de PV_Q1 test. Hieronder volgt een korte uitleg wat het exact betekent:

De 'PV_Q1'-statistiek is een afgeleide versie van de Q1-statistiek, een chi-kwadraatstatistiek gebruikt in item respons theorie (IRT) modellen om te testen of een item goed past bij het model. 
Een hoge 'PV_Q1'-waarde duidt op een slechte overeenkomst tussen de waargenomen gegevens en voorspellingen van het model, wat suggereert dat het item mogelijk niet conform het verwachte gedrag 
volgens het IRT-model functioneert. Een niet-significante p-waarde, meestal p > 0.05, in combinatie met lage 'PV_Q1'- en RMSEA.PV_Q1-waarden, geeft aan dat het item goed past bij het IRT-model.
Daarnaast is het belangrijk op te merken dat de 'PV_Q1'-statistiek robuust is tegen invloeden van steekproefomvang en lokale afhankelijkheid, waardoor het een betrouwbaar instrument is om de pasvorm
van een item met het IRT-model te beoordelen, ongeacht de omvang van de steekproef of eventuele lokale onderlinge afhankelijkheden tussen de waarnemingen. Hieronder de resultaten:

```{r leerkracht motivatie om te presteren - robust item fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(robust_item_fit)
```

Om een beter beeld te krijgen, volgen hieronder the item parameters van de vragen. 
In IRT, meet een factor loading (ook bekend als een discriminatieparameter) hoe goed een item onderscheid maakt tussen personen met verschillende niveaus van de onderliggende eigenschap. Een hoge factor loading betekent dat het item gevoelig is voor verschillen in de onderliggende eigenschap.
Het belangrijkste verschil met factor analyse, is dat in IRT, de factor loadings ook rekening houden met de moeilijkheidsgraad van de items. Dit betekent dat items die dezelfde moeilijkheidsgraad hebben (dat wil zeggen, ze zijn even moeilijk om te beantwoorden) worden geacht dezelfde hoeveelheid informatie te geven over de onderliggende eigenschap, ongeacht hun factor loadings. Dit is niet het geval in factoranalyse, waar de factor loadings alleen de relatie tussen de geobserveerde variabelen en de onderliggende factor meten.
Tevens kan het zijn dat de factor loadings en de item parameters niet overeenkomen. Bijvoorbeeld een item kan een hoge factor loading hebben (wat aangeeft dat het goed discrimineert) maar toch een slechte fit hebben (X2-statistiek) als het bijvoorbeeld verkeerd wordt begrepen door de respondenten. Omgekeerd kan een item goed passen bij het model maar niet goed discrimineren tussen verschillende niveaus van de eigenschap.
In een Graded response model, zoals hier gebruikt, zijn er meerdere moeilijkheids parameters, een voor elke vrijheidsgraaad van de vraag (totaal antwoordmogelijk - 1). 

```{r leerkracht motivatie om te presteren - item parameters, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(params)
params
```

##### samenwerken

```{r leerkracht samenwerken - stat extraction, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
# Model fit

fit <- irt_fit$gedrag_houding_samenwerken_irt_fit
model_fit <- as.data.frame(M2(fit, na.rm = TRUE))
model_fit <- cbind(model_fit, nrow(data))
model_fit <- rename(model_fit, `N` = `nrow(data)`)
model_fit <- tidyr::pivot_longer(model_fit, cols = -N, names_to = "Fit Index", values_to = "Value")
model_fit <- dplyr::select(model_fit, `Fit Index`, Value)
model_fit$`Cut-Off` <- c("", "", "> .05", "< .05 | .05 - .1 | > .1", "", "", "< .08 | .08 - .10 | > .10", "> .95 | .90 - .95 | < .90", "> .95 | .90 - .95 | < .90")

# item fit
item_fit <- itemfit(fit, na.rm = TRUE, fit_stats = "PV_Q1")
colnames(item_fit) <- c("Item", "X2", "df", "RMSEA", "p")
item_fit <- item_fit[order(item_fit$X2), ]
rownames(item_fit) <- NULL

robust_item_fit <- itemfit(fit, na.rm = TRUE)
colnames(robust_item_fit) <- c("Item", "PV_Q1", "df", "RMSEA", "p")
robust_item_fit <- robust_item_fit[order(robust_item_fit$PV_Q1), ]
rownames(robust_item_fit) <- NULL

# Item parameters
params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
params <- params[, -which(colnames(params) %in% c("items.g", "items.u", "means", "F1"))]
sink("/dev/null")
sum <- suppressWarnings(suppressMessages(summary(fit, silent = TRUE)))
sink()
params <- cbind(params, sum$rotF)
params <- cbind(params, sum$h2)
params <- params[, c("F1", colnames(params)[-which(colnames(params) %in% "F1")])]
colnames(params) <- c("Factor Loading", "Discrimination Parameter", "Difficulty Parameter 1", "Difficulty Parameter 2", "Difficulty Parameter 3", "Difficulty Parameter 4", "Communalities")
params <- params[order(params$`Factor Loading`, decreasing = TRUE), ]


# # Differential Item Functionining DIF
# # Prepare the data
# gender <- data$gender
# gender <- dplyr::recode(gender, "Man" = 1, "Vrouw" = 2, "Onbekend" = 2)
# gender <- as.factor(gender)
# local_df <- data[, !grepl("package_duration_raw|student_number|student_name|birth_date|gender", colnames(data))]

# # Fit the multiple group model
# dif_fit <- mirt::multipleGroup(
#   data = local_df,
#   model = 1,
#   group = gender
# )

# # Perform DIF analysis
# dif <- mirt::DIF(dif_fit, group = gender, which.par = c("a1", "d"), na.rm = TRUE, plotdif = TRUE)

# person fit
# person_fit <- personfit(fit) %>%
#   reframe(
#     infit.outside = prop.table(table(z.infit > 1.96 | z.infit < -1.96)),
#     outfit.outside = prop.table(table(z.outfit > 1.96 | z.outfit < -1.96))
#   ) # lower row = non-fitting people
# person_fit$Correct <- c("Correct", "Incorrect")
# person_fit <- person_fit %>% dplyr::select(Correct, everything())
```

###### Model Fit

Hieronder bevinden zich de verschillende model fit indices.Hierbij een korte uitleg wat het exact betekent:

- M2: Dit is de M2-statistiek van Maydeu-Olivares, die een maat is voor de absolute fit van het model. Een lager getal duidt op een betere fit. Er is geen vaste cut-off waarde, maar over het algemeen wordt een lagere waarde als beter beschouwd. Als de bijbehorende p-waarde kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- df: de degrees of freedom van het model. Dit is het aantal parameters dat is geschat, minus het aantal constraints dat is opgelegd.
- p-value: de p-waarde van de Goodness of fit test. Als deze kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- RMSEA: RMSEA meet hoe goed een model de waargenomen data benadert, rekening houdend met de fout in het model. Lagere waarden duiden op een betere pasvorm, typisch onder 0.05 voor goede pasvorm en tussen 0.05 en 0.08 voor redelijke pasvorm.
  - RMSEA_5 en RMSEA_95: de 95% betrouwbaarheidsinterval van de RMSEA. Hoe kleiner dit interval rondom de RMSEA waarde, hoe zekerder we zijn van die waarde. 
- SRMSR: SRMSR staat voor "Standardized Root Mean Square Residual". Het is een maat voor de gemiddelde discrepantie tussen de waargenomen en voorspelde covarianties. Een lagere waarde duidt op een betere fit. Een SRMSR van minder dan 0.08 wordt over het algemeen als goed beschouwd.
- TLI: de Tucker-Lewis Index. TLI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.
- CFI: de Comparative Fit Index. De CFI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.
- N: het aantal respondenten in de steekproef.

```{r leerkracht samenwerken - model fit, echo=FALSE, message=FALSE, warning=FALSE}
model_fit$`Conclusion` <- c("", "", "Poor", "Acceptable", "", "", "Good", "Poor", "Poor")
knitr::kable(model_fit)
```

###### Item Fit

Hieronder bevinden zich de verschillende item fit indices. Hierbij een korte uitleg wat het exact betekent:

- X2: Dit is de X2-statistiek van de item fit test. Een lager getal duidt op een betere fit. Er is geen vaste cut-off waarde, maar over het algemeen wordt een lagere waarde als beter beschouwd. Als de bijbehorende p-waarde kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het item niet goed past.
- df: de degrees of freedom van het model. Dit is het aantal parameters dat is geschat, minus het aantal constraints dat is opgelegd.
- p-value: de p-waarde van de Goodness of fit test. Als deze kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- RMSEA: RMSEA meet hoe goed een model de waargenomen data benadert, rekening houdend met de fout in het model. Lagere waarden duiden op een betere pasvorm, typisch onder 0.05 voor goede pasvorm en tussen 0.05 en 0.08 voor redelijke pasvorm.
 
```{r leerkracht samenwerken - item fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(item_fit)
```


Als bepaalde waarden niet goed passen in dit model, kan dit verschillende dingen betekenen.
Ten eerste kan het zijn dat de items niet allemaal dezelfde onderliggende eigenschap meten. Ten tweede kan het zijn dat sommige items verkeerd worden begrepen door de respondenten.
Ten derde kan het zijn dat de veronderstelling van lokale onafhankelijkheid wordt geschonden. Dit betekent dat de reacties op de items afhankelijk zijn van elkaar, zelfs na controle voor de onderliggende eigenschap. Bijvoorbeeld, als het beantwoorden van het ene item helpt bij het beantwoorden van een ander item, dan zullen deze items niet goed passen in het model.

Hieronder volgt een robuste variant van de X2 test, namelijk de PV_Q1 test. Hieronder volgt een korte uitleg wat het exact betekent:

De 'PV_Q1'-statistiek is een afgeleide versie van de Q1-statistiek, een chi-kwadraatstatistiek gebruikt in item respons theorie (IRT) modellen om te testen of een item goed past bij het model. 
Een hoge 'PV_Q1'-waarde duidt op een slechte overeenkomst tussen de waargenomen gegevens en voorspellingen van het model, wat suggereert dat het item mogelijk niet conform het verwachte gedrag 
volgens het IRT-model functioneert. Een niet-significante p-waarde, meestal p > 0.05, in combinatie met lage 'PV_Q1'- en RMSEA.PV_Q1-waarden, geeft aan dat het item goed past bij het IRT-model.
Daarnaast is het belangrijk op te merken dat de 'PV_Q1'-statistiek robuust is tegen invloeden van steekproefomvang en lokale afhankelijkheid, waardoor het een betrouwbaar instrument is om de pasvorm
van een item met het IRT-model te beoordelen, ongeacht de omvang van de steekproef of eventuele lokale onderlinge afhankelijkheden tussen de waarnemingen. Hieronder de resultaten:

```{r leerkracht samenwerken - robust item fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(robust_item_fit)
```

Om een beter beeld te krijgen, volgen hieronder the item parameters van de vragen. 
In IRT, meet een factor loading (ook bekend als een discriminatieparameter) hoe goed een item onderscheid maakt tussen personen met verschillende niveaus van de onderliggende eigenschap. Een hoge factor loading betekent dat het item gevoelig is voor verschillen in de onderliggende eigenschap.
Het belangrijkste verschil met factor analyse, is dat in IRT, de factor loadings ook rekening houden met de moeilijkheidsgraad van de items. Dit betekent dat items die dezelfde moeilijkheidsgraad hebben (dat wil zeggen, ze zijn even moeilijk om te beantwoorden) worden geacht dezelfde hoeveelheid informatie te geven over de onderliggende eigenschap, ongeacht hun factor loadings. Dit is niet het geval in factoranalyse, waar de factor loadings alleen de relatie tussen de geobserveerde variabelen en de onderliggende factor meten.
Tevens kan het zijn dat de factor loadings en de item parameters niet overeenkomen. Bijvoorbeeld een item kan een hoge factor loading hebben (wat aangeeft dat het goed discrimineert) maar toch een slechte fit hebben (X2-statistiek) als het bijvoorbeeld verkeerd wordt begrepen door de respondenten. Omgekeerd kan een item goed passen bij het model maar niet goed discrimineren tussen verschillende niveaus van de eigenschap.
In een Graded response model, zoals hier gebruikt, zijn er meerdere moeilijkheids parameters, een voor elke vrijheidsgraaad van de vraag (totaal antwoordmogelijk - 1). 

```{r leerkracht samenwerken - item parameters, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(params)
params
```


##### zelfvertrouwen

```{r leerkracht zelfvertrouwen - stat extraction, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
# Model fit

fit <- irt_fit$gedrag_houding_zelfvertrouwen_irt_fit
model_fit <- as.data.frame(M2(fit, na.rm = TRUE))
model_fit <- cbind(model_fit, nrow(data))
model_fit <- rename(model_fit, `N` = `nrow(data)`)
model_fit <- tidyr::pivot_longer(model_fit, cols = -N, names_to = "Fit Index", values_to = "Value")
model_fit <- dplyr::select(model_fit, `Fit Index`, Value)
model_fit$`Cut-Off` <- c("", "", "> .05", "< .05 | .05 - .1 | > .1", "", "", "< .08 | .08 - .10 | > .10", "> .95 | .90 - .95 | < .90", "> .95 | .90 - .95 | < .90")

# item fit
item_fit <- itemfit(fit, na.rm = TRUE, fit_stats = "PV_Q1")
colnames(item_fit) <- c("Item", "X2", "df", "RMSEA", "p")
item_fit <- item_fit[order(item_fit$X2), ]
rownames(item_fit) <- NULL

robust_item_fit <- itemfit(fit, na.rm = TRUE)
colnames(robust_item_fit) <- c("Item", "PV_Q1", "df", "RMSEA", "p")
robust_item_fit <- robust_item_fit[order(robust_item_fit$PV_Q1), ]
rownames(robust_item_fit) <- NULL

# Item parameters
params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
params <- params[, -which(colnames(params) %in% c("items.g", "items.u", "means", "F1"))]
sink("/dev/null")
sum <- suppressWarnings(suppressMessages(summary(fit, silent = TRUE)))
sink()
params <- cbind(params, sum$rotF)
params <- cbind(params, sum$h2)
params <- params[, c("F1", colnames(params)[-which(colnames(params) %in% "F1")])]
colnames(params) <- c("Factor Loading", "Discrimination Parameter", "Difficulty Parameter 1", "Difficulty Parameter 2", "Difficulty Parameter 3", "Difficulty Parameter 4", "Communalities")
params <- params[order(params$`Factor Loading`, decreasing = TRUE), ]


# # Differential Item Functionining DIF
# # Prepare the data
# gender <- data$gender
# gender <- dplyr::recode(gender, "Man" = 1, "Vrouw" = 2, "Onbekend" = 2)
# gender <- as.factor(gender)
# local_df <- data[, !grepl("package_duration_raw|student_number|student_name|birth_date|gender", colnames(data))]

# # Fit the multiple group model
# dif_fit <- mirt::multipleGroup(
#   data = local_df,
#   model = 1,
#   group = gender
# )

# # Perform DIF analysis
# dif <- mirt::DIF(dif_fit, group = gender, which.par = c("a1", "d"), na.rm = TRUE, plotdif = TRUE)

# person fit
# person_fit <- personfit(fit) %>%
#   reframe(
#     infit.outside = prop.table(table(z.infit > 1.96 | z.infit < -1.96)),
#     outfit.outside = prop.table(table(z.outfit > 1.96 | z.outfit < -1.96))
#   ) # lower row = non-fitting people
# person_fit$Correct <- c("Correct", "Incorrect")
# person_fit <- person_fit %>% dplyr::select(Correct, everything())
```

###### Model Fit

Hieronder bevinden zich de verschillende model fit indices.Hierbij een korte uitleg wat het exact betekent:

- M2: Dit is de M2-statistiek van Maydeu-Olivares, die een maat is voor de absolute fit van het model. Een lager getal duidt op een betere fit. Er is geen vaste cut-off waarde, maar over het algemeen wordt een lagere waarde als beter beschouwd. Als de bijbehorende p-waarde kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- df: de degrees of freedom van het model. Dit is het aantal parameters dat is geschat, minus het aantal constraints dat is opgelegd.
- p-value: de p-waarde van de Goodness of fit test. Als deze kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- RMSEA: RMSEA meet hoe goed een model de waargenomen data benadert, rekening houdend met de fout in het model. Lagere waarden duiden op een betere pasvorm, typisch onder 0.05 voor goede pasvorm en tussen 0.05 en 0.08 voor redelijke pasvorm.
  - RMSEA_5 en RMSEA_95: de 95% betrouwbaarheidsinterval van de RMSEA. Hoe kleiner dit interval rondom de RMSEA waarde, hoe zekerder we zijn van die waarde. 
- SRMSR: SRMSR staat voor "Standardized Root Mean Square Residual". Het is een maat voor de gemiddelde discrepantie tussen de waargenomen en voorspelde covarianties. Een lagere waarde duidt op een betere fit. Een SRMSR van minder dan 0.08 wordt over het algemeen als goed beschouwd.
- TLI: de Tucker-Lewis Index. TLI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.
- CFI: de Comparative Fit Index. De CFI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.
- N: het aantal respondenten in de steekproef.

```{r leerkracht zelfvertrouwen - model fit, echo=FALSE, message=FALSE, warning=FALSE}
model_fit$`Conclusion` <- c("", "", "Poor", "Acceptable", "", "", "Good", "Poor", "Poor")
knitr::kable(model_fit)
```

###### Item Fit

Hieronder bevinden zich de verschillende item fit indices. Hierbij een korte uitleg wat het exact betekent:

- X2: Dit is de X2-statistiek van de item fit test. Een lager getal duidt op een betere fit. Er is geen vaste cut-off waarde, maar over het algemeen wordt een lagere waarde als beter beschouwd. Als de bijbehorende p-waarde kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het item niet goed past.
- df: de degrees of freedom van het model. Dit is het aantal parameters dat is geschat, minus het aantal constraints dat is opgelegd.
- p-value: de p-waarde van de Goodness of fit test. Als deze kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- RMSEA: RMSEA meet hoe goed een model de waargenomen data benadert, rekening houdend met de fout in het model. Lagere waarden duiden op een betere pasvorm, typisch onder 0.05 voor goede pasvorm en tussen 0.05 en 0.08 voor redelijke pasvorm.
 
```{r leerkracht zelfvertrouwen - item fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(item_fit)
```


Als bepaalde waarden niet goed passen in dit model, kan dit verschillende dingen betekenen.
Ten eerste kan het zijn dat de items niet allemaal dezelfde onderliggende eigenschap meten. Ten tweede kan het zijn dat sommige items verkeerd worden begrepen door de respondenten.
Ten derde kan het zijn dat de veronderstelling van lokale onafhankelijkheid wordt geschonden. Dit betekent dat de reacties op de items afhankelijk zijn van elkaar, zelfs na controle voor de onderliggende eigenschap. Bijvoorbeeld, als het beantwoorden van het ene item helpt bij het beantwoorden van een ander item, dan zullen deze items niet goed passen in het model.

Hieronder volgt een robuste variant van de X2 test, namelijk de PV_Q1 test. Hieronder volgt een korte uitleg wat het exact betekent:

De 'PV_Q1'-statistiek is een afgeleide versie van de Q1-statistiek, een chi-kwadraatstatistiek gebruikt in item respons theorie (IRT) modellen om te testen of een item goed past bij het model. 
Een hoge 'PV_Q1'-waarde duidt op een slechte overeenkomst tussen de waargenomen gegevens en voorspellingen van het model, wat suggereert dat het item mogelijk niet conform het verwachte gedrag 
volgens het IRT-model functioneert. Een niet-significante p-waarde, meestal p > 0.05, in combinatie met lage 'PV_Q1'- en RMSEA.PV_Q1-waarden, geeft aan dat het item goed past bij het IRT-model.
Daarnaast is het belangrijk op te merken dat de 'PV_Q1'-statistiek robuust is tegen invloeden van steekproefomvang en lokale afhankelijkheid, waardoor het een betrouwbaar instrument is om de pasvorm
van een item met het IRT-model te beoordelen, ongeacht de omvang van de steekproef of eventuele lokale onderlinge afhankelijkheden tussen de waarnemingen. Hieronder de resultaten:

```{r leerkracht zelfvertrouwen - robust item fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(robust_item_fit)
```

Om een beter beeld te krijgen, volgen hieronder the item parameters van de vragen. 
In IRT, meet een factor loading (ook bekend als een discriminatieparameter) hoe goed een item onderscheid maakt tussen personen met verschillende niveaus van de onderliggende eigenschap. Een hoge factor loading betekent dat het item gevoelig is voor verschillen in de onderliggende eigenschap.
Het belangrijkste verschil met factor analyse, is dat in IRT, de factor loadings ook rekening houden met de moeilijkheidsgraad van de items. Dit betekent dat items die dezelfde moeilijkheidsgraad hebben (dat wil zeggen, ze zijn even moeilijk om te beantwoorden) worden geacht dezelfde hoeveelheid informatie te geven over de onderliggende eigenschap, ongeacht hun factor loadings. Dit is niet het geval in factoranalyse, waar de factor loadings alleen de relatie tussen de geobserveerde variabelen en de onderliggende factor meten.
Tevens kan het zijn dat de factor loadings en de item parameters niet overeenkomen. Bijvoorbeeld een item kan een hoge factor loading hebben (wat aangeeft dat het goed discrimineert) maar toch een slechte fit hebben (X2-statistiek) als het bijvoorbeeld verkeerd wordt begrepen door de respondenten. Omgekeerd kan een item goed passen bij het model maar niet goed discrimineren tussen verschillende niveaus van de eigenschap.
In een Graded response model, zoals hier gebruikt, zijn er meerdere moeilijkheids parameters, een voor elke vrijheidsgraaad van de vraag (totaal antwoordmogelijk - 1). 

```{r leerkracht zelfvertrouwen - item parameters, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(params)
params
```


##### stabiel

```{r leerkracht stabiel - stat extraction, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
# Model fit

fit <- irt_fit$gedrag_houding_stabiel_irt_fit
model_fit <- as.data.frame(M2(fit, na.rm = TRUE))
model_fit <- cbind(model_fit, nrow(data))
model_fit <- rename(model_fit, `N` = `nrow(data)`)
model_fit <- tidyr::pivot_longer(model_fit, cols = -N, names_to = "Fit Index", values_to = "Value")
model_fit <- dplyr::select(model_fit, `Fit Index`, Value)
model_fit$`Cut-Off` <- c("", "", "> .05", "< .05 | .05 - .1 | > .1", "", "", "< .08 | .08 - .10 | > .10", "> .95 | .90 - .95 | < .90", "> .95 | .90 - .95 | < .90")

# item fit
item_fit <- itemfit(fit, na.rm = TRUE, fit_stats = "PV_Q1")
colnames(item_fit) <- c("Item", "X2", "df", "RMSEA", "p")
item_fit <- item_fit[order(item_fit$X2), ]
rownames(item_fit) <- NULL

robust_item_fit <- itemfit(fit, na.rm = TRUE)
colnames(robust_item_fit) <- c("Item", "PV_Q1", "df", "RMSEA", "p")
robust_item_fit <- robust_item_fit[order(robust_item_fit$PV_Q1), ]
rownames(robust_item_fit) <- NULL

# Item parameters
params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
params <- params[, -which(colnames(params) %in% c("items.g", "items.u", "means", "F1"))]
sink("/dev/null")
sum <- suppressWarnings(suppressMessages(summary(fit, silent = TRUE)))
sink()
params <- cbind(params, sum$rotF)
params <- cbind(params, sum$h2)
params <- params[, c("F1", colnames(params)[-which(colnames(params) %in% "F1")])]
colnames(params) <- c("Factor Loading", "Discrimination Parameter", "Difficulty Parameter 1", "Difficulty Parameter 2", "Difficulty Parameter 3", "Difficulty Parameter 4", "Communalities")
params <- params[order(params$`Factor Loading`, decreasing = TRUE), ]


# # Differential Item Functionining DIF
# # Prepare the data
# gender <- data$gender
# gender <- dplyr::recode(gender, "Man" = 1, "Vrouw" = 2, "Onbekend" = 2)
# gender <- as.factor(gender)
# local_df <- data[, !grepl("package_duration_raw|student_number|student_name|birth_date|gender", colnames(data))]

# # Fit the multiple group model
# dif_fit <- mirt::multipleGroup(
#   data = local_df,
#   model = 1,
#   group = gender
# )

# # Perform DIF analysis
# dif <- mirt::DIF(dif_fit, group = gender, which.par = c("a1", "d"), na.rm = TRUE, plotdif = TRUE)

# person fit
# person_fit <- personfit(fit) %>%
#   reframe(
#     infit.outside = prop.table(table(z.infit > 1.96 | z.infit < -1.96)),
#     outfit.outside = prop.table(table(z.outfit > 1.96 | z.outfit < -1.96))
#   ) # lower row = non-fitting people
# person_fit$Correct <- c("Correct", "Incorrect")
# person_fit <- person_fit %>% dplyr::select(Correct, everything())
```

###### Model Fit

Hieronder bevinden zich de verschillende model fit indices.Hierbij een korte uitleg wat het exact betekent:

- M2: Dit is de M2-statistiek van Maydeu-Olivares, die een maat is voor de absolute fit van het model. Een lager getal duidt op een betere fit. Er is geen vaste cut-off waarde, maar over het algemeen wordt een lagere waarde als beter beschouwd. Als de bijbehorende p-waarde kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- df: de degrees of freedom van het model. Dit is het aantal parameters dat is geschat, minus het aantal constraints dat is opgelegd.
- p-value: de p-waarde van de Goodness of fit test. Als deze kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- RMSEA: RMSEA meet hoe goed een model de waargenomen data benadert, rekening houdend met de fout in het model. Lagere waarden duiden op een betere pasvorm, typisch onder 0.05 voor goede pasvorm en tussen 0.05 en 0.08 voor redelijke pasvorm.
  - RMSEA_5 en RMSEA_95: de 95% betrouwbaarheidsinterval van de RMSEA. Hoe kleiner dit interval rondom de RMSEA waarde, hoe zekerder we zijn van die waarde. 
- SRMSR: SRMSR staat voor "Standardized Root Mean Square Residual". Het is een maat voor de gemiddelde discrepantie tussen de waargenomen en voorspelde covarianties. Een lagere waarde duidt op een betere fit. Een SRMSR van minder dan 0.08 wordt over het algemeen als goed beschouwd.
- TLI: de Tucker-Lewis Index. TLI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.
- CFI: de Comparative Fit Index. De CFI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.
- N: het aantal respondenten in de steekproef.

```{r leerkracht stabiel - model fit, echo=FALSE, message=FALSE, warning=FALSE}
model_fit$`Conclusion` <- c("", "", "Poor", "Acceptable", "", "", "Good", "Poor", "Poor")
knitr::kable(model_fit)
```

###### Item Fit

Hieronder bevinden zich de verschillende item fit indices. Hierbij een korte uitleg wat het exact betekent:

- X2: Dit is de X2-statistiek van de item fit test. Een lager getal duidt op een betere fit. Er is geen vaste cut-off waarde, maar over het algemeen wordt een lagere waarde als beter beschouwd. Als de bijbehorende p-waarde kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het item niet goed past.
- df: de degrees of freedom van het model. Dit is het aantal parameters dat is geschat, minus het aantal constraints dat is opgelegd.
- p-value: de p-waarde van de Goodness of fit test. Als deze kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- RMSEA: RMSEA meet hoe goed een model de waargenomen data benadert, rekening houdend met de fout in het model. Lagere waarden duiden op een betere pasvorm, typisch onder 0.05 voor goede pasvorm en tussen 0.05 en 0.08 voor redelijke pasvorm.
 
```{r leerkracht stabiel - item fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(item_fit)
```


Als bepaalde waarden niet goed passen in dit model, kan dit verschillende dingen betekenen.
Ten eerste kan het zijn dat de items niet allemaal dezelfde onderliggende eigenschap meten. Ten tweede kan het zijn dat sommige items verkeerd worden begrepen door de respondenten.
Ten derde kan het zijn dat de veronderstelling van lokale onafhankelijkheid wordt geschonden. Dit betekent dat de reacties op de items afhankelijk zijn van elkaar, zelfs na controle voor de onderliggende eigenschap. Bijvoorbeeld, als het beantwoorden van het ene item helpt bij het beantwoorden van een ander item, dan zullen deze items niet goed passen in het model.

Hieronder volgt een robuste variant van de X2 test, namelijk de PV_Q1 test. Hieronder volgt een korte uitleg wat het exact betekent:

De 'PV_Q1'-statistiek is een afgeleide versie van de Q1-statistiek, een chi-kwadraatstatistiek gebruikt in item respons theorie (IRT) modellen om te testen of een item goed past bij het model. 
Een hoge 'PV_Q1'-waarde duidt op een slechte overeenkomst tussen de waargenomen gegevens en voorspellingen van het model, wat suggereert dat het item mogelijk niet conform het verwachte gedrag 
volgens het IRT-model functioneert. Een niet-significante p-waarde, meestal p > 0.05, in combinatie met lage 'PV_Q1'- en RMSEA.PV_Q1-waarden, geeft aan dat het item goed past bij het IRT-model.
Daarnaast is het belangrijk op te merken dat de 'PV_Q1'-statistiek robuust is tegen invloeden van steekproefomvang en lokale afhankelijkheid, waardoor het een betrouwbaar instrument is om de pasvorm
van een item met het IRT-model te beoordelen, ongeacht de omvang van de steekproef of eventuele lokale onderlinge afhankelijkheden tussen de waarnemingen. Hieronder de resultaten:

```{r leerkracht stabiel - robust item fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(robust_item_fit)
```

Om een beter beeld te krijgen, volgen hieronder the item parameters van de vragen. 
In IRT, meet een factor loading (ook bekend als een discriminatieparameter) hoe goed een item onderscheid maakt tussen personen met verschillende niveaus van de onderliggende eigenschap. Een hoge factor loading betekent dat het item gevoelig is voor verschillen in de onderliggende eigenschap.
Het belangrijkste verschil met factor analyse, is dat in IRT, de factor loadings ook rekening houden met de moeilijkheidsgraad van de items. Dit betekent dat items die dezelfde moeilijkheidsgraad hebben (dat wil zeggen, ze zijn even moeilijk om te beantwoorden) worden geacht dezelfde hoeveelheid informatie te geven over de onderliggende eigenschap, ongeacht hun factor loadings. Dit is niet het geval in factoranalyse, waar de factor loadings alleen de relatie tussen de geobserveerde variabelen en de onderliggende factor meten.
Tevens kan het zijn dat de factor loadings en de item parameters niet overeenkomen. Bijvoorbeeld een item kan een hoge factor loading hebben (wat aangeeft dat het goed discrimineert) maar toch een slechte fit hebben (X2-statistiek) als het bijvoorbeeld verkeerd wordt begrepen door de respondenten. Omgekeerd kan een item goed passen bij het model maar niet goed discrimineren tussen verschillende niveaus van de eigenschap.
In een Graded response model, zoals hier gebruikt, zijn er meerdere moeilijkheids parameters, een voor elke vrijheidsgraaad van de vraag (totaal antwoordmogelijk - 1). 

```{r leerkracht stabiel - item parameters, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(params)
params
```

##### zorgvuldig

```{r leerkracht zorgvuldig - stat extraction, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
# Model fit

fit <- irt_fit$gedrag_houding_zorgvuldig_irt_fit
model_fit <- as.data.frame(M2(fit, na.rm = TRUE))
model_fit <- cbind(model_fit, nrow(data))
model_fit <- rename(model_fit, `N` = `nrow(data)`)
model_fit <- tidyr::pivot_longer(model_fit, cols = -N, names_to = "Fit Index", values_to = "Value")
model_fit <- dplyr::select(model_fit, `Fit Index`, Value)
model_fit$`Cut-Off` <- c("", "", "> .05", "< .05 | .05 - .1 | > .1", "", "", "< .08 | .08 - .10 | > .10", "> .95 | .90 - .95 | < .90", "> .95 | .90 - .95 | < .90")

# item fit
item_fit <- itemfit(fit, na.rm = TRUE, fit_stats = "PV_Q1")
colnames(item_fit) <- c("Item", "X2", "df", "RMSEA", "p")
item_fit <- item_fit[order(item_fit$X2), ]
rownames(item_fit) <- NULL

robust_item_fit <- itemfit(fit, na.rm = TRUE)
colnames(robust_item_fit) <- c("Item", "PV_Q1", "df", "RMSEA", "p")
robust_item_fit <- robust_item_fit[order(robust_item_fit$PV_Q1), ]
rownames(robust_item_fit) <- NULL

# Item parameters
params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
params <- params[, -which(colnames(params) %in% c("items.g", "items.u", "means", "F1"))]
sink("/dev/null")
sum <- suppressWarnings(suppressMessages(summary(fit, silent = TRUE)))
sink()
params <- cbind(params, sum$rotF)
params <- cbind(params, sum$h2)
params <- params[, c("F1", colnames(params)[-which(colnames(params) %in% "F1")])]
colnames(params) <- c("Factor Loading", "Discrimination Parameter", "Difficulty Parameter 1", "Difficulty Parameter 2", "Difficulty Parameter 3", "Difficulty Parameter 4", "Communalities")
params <- params[order(params$`Factor Loading`, decreasing = TRUE), ]


# # Differential Item Functionining DIF
# # Prepare the data
# gender <- data$gender
# gender <- dplyr::recode(gender, "Man" = 1, "Vrouw" = 2, "Onbekend" = 2)
# gender <- as.factor(gender)
# local_df <- data[, !grepl("package_duration_raw|student_number|student_name|birth_date|gender", colnames(data))]

# # Fit the multiple group model
# dif_fit <- mirt::multipleGroup(
#   data = local_df,
#   model = 1,
#   group = gender
# )

# # Perform DIF analysis
# dif <- mirt::DIF(dif_fit, group = gender, which.par = c("a1", "d"), na.rm = TRUE, plotdif = TRUE)

# person fit
# person_fit <- personfit(fit) %>%
#   reframe(
#     infit.outside = prop.table(table(z.infit > 1.96 | z.infit < -1.96)),
#     outfit.outside = prop.table(table(z.outfit > 1.96 | z.outfit < -1.96))
#   ) # lower row = non-fitting people
# person_fit$Correct <- c("Correct", "Incorrect")
# person_fit <- person_fit %>% dplyr::select(Correct, everything())
```

###### Model Fit

Hieronder bevinden zich de verschillende model fit indices.Hierbij een korte uitleg wat het exact betekent:

- M2: Dit is de M2-statistiek van Maydeu-Olivares, die een maat is voor de absolute fit van het model. Een lager getal duidt op een betere fit. Er is geen vaste cut-off waarde, maar over het algemeen wordt een lagere waarde als beter beschouwd. Als de bijbehorende p-waarde kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- df: de degrees of freedom van het model. Dit is het aantal parameters dat is geschat, minus het aantal constraints dat is opgelegd.
- p-value: de p-waarde van de Goodness of fit test. Als deze kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- RMSEA: RMSEA meet hoe goed een model de waargenomen data benadert, rekening houdend met de fout in het model. Lagere waarden duiden op een betere pasvorm, typisch onder 0.05 voor goede pasvorm en tussen 0.05 en 0.08 voor redelijke pasvorm.
  - RMSEA_5 en RMSEA_95: de 95% betrouwbaarheidsinterval van de RMSEA. Hoe kleiner dit interval rondom de RMSEA waarde, hoe zekerder we zijn van die waarde. 
- SRMSR: SRMSR staat voor "Standardized Root Mean Square Residual". Het is een maat voor de gemiddelde discrepantie tussen de waargenomen en voorspelde covarianties. Een lagere waarde duidt op een betere fit. Een SRMSR van minder dan 0.08 wordt over het algemeen als goed beschouwd.
- TLI: de Tucker-Lewis Index. TLI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.
- CFI: de Comparative Fit Index. De CFI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.
- N: het aantal respondenten in de steekproef.

```{r leerkracht zorgvuldig - model fit, echo=FALSE, message=FALSE, warning=FALSE}
model_fit$`Conclusion` <- c("", "", "Poor", "Acceptable", "", "", "Good", "Poor", "Poor")
knitr::kable(model_fit)
```

###### Item Fit

Hieronder bevinden zich de verschillende item fit indices. Hierbij een korte uitleg wat het exact betekent:

- X2: Dit is de X2-statistiek van de item fit test. Een lager getal duidt op een betere fit. Er is geen vaste cut-off waarde, maar over het algemeen wordt een lagere waarde als beter beschouwd. Als de bijbehorende p-waarde kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het item niet goed past.
- df: de degrees of freedom van het model. Dit is het aantal parameters dat is geschat, minus het aantal constraints dat is opgelegd.
- p-value: de p-waarde van de Goodness of fit test. Als deze kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- RMSEA: RMSEA meet hoe goed een model de waargenomen data benadert, rekening houdend met de fout in het model. Lagere waarden duiden op een betere pasvorm, typisch onder 0.05 voor goede pasvorm en tussen 0.05 en 0.08 voor redelijke pasvorm.
 
```{r leerkracht zorgvuldig - item fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(item_fit)
```


Als bepaalde waarden niet goed passen in dit model, kan dit verschillende dingen betekenen.
Ten eerste kan het zijn dat de items niet allemaal dezelfde onderliggende eigenschap meten. Ten tweede kan het zijn dat sommige items verkeerd worden begrepen door de respondenten.
Ten derde kan het zijn dat de veronderstelling van lokale onafhankelijkheid wordt geschonden. Dit betekent dat de reacties op de items afhankelijk zijn van elkaar, zelfs na controle voor de onderliggende eigenschap. Bijvoorbeeld, als het beantwoorden van het ene item helpt bij het beantwoorden van een ander item, dan zullen deze items niet goed passen in het model.

Hieronder volgt een robuste variant van de X2 test, namelijk de PV_Q1 test. Hieronder volgt een korte uitleg wat het exact betekent:

De 'PV_Q1'-statistiek is een afgeleide versie van de Q1-statistiek, een chi-kwadraatstatistiek gebruikt in item respons theorie (IRT) modellen om te testen of een item goed past bij het model. 
Een hoge 'PV_Q1'-waarde duidt op een slechte overeenkomst tussen de waargenomen gegevens en voorspellingen van het model, wat suggereert dat het item mogelijk niet conform het verwachte gedrag 
volgens het IRT-model functioneert. Een niet-significante p-waarde, meestal p > 0.05, in combinatie met lage 'PV_Q1'- en RMSEA.PV_Q1-waarden, geeft aan dat het item goed past bij het IRT-model.
Daarnaast is het belangrijk op te merken dat de 'PV_Q1'-statistiek robuust is tegen invloeden van steekproefomvang en lokale afhankelijkheid, waardoor het een betrouwbaar instrument is om de pasvorm
van een item met het IRT-model te beoordelen, ongeacht de omvang van de steekproef of eventuele lokale onderlinge afhankelijkheden tussen de waarnemingen. Hieronder de resultaten:

```{r leerkracht zorgvuldig - robust item fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(robust_item_fit)
```

Om een beter beeld te krijgen, volgen hieronder the item parameters van de vragen. 
In IRT, meet een factor loading (ook bekend als een discriminatieparameter) hoe goed een item onderscheid maakt tussen personen met verschillende niveaus van de onderliggende eigenschap. Een hoge factor loading betekent dat het item gevoelig is voor verschillen in de onderliggende eigenschap.
Het belangrijkste verschil met factor analyse, is dat in IRT, de factor loadings ook rekening houden met de moeilijkheidsgraad van de items. Dit betekent dat items die dezelfde moeilijkheidsgraad hebben (dat wil zeggen, ze zijn even moeilijk om te beantwoorden) worden geacht dezelfde hoeveelheid informatie te geven over de onderliggende eigenschap, ongeacht hun factor loadings. Dit is niet het geval in factoranalyse, waar de factor loadings alleen de relatie tussen de geobserveerde variabelen en de onderliggende factor meten.
Tevens kan het zijn dat de factor loadings en de item parameters niet overeenkomen. Bijvoorbeeld een item kan een hoge factor loading hebben (wat aangeeft dat het goed discrimineert) maar toch een slechte fit hebben (X2-statistiek) als het bijvoorbeeld verkeerd wordt begrepen door de respondenten. Omgekeerd kan een item goed passen bij het model maar niet goed discrimineren tussen verschillende niveaus van de eigenschap.
In een Graded response model, zoals hier gebruikt, zijn er meerdere moeilijkheids parameters, een voor elke vrijheidsgraaad van de vraag (totaal antwoordmogelijk - 1). 

```{r leerkracht zorgvuldig - item parameters, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(params)
params
```


##### open

```{r leerkracht open - stat extraction, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
# Model fit

fit <- irt_fit$gedrag_houding_open_irt_fit
model_fit <- as.data.frame(M2(fit, na.rm = TRUE))
model_fit <- cbind(model_fit, nrow(data))
model_fit <- rename(model_fit, `N` = `nrow(data)`)
model_fit <- tidyr::pivot_longer(model_fit, cols = -N, names_to = "Fit Index", values_to = "Value")
model_fit <- dplyr::select(model_fit, `Fit Index`, Value)
model_fit$`Cut-Off` <- c("", "", "> .05", "< .05 | .05 - .1 | > .1", "", "", "< .08 | .08 - .10 | > .10", "> .95 | .90 - .95 | < .90", "> .95 | .90 - .95 | < .90")

# item fit
item_fit <- itemfit(fit, na.rm = TRUE, fit_stats = "PV_Q1")
colnames(item_fit) <- c("Item", "X2", "df", "RMSEA", "p")
item_fit <- item_fit[order(item_fit$X2), ]
rownames(item_fit) <- NULL

robust_item_fit <- itemfit(fit, na.rm = TRUE)
colnames(robust_item_fit) <- c("Item", "PV_Q1", "df", "RMSEA", "p")
robust_item_fit <- robust_item_fit[order(robust_item_fit$PV_Q1), ]
rownames(robust_item_fit) <- NULL

# Item parameters
params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
params <- params[, -which(colnames(params) %in% c("items.g", "items.u", "means", "F1"))]
sink("/dev/null")
sum <- suppressWarnings(suppressMessages(summary(fit, silent = TRUE)))
sink()
params <- cbind(params, sum$rotF)
params <- cbind(params, sum$h2)
params <- params[, c("F1", colnames(params)[-which(colnames(params) %in% "F1")])]
colnames(params) <- c("Factor Loading", "Discrimination Parameter", "Difficulty Parameter 1", "Difficulty Parameter 2", "Difficulty Parameter 3", "Difficulty Parameter 4", "Communalities")
params <- params[order(params$`Factor Loading`, decreasing = TRUE), ]


# # Differential Item Functionining DIF
# # Prepare the data
# gender <- data$gender
# gender <- dplyr::recode(gender, "Man" = 1, "Vrouw" = 2, "Onbekend" = 2)
# gender <- as.factor(gender)
# local_df <- data[, !grepl("package_duration_raw|student_number|student_name|birth_date|gender", colnames(data))]

# # Fit the multiple group model
# dif_fit <- mirt::multipleGroup(
#   data = local_df,
#   model = 1,
#   group = gender
# )

# # Perform DIF analysis
# dif <- mirt::DIF(dif_fit, group = gender, which.par = c("a1", "d"), na.rm = TRUE, plotdif = TRUE)

# person fit
# person_fit <- personfit(fit) %>%
#   reframe(
#     infit.outside = prop.table(table(z.infit > 1.96 | z.infit < -1.96)),
#     outfit.outside = prop.table(table(z.outfit > 1.96 | z.outfit < -1.96))
#   ) # lower row = non-fitting people
# person_fit$Correct <- c("Correct", "Incorrect")
# person_fit <- person_fit %>% dplyr::select(Correct, everything())
```

###### Model Fit

Hieronder bevinden zich de verschillende model fit indices.Hierbij een korte uitleg wat het exact betekent:

- M2: Dit is de M2-statistiek van Maydeu-Olivares, die een maat is voor de absolute fit van het model. Een lager getal duidt op een betere fit. Er is geen vaste cut-off waarde, maar over het algemeen wordt een lagere waarde als beter beschouwd. Als de bijbehorende p-waarde kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- df: de degrees of freedom van het model. Dit is het aantal parameters dat is geschat, minus het aantal constraints dat is opgelegd.
- p-value: de p-waarde van de Goodness of fit test. Als deze kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- RMSEA: RMSEA meet hoe goed een model de waargenomen data benadert, rekening houdend met de fout in het model. Lagere waarden duiden op een betere pasvorm, typisch onder 0.05 voor goede pasvorm en tussen 0.05 en 0.08 voor redelijke pasvorm.
  - RMSEA_5 en RMSEA_95: de 95% betrouwbaarheidsinterval van de RMSEA. Hoe kleiner dit interval rondom de RMSEA waarde, hoe zekerder we zijn van die waarde. 
- SRMSR: SRMSR staat voor "Standardized Root Mean Square Residual". Het is een maat voor de gemiddelde discrepantie tussen de waargenomen en voorspelde covarianties. Een lagere waarde duidt op een betere fit. Een SRMSR van minder dan 0.08 wordt over het algemeen als goed beschouwd.
- TLI: de Tucker-Lewis Index. TLI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.
- CFI: de Comparative Fit Index. De CFI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.
- N: het aantal respondenten in de steekproef.

```{r leerkracht open - model fit, echo=FALSE, message=FALSE, warning=FALSE}
model_fit$`Conclusion` <- c("", "", "Poor", "Acceptable", "", "", "Good", "Poor", "Poor")
knitr::kable(model_fit)
```

###### Item Fit

Hieronder bevinden zich de verschillende item fit indices. Hierbij een korte uitleg wat het exact betekent:

- X2: Dit is de X2-statistiek van de item fit test. Een lager getal duidt op een betere fit. Er is geen vaste cut-off waarde, maar over het algemeen wordt een lagere waarde als beter beschouwd. Als de bijbehorende p-waarde kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het item niet goed past.
- df: de degrees of freedom van het model. Dit is het aantal parameters dat is geschat, minus het aantal constraints dat is opgelegd.
- p-value: de p-waarde van de Goodness of fit test. Als deze kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- RMSEA: RMSEA meet hoe goed een model de waargenomen data benadert, rekening houdend met de fout in het model. Lagere waarden duiden op een betere pasvorm, typisch onder 0.05 voor goede pasvorm en tussen 0.05 en 0.08 voor redelijke pasvorm.
 
```{r leerkracht open - item fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(item_fit)
```


Als bepaalde waarden niet goed passen in dit model, kan dit verschillende dingen betekenen.
Ten eerste kan het zijn dat de items niet allemaal dezelfde onderliggende eigenschap meten. Ten tweede kan het zijn dat sommige items verkeerd worden begrepen door de respondenten.
Ten derde kan het zijn dat de veronderstelling van lokale onafhankelijkheid wordt geschonden. Dit betekent dat de reacties op de items afhankelijk zijn van elkaar, zelfs na controle voor de onderliggende eigenschap. Bijvoorbeeld, als het beantwoorden van het ene item helpt bij het beantwoorden van een ander item, dan zullen deze items niet goed passen in het model.

Hieronder volgt een robuste variant van de X2 test, namelijk de PV_Q1 test. Hieronder volgt een korte uitleg wat het exact betekent:

De 'PV_Q1'-statistiek is een afgeleide versie van de Q1-statistiek, een chi-kwadraatstatistiek gebruikt in item respons theorie (IRT) modellen om te testen of een item goed past bij het model. 
Een hoge 'PV_Q1'-waarde duidt op een slechte overeenkomst tussen de waargenomen gegevens en voorspellingen van het model, wat suggereert dat het item mogelijk niet conform het verwachte gedrag 
volgens het IRT-model functioneert. Een niet-significante p-waarde, meestal p > 0.05, in combinatie met lage 'PV_Q1'- en RMSEA.PV_Q1-waarden, geeft aan dat het item goed past bij het IRT-model.
Daarnaast is het belangrijk op te merken dat de 'PV_Q1'-statistiek robuust is tegen invloeden van steekproefomvang en lokale afhankelijkheid, waardoor het een betrouwbaar instrument is om de pasvorm
van een item met het IRT-model te beoordelen, ongeacht de omvang van de steekproef of eventuele lokale onderlinge afhankelijkheden tussen de waarnemingen. Hieronder de resultaten:

```{r leerkracht open - robust item fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(robust_item_fit)
```

Om een beter beeld te krijgen, volgen hieronder the item parameters van de vragen. 
In IRT, meet een factor loading (ook bekend als een discriminatieparameter) hoe goed een item onderscheid maakt tussen personen met verschillende niveaus van de onderliggende eigenschap. Een hoge factor loading betekent dat het item gevoelig is voor verschillen in de onderliggende eigenschap.
Het belangrijkste verschil met factor analyse, is dat in IRT, de factor loadings ook rekening houden met de moeilijkheidsgraad van de items. Dit betekent dat items die dezelfde moeilijkheidsgraad hebben (dat wil zeggen, ze zijn even moeilijk om te beantwoorden) worden geacht dezelfde hoeveelheid informatie te geven over de onderliggende eigenschap, ongeacht hun factor loadings. Dit is niet het geval in factoranalyse, waar de factor loadings alleen de relatie tussen de geobserveerde variabelen en de onderliggende factor meten.
Tevens kan het zijn dat de factor loadings en de item parameters niet overeenkomen. Bijvoorbeeld een item kan een hoge factor loading hebben (wat aangeeft dat het goed discrimineert) maar toch een slechte fit hebben (X2-statistiek) als het bijvoorbeeld verkeerd wordt begrepen door de respondenten. Omgekeerd kan een item goed passen bij het model maar niet goed discrimineren tussen verschillende niveaus van de eigenschap.
In een Graded response model, zoals hier gebruikt, zijn er meerdere moeilijkheids parameters, een voor elke vrijheidsgraaad van de vraag (totaal antwoordmogelijk - 1). 

```{r leerkracht open - item parameters, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(params)
params
```


### Gedrag_houding (ouder)


#### Factor Analyse

```{r gedrag_houding_ouder$gedrag_houding_ouder - stat extraction, echo=FALSE, message=FALSE, warning=FALSE}
fit <- fitMeasures(gedrag_houding_ouder$gedrag_houding_ouder)

# null hypothesis: empirical chi-square


p <- round(fit["pvalue"][[1]], 3)
chisq <- fit["chisq"][[1]]

# model fit indices
rmsea <- fit["rmsea"][[1]] # < .06 or .08
p_rmsea <- fit["rmsea.pvalue"][[1]] # > .05
tli <- fit["tli"][[1]] # > 0.95
rms <- fit["crmr"][[1]] # correct RMS, < 0.08
cfi <- fit["cfi"][[1]] # > 0.90

# N
lavInspect(gedrag_houding_ouder$gedrag_houding_ouder, what = "nobs")

# Explained variance
r2 <- inspect(gedrag_houding_ouder$gedrag_houding_ouder, what = "r2")

# Factor loadings
loadings <- parameterEstimates(gedrag_houding_ouder$gedrag_houding_ouder)[parameterEstimates(gedrag_houding_ouder$gedrag_houding_ouder)$op == "=~", ][c("rhs", "est")]
weights <- inspect(gedrag_houding_ouder$gedrag_houding_ouder, what = "std")[["lambda"]] # These are the factor score coefficients, which are used to compute factor scores for each observation in your data.
```

##### Model Beoordeling

Hieronder volgen alle model fit van de factor analyse voor gedrag_houding_ouder. Hierbij moet rekening gehouden worden dat de factor analyses expliciet gemaakt zijn met 10 factoren,
en de onderliggende bijbehorende vragen alleen laden op de voor ingestelde factor. 

> @FLEUR: dit kan ik ook veranderen. m.a.w. dat ze wel op meerdere factoren laden. Dit geldt ook voor alle andere factor analyses. Maar ik het het nu zo gedaan
> om een wat globaler beeld te krijgen. Vaak zijn vragen die op deze manier slecht laden beter behorend bij andere factoren. Dit zou ik dan op jouw aanraden nog kunnen onderzoeken.
  
Hieronder, in tabel 2, zie je de model-fit statistieken van de CFA voor betekenissen. Hierbij een korte uitleg wat het exact betekent:

- p-value: de p-waarde van de empirische chi-square. Als deze kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- RMSEA: RMSEA meet hoe goed een model de waargenomen data benadert, rekening houdend met de fout in het model. Lagere waarden duiden op een betere pasvorm, typisch onder 0.05 voor goede pasvorm en tussen 0.05 en 0.08 voor redelijke pasvorm.
- TLI: de Tucker-Lewis Index. TLI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.
- CRMS: de Corrected Root Mean Square. Dit kwantificeert het gemiddelde verschil tussen waargenomen en voorspelde waarden in een model. Het geeft een maat voor hoe goed de voorspellingen van het model overeenkomen met de daadwerkelijke gegevens. Lagere RMRS-waarden geven een betere overeenkomst aan, wat wijst op een nauwkeuriger model.
- CFI: de Comparative Fit Index. De CFI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.


**Tabel 2: Betekenissen - Model-fit statistieken**

```{r gedrag_houding_ouder - model fit table, echo=FALSE, message=FALSE, warning=FALSE}
# Create dataframe
df <- data.frame(
  Statistic_Name = c("p-value", "RMSEA", "TLI", "CRMS", "CFI"),
  Result = c(p, rmsea, tli, rms, cfi),
  Cut_off = c("> .05", "< .05 | .05 - .1 | > .1", "> .95 | .90 - .95 | < .90", "< .08 | .08 - .10 | > .10", "> .95 | .90 - .95 | < .90"),
  Conclusion = c("Poor", "Good", "Poor", "Good", "Poor")
)

# Create table
knitr::kable(df, col.names = c("Statistic", "Result", "Cut-off", "Conclusion"))
```


De model-fit resultaten zijn niet heel positief.
Gezien de gevoeligheid van p-waardes in empirische chi-square testen, en de hoge hoeveelheid observaties, is het verstandiger
de andere fit indexen te hanteren.

Als conclusie hier is dat het model goed past, en (dus) een factor voldoende is.


##### Item Beoordeling

Hieronder staan de factor loadings, factor weights, communalities, en uniquenesses weergegeven. Hieronder een beknopte uitleg van wat deze waarden betekenen:

- Factor loadings: de factor loadings geven aan hoe sterk een item correleert met een factor. Hoe hoger de factor loading, hoe sterker de correlatie. Een factor loading van .3 of hoger wordt gezien als een goede factor loading.
- Factor weights: de factor weights geven aan hoeveel gewicht een item heeft op een factor. Deze wordt gebruikt om de factor scores te berekenen. Hoe hoger de factor weight, hoe meer gewicht een item heeft op een factor.


**Tabel 3: Betekenissen - Model-fit statistieken**

```{r gedrag_houding_ouder - Item loadings, echo=FALSE, message=FALSE, warning=FALSE}
# Create dataframe

# Create table
loadings <- loadings[order(loadings$est, decreasing = TRUE), ]
knitr::kable(loadings, col.names = c("Naam", "std. Factor Loading"))
```

Op basis van de factor loadings en de communality/uniqueness waardes, kunnen twee reccomendaties gemaakt worden.
Een factor loading van een vraag wordt als "goed" gezien zodra deze hoger ligt dan .3. Dit is bij meerdere vragen
niet het geval. Specifiek gaat dit om de volgende vragen (van hoogste naar laagste):

- ASL_pers_lln_083_open          : 0.2751208
- ASL_pers_lln_058_zelfvertrouwen: 0.2321667

Hier volgen nog de gewichten van alle vragen op diens eigen factoren:

**Tabel 4: Betekenissen - Model-fit statistieken**

```{r gedrag_houding_ouder - Weights, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(weights)
```


Zie figuur 3 voor een grafische weergave:

**Figuur 3: Factor Loadings Betekenissen**

```{r gedrag_houding_ouder - factor loadings plot, echo=FALSE, message=FALSE, warning=FALSE}
plot <- plot(loadings$est)
plot <- lines(loadings$est)
plot <- abline(h = 0.3, lty = 2) # lty = 2 makes the line dashed
plot <- title(main = "Factor Loadings Betekenissen", sub = "Item factor loadings; order based on item factor loadings order")
```



#### Item Response Analyse
```{r ouder gedrag_houding  - load data, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
# load data
irt_fit <- readRDS(paste0(here(), "/data/item_analyses_pers2024/irt_fit_gedrag_houding_ouder.rds"))
data <- read.csv(paste0(here(), "/data/data_interrim/gedrag_houding.csv"))
```

##### Betrokkenheid thuis

```{r ouder betrokkenheid_thuis - stat extraction, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
# Model fit
fit <- irt_fit$gedrag_houding_betrokkenheid_thuis_irt_fit
model_fit <- as.data.frame(M2(fit, na.rm = TRUE))
model_fit <- cbind(model_fit, nrow(data))
model_fit <- rename(model_fit, `N` = `nrow(data)`)
model_fit <- tidyr::pivot_longer(model_fit, cols = -N, names_to = "Fit Index", values_to = "Value")
model_fit <- dplyr::select(model_fit, `Fit Index`, Value)
model_fit$`Cut-Off` <- c("", "", "> .05", "< .05 | .05 - .1 | > .1", "", "", "< .08 | .08 - .10 | > .10", "> .95 | .90 - .95 | < .90", "> .95 | .90 - .95 | < .90")

# item fit
item_fit <- itemfit(fit, na.rm = TRUE, fit_stats = "PV_Q1")
colnames(item_fit) <- c("Item", "X2", "df", "RMSEA", "p")
item_fit <- item_fit[order(item_fit$X2), ]
rownames(item_fit) <- NULL

robust_item_fit <- itemfit(fit, na.rm = TRUE)
colnames(robust_item_fit) <- c("Item", "PV_Q1", "df", "RMSEA", "p")
robust_item_fit <- robust_item_fit[order(robust_item_fit$PV_Q1), ]
rownames(robust_item_fit) <- NULL

# Item parameters
params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
params <- params[, -which(colnames(params) %in% c("items.g", "items.u", "means", "F1"))]
sink("/dev/null")
sum <- suppressWarnings(suppressMessages(summary(fit, silent = TRUE)))
sink()
params <- cbind(params, sum$rotF)
params <- cbind(params, sum$h2)
params <- params[, c("F1", colnames(params)[-which(colnames(params) %in% "F1")])]
colnames(params) <- c("Factor Loading", "Discrimination Parameter", "Difficulty Parameter 1", "Difficulty Parameter 2", "Difficulty Parameter 3", "Difficulty Parameter 4", "Communalities")
params <- params[order(params$`Factor Loading`, decreasing = TRUE), ]


# # Differential Item Functionining DIF
# # Prepare the data
# gender <- data$gender
# gender <- dplyr::recode(gender, "Man" = 1, "Vrouw" = 2, "Onbekend" = 2)
# gender <- as.factor(gender)
# local_df <- data[, !grepl("package_duration_raw|student_number|student_name|birth_date|gender", colnames(data))]

# # Fit the multiple group model
# dif_fit <- mirt::multipleGroup(
#   data = local_df,
#   model = 1,
#   group = gender
# )

# # Perform DIF analysis
# dif <- mirt::DIF(dif_fit, group = gender, which.par = c("a1", "d"), na.rm = TRUE, plotdif = TRUE)

# person fit
# person_fit <- personfit(fit) %>%
#   reframe(
#     infit.outside = prop.table(table(z.infit > 1.96 | z.infit < -1.96)),
#     outfit.outside = prop.table(table(z.outfit > 1.96 | z.outfit < -1.96))
#   ) # lower row = non-fitting people
# person_fit$Correct <- c("Correct", "Incorrect")
# person_fit <- person_fit %>% dplyr::select(Correct, everything())
```

###### Model Fit

Hieronder bevinden zich de verschillende model fit indices.Hierbij een korte uitleg wat het exact betekent:

- M2: Dit is de M2-statistiek van Maydeu-Olivares, die een maat is voor de absolute fit van het model. Een lager getal duidt op een betere fit. Er is geen vaste cut-off waarde, maar over het algemeen wordt een lagere waarde als beter beschouwd. Als de bijbehorende p-waarde kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- df: de degrees of freedom van het model. Dit is het aantal parameters dat is geschat, minus het aantal constraints dat is opgelegd.
- p-value: de p-waarde van de Goodness of fit test. Als deze kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- RMSEA: RMSEA meet hoe goed een model de waargenomen data benadert, rekening houdend met de fout in het model. Lagere waarden duiden op een betere pasvorm, typisch onder 0.05 voor goede pasvorm en tussen 0.05 en 0.08 voor redelijke pasvorm.
  - RMSEA_5 en RMSEA_95: de 95% betrouwbaarheidsinterval van de RMSEA. Hoe kleiner dit interval rondom de RMSEA waarde, hoe zekerder we zijn van die waarde. 
- SRMSR: SRMSR staat voor "Standardized Root Mean Square Residual". Het is een maat voor de gemiddelde discrepantie tussen de waargenomen en voorspelde covarianties. Een lagere waarde duidt op een betere fit. Een SRMSR van minder dan 0.08 wordt over het algemeen als goed beschouwd.
- TLI: de Tucker-Lewis Index. TLI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.
- CFI: de Comparative Fit Index. De CFI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.
- N: het aantal respondenten in de steekproef.

```{r ouder betrokkenheid thuis - model fit, echo=FALSE, message=FALSE, warning=FALSE}
model_fit$`Conclusion` <- c("", "", "Poor", "Acceptable", "", "", "Good", "Poor", "Poor")
knitr::kable(model_fit)
```

###### Item Fit

Hieronder bevinden zich de verschillende item fit indices. Hierbij een korte uitleg wat het exact betekent:

- X2: Dit is de X2-statistiek van de item fit test. Een lager getal duidt op een betere fit. Er is geen vaste cut-off waarde, maar over het algemeen wordt een lagere waarde als beter beschouwd. Als de bijbehorende p-waarde kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het item niet goed past.
- df: de degrees of freedom van het model. Dit is het aantal parameters dat is geschat, minus het aantal constraints dat is opgelegd.
- p-value: de p-waarde van de Goodness of fit test. Als deze kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- RMSEA: RMSEA meet hoe goed een model de waargenomen data benadert, rekening houdend met de fout in het model. Lagere waarden duiden op een betere pasvorm, typisch onder 0.05 voor goede pasvorm en tussen 0.05 en 0.08 voor redelijke pasvorm.
 
```{r ouder betrokkenheid thuis - item fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(item_fit)
```


Als bepaalde waarden niet goed passen in dit model, kan dit verschillende dingen betekenen.
Ten eerste kan het zijn dat de items niet allemaal dezelfde onderliggende eigenschap meten. Ten tweede kan het zijn dat sommige items verkeerd worden begrepen door de respondenten.
Ten derde kan het zijn dat de veronderstelling van lokale onafhankelijkheid wordt geschonden. Dit betekent dat de reacties op de items afhankelijk zijn van elkaar, zelfs na controle voor de onderliggende eigenschap. Bijvoorbeeld, als het beantwoorden van het ene item helpt bij het beantwoorden van een ander item, dan zullen deze items niet goed passen in het model.

Hieronder volgt een robuste variant van de X2 test, namelijk de PV_Q1 test. Hieronder volgt een korte uitleg wat het exact betekent:

De 'PV_Q1'-statistiek is een afgeleide versie van de Q1-statistiek, een chi-kwadraatstatistiek gebruikt in item respons theorie (IRT) modellen om te testen of een item goed past bij het model. 
Een hoge 'PV_Q1'-waarde duidt op een slechte overeenkomst tussen de waargenomen gegevens en voorspellingen van het model, wat suggereert dat het item mogelijk niet conform het verwachte gedrag 
volgens het IRT-model functioneert. Een niet-significante p-waarde, meestal p > 0.05, in combinatie met lage 'PV_Q1'- en RMSEA.PV_Q1-waarden, geeft aan dat het item goed past bij het IRT-model.
Daarnaast is het belangrijk op te merken dat de 'PV_Q1'-statistiek robuust is tegen invloeden van steekproefomvang en lokale afhankelijkheid, waardoor het een betrouwbaar instrument is om de pasvorm
van een item met het IRT-model te beoordelen, ongeacht de omvang van de steekproef of eventuele lokale onderlinge afhankelijkheden tussen de waarnemingen. Hieronder de resultaten:

```{r ouder betrokkenheid thuis - robust item fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(robust_item_fit)
```

Om een beter beeld te krijgen, volgen hieronder the item parameters van de vragen. 
In IRT, meet een factor loading (ook bekend als een discriminatieparameter) hoe goed een item onderscheid maakt tussen personen met verschillende niveaus van de onderliggende eigenschap. Een hoge factor loading betekent dat het item gevoelig is voor verschillen in de onderliggende eigenschap.
Het belangrijkste verschil met factor analyse, is dat in IRT, de factor loadings ook rekening houden met de moeilijkheidsgraad van de items. Dit betekent dat items die dezelfde moeilijkheidsgraad hebben (dat wil zeggen, ze zijn even moeilijk om te beantwoorden) worden geacht dezelfde hoeveelheid informatie te geven over de onderliggende eigenschap, ongeacht hun factor loadings. Dit is niet het geval in factoranalyse, waar de factor loadings alleen de relatie tussen de geobserveerde variabelen en de onderliggende factor meten.
Tevens kan het zijn dat de factor loadings en de item parameters niet overeenkomen. Bijvoorbeeld een item kan een hoge factor loading hebben (wat aangeeft dat het goed discrimineert) maar toch een slechte fit hebben (X2-statistiek) als het bijvoorbeeld verkeerd wordt begrepen door de respondenten. Omgekeerd kan een item goed passen bij het model maar niet goed discrimineren tussen verschillende niveaus van de eigenschap.
In een Graded response model, zoals hier gebruikt, zijn er meerdere moeilijkheids parameters, een voor elke vrijheidsgraaad van de vraag (totaal antwoordmogelijk - 1). 

```{r ouder betrokkenheid thuis - item parameters, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(params)
params
```

##### concentratie

```{r ouder concentratie - stat extraction, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
# Model fit

fit <- irt_fit$gedrag_houding_concentratie_irt_fit
model_fit <- as.data.frame(M2(fit, na.rm = TRUE))
model_fit <- cbind(model_fit, nrow(data))
model_fit <- rename(model_fit, `N` = `nrow(data)`)
model_fit <- tidyr::pivot_longer(model_fit, cols = -N, names_to = "Fit Index", values_to = "Value")
model_fit <- dplyr::select(model_fit, `Fit Index`, Value)
model_fit$`Cut-Off` <- c("", "", "> .05", "< .05 | .05 - .1 | > .1", "", "", "< .08 | .08 - .10 | > .10", "> .95 | .90 - .95 | < .90", "> .95 | .90 - .95 | < .90")

# item fit
item_fit <- itemfit(fit, na.rm = TRUE, fit_stats = "PV_Q1")
colnames(item_fit) <- c("Item", "X2", "df", "RMSEA", "p")
item_fit <- item_fit[order(item_fit$X2), ]
rownames(item_fit) <- NULL

robust_item_fit <- itemfit(fit, na.rm = TRUE)
colnames(robust_item_fit) <- c("Item", "PV_Q1", "df", "RMSEA", "p")
robust_item_fit <- robust_item_fit[order(robust_item_fit$PV_Q1), ]
rownames(robust_item_fit) <- NULL

# Item parameters
params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
params <- params[, -which(colnames(params) %in% c("items.g", "items.u", "means", "F1"))]
sink("/dev/null")
sum <- suppressWarnings(suppressMessages(summary(fit, silent = TRUE)))
sink()
params <- cbind(params, sum$rotF)
params <- cbind(params, sum$h2)
params <- params[, c("F1", colnames(params)[-which(colnames(params) %in% "F1")])]
colnames(params) <- c("Factor Loading", "Discrimination Parameter", "Difficulty Parameter 1", "Difficulty Parameter 2", "Difficulty Parameter 3", "Difficulty Parameter 4", "Communalities")
params <- params[order(params$`Factor Loading`, decreasing = TRUE), ]


# # Differential Item Functionining DIF
# # Prepare the data
# gender <- data$gender
# gender <- dplyr::recode(gender, "Man" = 1, "Vrouw" = 2, "Onbekend" = 2)
# gender <- as.factor(gender)
# local_df <- data[, !grepl("package_duration_raw|student_number|student_name|birth_date|gender", colnames(data))]

# # Fit the multiple group model
# dif_fit <- mirt::multipleGroup(
#   data = local_df,
#   model = 1,
#   group = gender
# )

# # Perform DIF analysis
# dif <- mirt::DIF(dif_fit, group = gender, which.par = c("a1", "d"), na.rm = TRUE, plotdif = TRUE)

# person fit
# person_fit <- personfit(fit) %>%
#   reframe(
#     infit.outside = prop.table(table(z.infit > 1.96 | z.infit < -1.96)),
#     outfit.outside = prop.table(table(z.outfit > 1.96 | z.outfit < -1.96))
#   ) # lower row = non-fitting people
# person_fit$Correct <- c("Correct", "Incorrect")
# person_fit <- person_fit %>% dplyr::select(Correct, everything())
```

###### Model Fit

Hieronder bevinden zich de verschillende model fit indices.Hierbij een korte uitleg wat het exact betekent:

- M2: Dit is de M2-statistiek van Maydeu-Olivares, die een maat is voor de absolute fit van het model. Een lager getal duidt op een betere fit. Er is geen vaste cut-off waarde, maar over het algemeen wordt een lagere waarde als beter beschouwd. Als de bijbehorende p-waarde kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- df: de degrees of freedom van het model. Dit is het aantal parameters dat is geschat, minus het aantal constraints dat is opgelegd.
- p-value: de p-waarde van de Goodness of fit test. Als deze kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- RMSEA: RMSEA meet hoe goed een model de waargenomen data benadert, rekening houdend met de fout in het model. Lagere waarden duiden op een betere pasvorm, typisch onder 0.05 voor goede pasvorm en tussen 0.05 en 0.08 voor redelijke pasvorm.
  - RMSEA_5 en RMSEA_95: de 95% betrouwbaarheidsinterval van de RMSEA. Hoe kleiner dit interval rondom de RMSEA waarde, hoe zekerder we zijn van die waarde. 
- SRMSR: SRMSR staat voor "Standardized Root Mean Square Residual". Het is een maat voor de gemiddelde discrepantie tussen de waargenomen en voorspelde covarianties. Een lagere waarde duidt op een betere fit. Een SRMSR van minder dan 0.08 wordt over het algemeen als goed beschouwd.
- TLI: de Tucker-Lewis Index. TLI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.
- CFI: de Comparative Fit Index. De CFI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.
- N: het aantal respondenten in de steekproef.

```{r ouder concentratie - model fit, echo=FALSE, message=FALSE, warning=FALSE}
model_fit$`Conclusion` <- c("", "", "Poor", "Acceptable", "", "", "Good", "Poor", "Poor")
knitr::kable(model_fit)
```

###### Item Fit

Hieronder bevinden zich de verschillende item fit indices. Hierbij een korte uitleg wat het exact betekent:

- X2: Dit is de X2-statistiek van de item fit test. Een lager getal duidt op een betere fit. Er is geen vaste cut-off waarde, maar over het algemeen wordt een lagere waarde als beter beschouwd. Als de bijbehorende p-waarde kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het item niet goed past.
- df: de degrees of freedom van het model. Dit is het aantal parameters dat is geschat, minus het aantal constraints dat is opgelegd.
- p-value: de p-waarde van de Goodness of fit test. Als deze kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- RMSEA: RMSEA meet hoe goed een model de waargenomen data benadert, rekening houdend met de fout in het model. Lagere waarden duiden op een betere pasvorm, typisch onder 0.05 voor goede pasvorm en tussen 0.05 en 0.08 voor redelijke pasvorm.
 
```{r ouder concentratie - item fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(item_fit)
```


Als bepaalde waarden niet goed passen in dit model, kan dit verschillende dingen betekenen.
Ten eerste kan het zijn dat de items niet allemaal dezelfde onderliggende eigenschap meten. Ten tweede kan het zijn dat sommige items verkeerd worden begrepen door de respondenten.
Ten derde kan het zijn dat de veronderstelling van lokale onafhankelijkheid wordt geschonden. Dit betekent dat de reacties op de items afhankelijk zijn van elkaar, zelfs na controle voor de onderliggende eigenschap. Bijvoorbeeld, als het beantwoorden van het ene item helpt bij het beantwoorden van een ander item, dan zullen deze items niet goed passen in het model.

Hieronder volgt een robuste variant van de X2 test, namelijk de PV_Q1 test. Hieronder volgt een korte uitleg wat het exact betekent:

De 'PV_Q1'-statistiek is een afgeleide versie van de Q1-statistiek, een chi-kwadraatstatistiek gebruikt in item respons theorie (IRT) modellen om te testen of een item goed past bij het model. 
Een hoge 'PV_Q1'-waarde duidt op een slechte overeenkomst tussen de waargenomen gegevens en voorspellingen van het model, wat suggereert dat het item mogelijk niet conform het verwachte gedrag 
volgens het IRT-model functioneert. Een niet-significante p-waarde, meestal p > 0.05, in combinatie met lage 'PV_Q1'- en RMSEA.PV_Q1-waarden, geeft aan dat het item goed past bij het IRT-model.
Daarnaast is het belangrijk op te merken dat de 'PV_Q1'-statistiek robuust is tegen invloeden van steekproefomvang en lokale afhankelijkheid, waardoor het een betrouwbaar instrument is om de pasvorm
van een item met het IRT-model te beoordelen, ongeacht de omvang van de steekproef of eventuele lokale onderlinge afhankelijkheden tussen de waarnemingen. Hieronder de resultaten:

```{r ouder concentratie - robust item fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(robust_item_fit)
```

Om een beter beeld te krijgen, volgen hieronder the item parameters van de vragen. 
In IRT, meet een factor loading (ook bekend als een discriminatieparameter) hoe goed een item onderscheid maakt tussen personen met verschillende niveaus van de onderliggende eigenschap. Een hoge factor loading betekent dat het item gevoelig is voor verschillen in de onderliggende eigenschap.
Het belangrijkste verschil met factor analyse, is dat in IRT, de factor loadings ook rekening houden met de moeilijkheidsgraad van de items. Dit betekent dat items die dezelfde moeilijkheidsgraad hebben (dat wil zeggen, ze zijn even moeilijk om te beantwoorden) worden geacht dezelfde hoeveelheid informatie te geven over de onderliggende eigenschap, ongeacht hun factor loadings. Dit is niet het geval in factoranalyse, waar de factor loadings alleen de relatie tussen de geobserveerde variabelen en de onderliggende factor meten.
Tevens kan het zijn dat de factor loadings en de item parameters niet overeenkomen. Bijvoorbeeld een item kan een hoge factor loading hebben (wat aangeeft dat het goed discrimineert) maar toch een slechte fit hebben (X2-statistiek) als het bijvoorbeeld verkeerd wordt begrepen door de respondenten. Omgekeerd kan een item goed passen bij het model maar niet goed discrimineren tussen verschillende niveaus van de eigenschap.
In een Graded response model, zoals hier gebruikt, zijn er meerdere moeilijkheids parameters, een voor elke vrijheidsgraaad van de vraag (totaal antwoordmogelijk - 1). 

```{r ouder concentratie - item parameters, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(params)
params
```

##### contact met leeftijdsgenoten

```{r ouder contact met leeftijdsgenoten - stat extraction, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
# Model fit

# fit <- irt_fit$gedrag_houding_contact_met_leeftijdsgenoten_irt_fit
# model_fit <- as.data.frame(M2(fit, na.rm = TRUE))
# model_fit <- cbind(model_fit, nrow(data))
# model_fit <- rename(model_fit, `N` = `nrow(data)`)
# model_fit <- tidyr::pivot_longer(model_fit, cols = -N, names_to = "Fit Index", values_to = "Value")
# model_fit <- dplyr::select(model_fit, `Fit Index`, Value)
# model_fit$`Cut-Off` <- c("", "", "> .05", "< .05 | .05 - .1 | > .1", "", "", "< .08 | .08 - .10 | > .10", "> .95 | .90 - .95 | < .90", "> .95 | .90 - .95 | < .90")

# item fit
item_fit <- itemfit(fit, na.rm = TRUE, fit_stats = "PV_Q1")
colnames(item_fit) <- c("Item", "X2", "df", "RMSEA", "p")
item_fit <- item_fit[order(item_fit$X2), ]
rownames(item_fit) <- NULL

robust_item_fit <- itemfit(fit, na.rm = TRUE)
colnames(robust_item_fit) <- c("Item", "PV_Q1", "df", "RMSEA", "p")
robust_item_fit <- robust_item_fit[order(robust_item_fit$PV_Q1), ]
rownames(robust_item_fit) <- NULL

# Item parameters
params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
params <- params[, -which(colnames(params) %in% c("items.g", "items.u", "means", "F1"))]
sink("/dev/null")
sum <- suppressWarnings(suppressMessages(summary(fit, silent = TRUE)))
sink()
params <- cbind(params, sum$rotF)
params <- cbind(params, sum$h2)
params <- params[, c("F1", colnames(params)[-which(colnames(params) %in% "F1")])]
colnames(params) <- c("Factor Loading", "Discrimination Parameter", "Difficulty Parameter 1", "Difficulty Parameter 2", "Difficulty Parameter 3", "Difficulty Parameter 4", "Communalities")
params <- params[order(params$`Factor Loading`, decreasing = TRUE), ]


# # Differential Item Functionining DIF
# # Prepare the data
# gender <- data$gender
# gender <- dplyr::recode(gender, "Man" = 1, "Vrouw" = 2, "Onbekend" = 2)
# gender <- as.factor(gender)
# local_df <- data[, !grepl("package_duration_raw|student_number|student_name|birth_date|gender", colnames(data))]

# # Fit the multiple group model
# dif_fit <- mirt::multipleGroup(
#   data = local_df,
#   model = 1,
#   group = gender
# )

# # Perform DIF analysis
# dif <- mirt::DIF(dif_fit, group = gender, which.par = c("a1", "d"), na.rm = TRUE, plotdif = TRUE)

# person fit
# person_fit <- personfit(fit) %>%
#   reframe(
#     infit.outside = prop.table(table(z.infit > 1.96 | z.infit < -1.96)),
#     outfit.outside = prop.table(table(z.outfit > 1.96 | z.outfit < -1.96))
#   ) # lower row = non-fitting people
# person_fit$Correct <- c("Correct", "Incorrect")
# person_fit <- person_fit %>% dplyr::select(Correct, everything())
```

###### Model Fit

Hieronder bevinden zich de verschillende model fit indices.Hierbij een korte uitleg wat het exact betekent:

- M2: Dit is de M2-statistiek van Maydeu-Olivares, die een maat is voor de absolute fit van het model. Een lager getal duidt op een betere fit. Er is geen vaste cut-off waarde, maar over het algemeen wordt een lagere waarde als beter beschouwd. Als de bijbehorende p-waarde kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- df: de degrees of freedom van het model. Dit is het aantal parameters dat is geschat, minus het aantal constraints dat is opgelegd.
- p-value: de p-waarde van de Goodness of fit test. Als deze kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- RMSEA: RMSEA meet hoe goed een model de waargenomen data benadert, rekening houdend met de fout in het model. Lagere waarden duiden op een betere pasvorm, typisch onder 0.05 voor goede pasvorm en tussen 0.05 en 0.08 voor redelijke pasvorm.
  - RMSEA_5 en RMSEA_95: de 95% betrouwbaarheidsinterval van de RMSEA. Hoe kleiner dit interval rondom de RMSEA waarde, hoe zekerder we zijn van die waarde. 
- SRMSR: SRMSR staat voor "Standardized Root Mean Square Residual". Het is een maat voor de gemiddelde discrepantie tussen de waargenomen en voorspelde covarianties. Een lagere waarde duidt op een betere fit. Een SRMSR van minder dan 0.08 wordt over het algemeen als goed beschouwd.
- TLI: de Tucker-Lewis Index. TLI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.
- CFI: de Comparative Fit Index. De CFI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.
- N: het aantal respondenten in de steekproef.

```{r ouder contact met leeftijdsgenoten - model fit, echo=FALSE, message=FALSE, warning=FALSE}
# model_fit$`Conclusion` <- c("", "", "Poor", "Acceptable", "", "", "Good", "Poor", "Poor")
# knitr::kable(model_fit)
```

###### Item Fit

Hieronder bevinden zich de verschillende item fit indices. Hierbij een korte uitleg wat het exact betekent:

- X2: Dit is de X2-statistiek van de item fit test. Een lager getal duidt op een betere fit. Er is geen vaste cut-off waarde, maar over het algemeen wordt een lagere waarde als beter beschouwd. Als de bijbehorende p-waarde kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het item niet goed past.
- df: de degrees of freedom van het model. Dit is het aantal parameters dat is geschat, minus het aantal constraints dat is opgelegd.
- p-value: de p-waarde van de Goodness of fit test. Als deze kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- RMSEA: RMSEA meet hoe goed een model de waargenomen data benadert, rekening houdend met de fout in het model. Lagere waarden duiden op een betere pasvorm, typisch onder 0.05 voor goede pasvorm en tussen 0.05 en 0.08 voor redelijke pasvorm.
 
```{r ouder contact met leeftijdsgenoten - item fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(item_fit)
```


Als bepaalde waarden niet goed passen in dit model, kan dit verschillende dingen betekenen.
Ten eerste kan het zijn dat de items niet allemaal dezelfde onderliggende eigenschap meten. Ten tweede kan het zijn dat sommige items verkeerd worden begrepen door de respondenten.
Ten derde kan het zijn dat de veronderstelling van lokale onafhankelijkheid wordt geschonden. Dit betekent dat de reacties op de items afhankelijk zijn van elkaar, zelfs na controle voor de onderliggende eigenschap. Bijvoorbeeld, als het beantwoorden van het ene item helpt bij het beantwoorden van een ander item, dan zullen deze items niet goed passen in het model.

Hieronder volgt een robuste variant van de X2 test, namelijk de PV_Q1 test. Hieronder volgt een korte uitleg wat het exact betekent:

De 'PV_Q1'-statistiek is een afgeleide versie van de Q1-statistiek, een chi-kwadraatstatistiek gebruikt in item respons theorie (IRT) modellen om te testen of een item goed past bij het model. 
Een hoge 'PV_Q1'-waarde duidt op een slechte overeenkomst tussen de waargenomen gegevens en voorspellingen van het model, wat suggereert dat het item mogelijk niet conform het verwachte gedrag 
volgens het IRT-model functioneert. Een niet-significante p-waarde, meestal p > 0.05, in combinatie met lage 'PV_Q1'- en RMSEA.PV_Q1-waarden, geeft aan dat het item goed past bij het IRT-model.
Daarnaast is het belangrijk op te merken dat de 'PV_Q1'-statistiek robuust is tegen invloeden van steekproefomvang en lokale afhankelijkheid, waardoor het een betrouwbaar instrument is om de pasvorm
van een item met het IRT-model te beoordelen, ongeacht de omvang van de steekproef of eventuele lokale onderlinge afhankelijkheden tussen de waarnemingen. Hieronder de resultaten:

```{r ouder contact met leeftijdsgenoten - robust item fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(robust_item_fit)
```

Om een beter beeld te krijgen, volgen hieronder the item parameters van de vragen. 
In IRT, meet een factor loading (ook bekend als een discriminatieparameter) hoe goed een item onderscheid maakt tussen personen met verschillende niveaus van de onderliggende eigenschap. Een hoge factor loading betekent dat het item gevoelig is voor verschillen in de onderliggende eigenschap.
Het belangrijkste verschil met factor analyse, is dat in IRT, de factor loadings ook rekening houden met de moeilijkheidsgraad van de items. Dit betekent dat items die dezelfde moeilijkheidsgraad hebben (dat wil zeggen, ze zijn even moeilijk om te beantwoorden) worden geacht dezelfde hoeveelheid informatie te geven over de onderliggende eigenschap, ongeacht hun factor loadings. Dit is niet het geval in factoranalyse, waar de factor loadings alleen de relatie tussen de geobserveerde variabelen en de onderliggende factor meten.
Tevens kan het zijn dat de factor loadings en de item parameters niet overeenkomen. Bijvoorbeeld een item kan een hoge factor loading hebben (wat aangeeft dat het goed discrimineert) maar toch een slechte fit hebben (X2-statistiek) als het bijvoorbeeld verkeerd wordt begrepen door de respondenten. Omgekeerd kan een item goed passen bij het model maar niet goed discrimineren tussen verschillende niveaus van de eigenschap.
In een Graded response model, zoals hier gebruikt, zijn er meerdere moeilijkheids parameters, een voor elke vrijheidsgraaad van de vraag (totaal antwoordmogelijk - 1). 

```{r ouder contact met leeftijdsgenoten - item parameters, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(params)
params
```


##### interesse in school

```{r ouder interesse in school - stat extraction, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
# Model fit

fit <- irt_fit$gedrag_houding_interesse_in_school_irt_fit
model_fit <- as.data.frame(M2(fit, na.rm = TRUE))
model_fit <- cbind(model_fit, nrow(data))
model_fit <- rename(model_fit, `N` = `nrow(data)`)
model_fit <- tidyr::pivot_longer(model_fit, cols = -N, names_to = "Fit Index", values_to = "Value")
model_fit <- dplyr::select(model_fit, `Fit Index`, Value)
model_fit$`Cut-Off` <- c("", "", "> .05", "< .05 | .05 - .1 | > .1", "", "", "< .08 | .08 - .10 | > .10", "> .95 | .90 - .95 | < .90", "> .95 | .90 - .95 | < .90")

# item fit
item_fit <- itemfit(fit, na.rm = TRUE, fit_stats = "PV_Q1")
colnames(item_fit) <- c("Item", "X2", "df", "RMSEA", "p")
item_fit <- item_fit[order(item_fit$X2), ]
rownames(item_fit) <- NULL

robust_item_fit <- itemfit(fit, na.rm = TRUE)
colnames(robust_item_fit) <- c("Item", "PV_Q1", "df", "RMSEA", "p")
robust_item_fit <- robust_item_fit[order(robust_item_fit$PV_Q1), ]
rownames(robust_item_fit) <- NULL

# Item parameters
params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
params <- params[, -which(colnames(params) %in% c("items.g", "items.u", "means", "F1"))]
sink("/dev/null")
sum <- suppressWarnings(suppressMessages(summary(fit, silent = TRUE)))
sink()
params <- cbind(params, sum$rotF)
params <- cbind(params, sum$h2)
params <- params[, c("F1", colnames(params)[-which(colnames(params) %in% "F1")])]
colnames(params) <- c("Factor Loading", "Discrimination Parameter", "Difficulty Parameter 1", "Difficulty Parameter 2", "Difficulty Parameter 3", "Difficulty Parameter 4", "Communalities")
params <- params[order(params$`Factor Loading`, decreasing = TRUE), ]


# # Differential Item Functionining DIF
# # Prepare the data
# gender <- data$gender
# gender <- dplyr::recode(gender, "Man" = 1, "Vrouw" = 2, "Onbekend" = 2)
# gender <- as.factor(gender)
# local_df <- data[, !grepl("package_duration_raw|student_number|student_name|birth_date|gender", colnames(data))]

# # Fit the multiple group model
# dif_fit <- mirt::multipleGroup(
#   data = local_df,
#   model = 1,
#   group = gender
# )

# # Perform DIF analysis
# dif <- mirt::DIF(dif_fit, group = gender, which.par = c("a1", "d"), na.rm = TRUE, plotdif = TRUE)

# person fit
# person_fit <- personfit(fit) %>%
#   reframe(
#     infit.outside = prop.table(table(z.infit > 1.96 | z.infit < -1.96)),
#     outfit.outside = prop.table(table(z.outfit > 1.96 | z.outfit < -1.96))
#   ) # lower row = non-fitting people
# person_fit$Correct <- c("Correct", "Incorrect")
# person_fit <- person_fit %>% dplyr::select(Correct, everything())
```

###### Model Fit

Hieronder bevinden zich de verschillende model fit indices.Hierbij een korte uitleg wat het exact betekent:

- M2: Dit is de M2-statistiek van Maydeu-Olivares, die een maat is voor de absolute fit van het model. Een lager getal duidt op een betere fit. Er is geen vaste cut-off waarde, maar over het algemeen wordt een lagere waarde als beter beschouwd. Als de bijbehorende p-waarde kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- df: de degrees of freedom van het model. Dit is het aantal parameters dat is geschat, minus het aantal constraints dat is opgelegd.
- p-value: de p-waarde van de Goodness of fit test. Als deze kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- RMSEA: RMSEA meet hoe goed een model de waargenomen data benadert, rekening houdend met de fout in het model. Lagere waarden duiden op een betere pasvorm, typisch onder 0.05 voor goede pasvorm en tussen 0.05 en 0.08 voor redelijke pasvorm.
  - RMSEA_5 en RMSEA_95: de 95% betrouwbaarheidsinterval van de RMSEA. Hoe kleiner dit interval rondom de RMSEA waarde, hoe zekerder we zijn van die waarde. 
- SRMSR: SRMSR staat voor "Standardized Root Mean Square Residual". Het is een maat voor de gemiddelde discrepantie tussen de waargenomen en voorspelde covarianties. Een lagere waarde duidt op een betere fit. Een SRMSR van minder dan 0.08 wordt over het algemeen als goed beschouwd.
- TLI: de Tucker-Lewis Index. TLI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.
- CFI: de Comparative Fit Index. De CFI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.
- N: het aantal respondenten in de steekproef.

```{r ouder interesse in school - model fit, echo=FALSE, message=FALSE, warning=FALSE}
model_fit$`Conclusion` <- c("", "", "Poor", "Acceptable", "", "", "Good", "Poor", "Poor")
knitr::kable(model_fit)
```

###### Item Fit

Hieronder bevinden zich de verschillende item fit indices. Hierbij een korte uitleg wat het exact betekent:

- X2: Dit is de X2-statistiek van de item fit test. Een lager getal duidt op een betere fit. Er is geen vaste cut-off waarde, maar over het algemeen wordt een lagere waarde als beter beschouwd. Als de bijbehorende p-waarde kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het item niet goed past.
- df: de degrees of freedom van het model. Dit is het aantal parameters dat is geschat, minus het aantal constraints dat is opgelegd.
- p-value: de p-waarde van de Goodness of fit test. Als deze kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- RMSEA: RMSEA meet hoe goed een model de waargenomen data benadert, rekening houdend met de fout in het model. Lagere waarden duiden op een betere pasvorm, typisch onder 0.05 voor goede pasvorm en tussen 0.05 en 0.08 voor redelijke pasvorm.
 
```{r ouder interesse in school - item fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(item_fit)
```


Als bepaalde waarden niet goed passen in dit model, kan dit verschillende dingen betekenen.
Ten eerste kan het zijn dat de items niet allemaal dezelfde onderliggende eigenschap meten. Ten tweede kan het zijn dat sommige items verkeerd worden begrepen door de respondenten.
Ten derde kan het zijn dat de veronderstelling van lokale onafhankelijkheid wordt geschonden. Dit betekent dat de reacties op de items afhankelijk zijn van elkaar, zelfs na controle voor de onderliggende eigenschap. Bijvoorbeeld, als het beantwoorden van het ene item helpt bij het beantwoorden van een ander item, dan zullen deze items niet goed passen in het model.

Hieronder volgt een robuste variant van de X2 test, namelijk de PV_Q1 test. Hieronder volgt een korte uitleg wat het exact betekent:

De 'PV_Q1'-statistiek is een afgeleide versie van de Q1-statistiek, een chi-kwadraatstatistiek gebruikt in item respons theorie (IRT) modellen om te testen of een item goed past bij het model. 
Een hoge 'PV_Q1'-waarde duidt op een slechte overeenkomst tussen de waargenomen gegevens en voorspellingen van het model, wat suggereert dat het item mogelijk niet conform het verwachte gedrag 
volgens het IRT-model functioneert. Een niet-significante p-waarde, meestal p > 0.05, in combinatie met lage 'PV_Q1'- en RMSEA.PV_Q1-waarden, geeft aan dat het item goed past bij het IRT-model.
Daarnaast is het belangrijk op te merken dat de 'PV_Q1'-statistiek robuust is tegen invloeden van steekproefomvang en lokale afhankelijkheid, waardoor het een betrouwbaar instrument is om de pasvorm
van een item met het IRT-model te beoordelen, ongeacht de omvang van de steekproef of eventuele lokale onderlinge afhankelijkheden tussen de waarnemingen. Hieronder de resultaten:

```{r ouder interesse in school - robust item fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(robust_item_fit)
```

Om een beter beeld te krijgen, volgen hieronder the item parameters van de vragen. 
In IRT, meet een factor loading (ook bekend als een discriminatieparameter) hoe goed een item onderscheid maakt tussen personen met verschillende niveaus van de onderliggende eigenschap. Een hoge factor loading betekent dat het item gevoelig is voor verschillen in de onderliggende eigenschap.
Het belangrijkste verschil met factor analyse, is dat in IRT, de factor loadings ook rekening houden met de moeilijkheidsgraad van de items. Dit betekent dat items die dezelfde moeilijkheidsgraad hebben (dat wil zeggen, ze zijn even moeilijk om te beantwoorden) worden geacht dezelfde hoeveelheid informatie te geven over de onderliggende eigenschap, ongeacht hun factor loadings. Dit is niet het geval in factoranalyse, waar de factor loadings alleen de relatie tussen de geobserveerde variabelen en de onderliggende factor meten.
Tevens kan het zijn dat de factor loadings en de item parameters niet overeenkomen. Bijvoorbeeld een item kan een hoge factor loading hebben (wat aangeeft dat het goed discrimineert) maar toch een slechte fit hebben (X2-statistiek) als het bijvoorbeeld verkeerd wordt begrepen door de respondenten. Omgekeerd kan een item goed passen bij het model maar niet goed discrimineren tussen verschillende niveaus van de eigenschap.
In een Graded response model, zoals hier gebruikt, zijn er meerdere moeilijkheids parameters, een voor elke vrijheidsgraaad van de vraag (totaal antwoordmogelijk - 1). 

```{r ouder interesse in school - item parameters, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(params)
params
```

##### motivatie om te presteren

```{r ouder motivatie om te presteren - stat extraction, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
# Model fit

fit <- irt_fit$gedrag_houding_motivatie_om_te_presteren_irt_fit
model_fit <- as.data.frame(M2(fit, na.rm = TRUE))
model_fit <- cbind(model_fit, nrow(data))
model_fit <- rename(model_fit, `N` = `nrow(data)`)
model_fit <- tidyr::pivot_longer(model_fit, cols = -N, names_to = "Fit Index", values_to = "Value")
model_fit <- dplyr::select(model_fit, `Fit Index`, Value)
model_fit$`Cut-Off` <- c("", "", "> .05", "< .05 | .05 - .1 | > .1", "", "", "< .08 | .08 - .10 | > .10", "> .95 | .90 - .95 | < .90", "> .95 | .90 - .95 | < .90")

# item fit
item_fit <- itemfit(fit, na.rm = TRUE, fit_stats = "PV_Q1")
colnames(item_fit) <- c("Item", "X2", "df", "RMSEA", "p")
item_fit <- item_fit[order(item_fit$X2), ]
rownames(item_fit) <- NULL

robust_item_fit <- itemfit(fit, na.rm = TRUE)
colnames(robust_item_fit) <- c("Item", "PV_Q1", "df", "RMSEA", "p")
robust_item_fit <- robust_item_fit[order(robust_item_fit$PV_Q1), ]
rownames(robust_item_fit) <- NULL

# Item parameters
params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
params <- params[, -which(colnames(params) %in% c("items.g", "items.u", "means", "F1"))]
sink("/dev/null")
sum <- suppressWarnings(suppressMessages(summary(fit, silent = TRUE)))
sink()
params <- cbind(params, sum$rotF)
params <- cbind(params, sum$h2)
params <- params[, c("F1", colnames(params)[-which(colnames(params) %in% "F1")])]
colnames(params) <- c("Factor Loading", "Discrimination Parameter", "Difficulty Parameter 1", "Difficulty Parameter 2", "Difficulty Parameter 3", "Difficulty Parameter 4", "Communalities")
params <- params[order(params$`Factor Loading`, decreasing = TRUE), ]


# # Differential Item Functionining DIF
# # Prepare the data
# gender <- data$gender
# gender <- dplyr::recode(gender, "Man" = 1, "Vrouw" = 2, "Onbekend" = 2)
# gender <- as.factor(gender)
# local_df <- data[, !grepl("package_duration_raw|student_number|student_name|birth_date|gender", colnames(data))]

# # Fit the multiple group model
# dif_fit <- mirt::multipleGroup(
#   data = local_df,
#   model = 1,
#   group = gender
# )

# # Perform DIF analysis
# dif <- mirt::DIF(dif_fit, group = gender, which.par = c("a1", "d"), na.rm = TRUE, plotdif = TRUE)

# person fit
# person_fit <- personfit(fit) %>%
#   reframe(
#     infit.outside = prop.table(table(z.infit > 1.96 | z.infit < -1.96)),
#     outfit.outside = prop.table(table(z.outfit > 1.96 | z.outfit < -1.96))
#   ) # lower row = non-fitting people
# person_fit$Correct <- c("Correct", "Incorrect")
# person_fit <- person_fit %>% dplyr::select(Correct, everything())
```

###### Model Fit

Hieronder bevinden zich de verschillende model fit indices.Hierbij een korte uitleg wat het exact betekent:

- M2: Dit is de M2-statistiek van Maydeu-Olivares, die een maat is voor de absolute fit van het model. Een lager getal duidt op een betere fit. Er is geen vaste cut-off waarde, maar over het algemeen wordt een lagere waarde als beter beschouwd. Als de bijbehorende p-waarde kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- df: de degrees of freedom van het model. Dit is het aantal parameters dat is geschat, minus het aantal constraints dat is opgelegd.
- p-value: de p-waarde van de Goodness of fit test. Als deze kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- RMSEA: RMSEA meet hoe goed een model de waargenomen data benadert, rekening houdend met de fout in het model. Lagere waarden duiden op een betere pasvorm, typisch onder 0.05 voor goede pasvorm en tussen 0.05 en 0.08 voor redelijke pasvorm.
  - RMSEA_5 en RMSEA_95: de 95% betrouwbaarheidsinterval van de RMSEA. Hoe kleiner dit interval rondom de RMSEA waarde, hoe zekerder we zijn van die waarde. 
- SRMSR: SRMSR staat voor "Standardized Root Mean Square Residual". Het is een maat voor de gemiddelde discrepantie tussen de waargenomen en voorspelde covarianties. Een lagere waarde duidt op een betere fit. Een SRMSR van minder dan 0.08 wordt over het algemeen als goed beschouwd.
- TLI: de Tucker-Lewis Index. TLI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.
- CFI: de Comparative Fit Index. De CFI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.
- N: het aantal respondenten in de steekproef.

```{r ouder motivatie om te presteren - model fit, echo=FALSE, message=FALSE, warning=FALSE}
model_fit$`Conclusion` <- c("", "", "Poor", "Acceptable", "", "", "Good", "Poor", "Poor")
knitr::kable(model_fit)
```

###### Item Fit

Hieronder bevinden zich de verschillende item fit indices. Hierbij een korte uitleg wat het exact betekent:

- X2: Dit is de X2-statistiek van de item fit test. Een lager getal duidt op een betere fit. Er is geen vaste cut-off waarde, maar over het algemeen wordt een lagere waarde als beter beschouwd. Als de bijbehorende p-waarde kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het item niet goed past.
- df: de degrees of freedom van het model. Dit is het aantal parameters dat is geschat, minus het aantal constraints dat is opgelegd.
- p-value: de p-waarde van de Goodness of fit test. Als deze kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- RMSEA: RMSEA meet hoe goed een model de waargenomen data benadert, rekening houdend met de fout in het model. Lagere waarden duiden op een betere pasvorm, typisch onder 0.05 voor goede pasvorm en tussen 0.05 en 0.08 voor redelijke pasvorm.
 
```{r ouder motivatie om te presteren - item fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(item_fit)
```


Als bepaalde waarden niet goed passen in dit model, kan dit verschillende dingen betekenen.
Ten eerste kan het zijn dat de items niet allemaal dezelfde onderliggende eigenschap meten. Ten tweede kan het zijn dat sommige items verkeerd worden begrepen door de respondenten.
Ten derde kan het zijn dat de veronderstelling van lokale onafhankelijkheid wordt geschonden. Dit betekent dat de reacties op de items afhankelijk zijn van elkaar, zelfs na controle voor de onderliggende eigenschap. Bijvoorbeeld, als het beantwoorden van het ene item helpt bij het beantwoorden van een ander item, dan zullen deze items niet goed passen in het model.

Hieronder volgt een robuste variant van de X2 test, namelijk de PV_Q1 test. Hieronder volgt een korte uitleg wat het exact betekent:

De 'PV_Q1'-statistiek is een afgeleide versie van de Q1-statistiek, een chi-kwadraatstatistiek gebruikt in item respons theorie (IRT) modellen om te testen of een item goed past bij het model. 
Een hoge 'PV_Q1'-waarde duidt op een slechte overeenkomst tussen de waargenomen gegevens en voorspellingen van het model, wat suggereert dat het item mogelijk niet conform het verwachte gedrag 
volgens het IRT-model functioneert. Een niet-significante p-waarde, meestal p > 0.05, in combinatie met lage 'PV_Q1'- en RMSEA.PV_Q1-waarden, geeft aan dat het item goed past bij het IRT-model.
Daarnaast is het belangrijk op te merken dat de 'PV_Q1'-statistiek robuust is tegen invloeden van steekproefomvang en lokale afhankelijkheid, waardoor het een betrouwbaar instrument is om de pasvorm
van een item met het IRT-model te beoordelen, ongeacht de omvang van de steekproef of eventuele lokale onderlinge afhankelijkheden tussen de waarnemingen. Hieronder de resultaten:

```{r ouder motivatie om te presteren - robust item fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(robust_item_fit)
```

Om een beter beeld te krijgen, volgen hieronder the item parameters van de vragen. 
In IRT, meet een factor loading (ook bekend als een discriminatieparameter) hoe goed een item onderscheid maakt tussen personen met verschillende niveaus van de onderliggende eigenschap. Een hoge factor loading betekent dat het item gevoelig is voor verschillen in de onderliggende eigenschap.
Het belangrijkste verschil met factor analyse, is dat in IRT, de factor loadings ook rekening houden met de moeilijkheidsgraad van de items. Dit betekent dat items die dezelfde moeilijkheidsgraad hebben (dat wil zeggen, ze zijn even moeilijk om te beantwoorden) worden geacht dezelfde hoeveelheid informatie te geven over de onderliggende eigenschap, ongeacht hun factor loadings. Dit is niet het geval in factoranalyse, waar de factor loadings alleen de relatie tussen de geobserveerde variabelen en de onderliggende factor meten.
Tevens kan het zijn dat de factor loadings en de item parameters niet overeenkomen. Bijvoorbeeld een item kan een hoge factor loading hebben (wat aangeeft dat het goed discrimineert) maar toch een slechte fit hebben (X2-statistiek) als het bijvoorbeeld verkeerd wordt begrepen door de respondenten. Omgekeerd kan een item goed passen bij het model maar niet goed discrimineren tussen verschillende niveaus van de eigenschap.
In een Graded response model, zoals hier gebruikt, zijn er meerdere moeilijkheids parameters, een voor elke vrijheidsgraaad van de vraag (totaal antwoordmogelijk - 1). 

```{r ouder motivatie om te presteren - item parameters, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(params)
params
```

##### samenwerken

```{r ouder samenwerken - stat extraction, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
# Model fit

fit <- irt_fit$gedrag_houding_samenwerken_irt_fit
model_fit <- as.data.frame(M2(fit, na.rm = TRUE))
model_fit <- cbind(model_fit, nrow(data))
model_fit <- rename(model_fit, `N` = `nrow(data)`)
model_fit <- tidyr::pivot_longer(model_fit, cols = -N, names_to = "Fit Index", values_to = "Value")
model_fit <- dplyr::select(model_fit, `Fit Index`, Value)
model_fit$`Cut-Off` <- c("", "", "> .05", "< .05 | .05 - .1 | > .1", "", "", "< .08 | .08 - .10 | > .10", "> .95 | .90 - .95 | < .90", "> .95 | .90 - .95 | < .90")

# item fit
item_fit <- itemfit(fit, na.rm = TRUE, fit_stats = "PV_Q1")
colnames(item_fit) <- c("Item", "X2", "df", "RMSEA", "p")
item_fit <- item_fit[order(item_fit$X2), ]
rownames(item_fit) <- NULL

robust_item_fit <- itemfit(fit, na.rm = TRUE)
colnames(robust_item_fit) <- c("Item", "PV_Q1", "df", "RMSEA", "p")
robust_item_fit <- robust_item_fit[order(robust_item_fit$PV_Q1), ]
rownames(robust_item_fit) <- NULL

# Item parameters
params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
params <- params[, -which(colnames(params) %in% c("items.g", "items.u", "means", "F1"))]
sink("/dev/null")
sum <- suppressWarnings(suppressMessages(summary(fit, silent = TRUE)))
sink()
params <- cbind(params, sum$rotF)
params <- cbind(params, sum$h2)
params <- params[, c("F1", colnames(params)[-which(colnames(params) %in% "F1")])]
colnames(params) <- c("Factor Loading", "Discrimination Parameter", "Difficulty Parameter 1", "Difficulty Parameter 2", "Difficulty Parameter 3", "Difficulty Parameter 4", "Communalities")
params <- params[order(params$`Factor Loading`, decreasing = TRUE), ]


# # Differential Item Functionining DIF
# # Prepare the data
# gender <- data$gender
# gender <- dplyr::recode(gender, "Man" = 1, "Vrouw" = 2, "Onbekend" = 2)
# gender <- as.factor(gender)
# local_df <- data[, !grepl("package_duration_raw|student_number|student_name|birth_date|gender", colnames(data))]

# # Fit the multiple group model
# dif_fit <- mirt::multipleGroup(
#   data = local_df,
#   model = 1,
#   group = gender
# )

# # Perform DIF analysis
# dif <- mirt::DIF(dif_fit, group = gender, which.par = c("a1", "d"), na.rm = TRUE, plotdif = TRUE)

# person fit
# person_fit <- personfit(fit) %>%
#   reframe(
#     infit.outside = prop.table(table(z.infit > 1.96 | z.infit < -1.96)),
#     outfit.outside = prop.table(table(z.outfit > 1.96 | z.outfit < -1.96))
#   ) # lower row = non-fitting people
# person_fit$Correct <- c("Correct", "Incorrect")
# person_fit <- person_fit %>% dplyr::select(Correct, everything())
```

###### Model Fit

Hieronder bevinden zich de verschillende model fit indices.Hierbij een korte uitleg wat het exact betekent:

- M2: Dit is de M2-statistiek van Maydeu-Olivares, die een maat is voor de absolute fit van het model. Een lager getal duidt op een betere fit. Er is geen vaste cut-off waarde, maar over het algemeen wordt een lagere waarde als beter beschouwd. Als de bijbehorende p-waarde kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- df: de degrees of freedom van het model. Dit is het aantal parameters dat is geschat, minus het aantal constraints dat is opgelegd.
- p-value: de p-waarde van de Goodness of fit test. Als deze kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- RMSEA: RMSEA meet hoe goed een model de waargenomen data benadert, rekening houdend met de fout in het model. Lagere waarden duiden op een betere pasvorm, typisch onder 0.05 voor goede pasvorm en tussen 0.05 en 0.08 voor redelijke pasvorm.
  - RMSEA_5 en RMSEA_95: de 95% betrouwbaarheidsinterval van de RMSEA. Hoe kleiner dit interval rondom de RMSEA waarde, hoe zekerder we zijn van die waarde. 
- SRMSR: SRMSR staat voor "Standardized Root Mean Square Residual". Het is een maat voor de gemiddelde discrepantie tussen de waargenomen en voorspelde covarianties. Een lagere waarde duidt op een betere fit. Een SRMSR van minder dan 0.08 wordt over het algemeen als goed beschouwd.
- TLI: de Tucker-Lewis Index. TLI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.
- CFI: de Comparative Fit Index. De CFI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.
- N: het aantal respondenten in de steekproef.

```{r ouder samenwerken - model fit, echo=FALSE, message=FALSE, warning=FALSE}
model_fit$`Conclusion` <- c("", "", "Poor", "Acceptable", "", "", "Good", "Poor", "Poor")
knitr::kable(model_fit)
```

###### Item Fit

Hieronder bevinden zich de verschillende item fit indices. Hierbij een korte uitleg wat het exact betekent:

- X2: Dit is de X2-statistiek van de item fit test. Een lager getal duidt op een betere fit. Er is geen vaste cut-off waarde, maar over het algemeen wordt een lagere waarde als beter beschouwd. Als de bijbehorende p-waarde kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het item niet goed past.
- df: de degrees of freedom van het model. Dit is het aantal parameters dat is geschat, minus het aantal constraints dat is opgelegd.
- p-value: de p-waarde van de Goodness of fit test. Als deze kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- RMSEA: RMSEA meet hoe goed een model de waargenomen data benadert, rekening houdend met de fout in het model. Lagere waarden duiden op een betere pasvorm, typisch onder 0.05 voor goede pasvorm en tussen 0.05 en 0.08 voor redelijke pasvorm.
 
```{r ouder samenwerken - item fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(item_fit)
```


Als bepaalde waarden niet goed passen in dit model, kan dit verschillende dingen betekenen.
Ten eerste kan het zijn dat de items niet allemaal dezelfde onderliggende eigenschap meten. Ten tweede kan het zijn dat sommige items verkeerd worden begrepen door de respondenten.
Ten derde kan het zijn dat de veronderstelling van lokale onafhankelijkheid wordt geschonden. Dit betekent dat de reacties op de items afhankelijk zijn van elkaar, zelfs na controle voor de onderliggende eigenschap. Bijvoorbeeld, als het beantwoorden van het ene item helpt bij het beantwoorden van een ander item, dan zullen deze items niet goed passen in het model.

Hieronder volgt een robuste variant van de X2 test, namelijk de PV_Q1 test. Hieronder volgt een korte uitleg wat het exact betekent:

De 'PV_Q1'-statistiek is een afgeleide versie van de Q1-statistiek, een chi-kwadraatstatistiek gebruikt in item respons theorie (IRT) modellen om te testen of een item goed past bij het model. 
Een hoge 'PV_Q1'-waarde duidt op een slechte overeenkomst tussen de waargenomen gegevens en voorspellingen van het model, wat suggereert dat het item mogelijk niet conform het verwachte gedrag 
volgens het IRT-model functioneert. Een niet-significante p-waarde, meestal p > 0.05, in combinatie met lage 'PV_Q1'- en RMSEA.PV_Q1-waarden, geeft aan dat het item goed past bij het IRT-model.
Daarnaast is het belangrijk op te merken dat de 'PV_Q1'-statistiek robuust is tegen invloeden van steekproefomvang en lokale afhankelijkheid, waardoor het een betrouwbaar instrument is om de pasvorm
van een item met het IRT-model te beoordelen, ongeacht de omvang van de steekproef of eventuele lokale onderlinge afhankelijkheden tussen de waarnemingen. Hieronder de resultaten:

```{r ouder samenwerken - robust item fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(robust_item_fit)
```

Om een beter beeld te krijgen, volgen hieronder the item parameters van de vragen. 
In IRT, meet een factor loading (ook bekend als een discriminatieparameter) hoe goed een item onderscheid maakt tussen personen met verschillende niveaus van de onderliggende eigenschap. Een hoge factor loading betekent dat het item gevoelig is voor verschillen in de onderliggende eigenschap.
Het belangrijkste verschil met factor analyse, is dat in IRT, de factor loadings ook rekening houden met de moeilijkheidsgraad van de items. Dit betekent dat items die dezelfde moeilijkheidsgraad hebben (dat wil zeggen, ze zijn even moeilijk om te beantwoorden) worden geacht dezelfde hoeveelheid informatie te geven over de onderliggende eigenschap, ongeacht hun factor loadings. Dit is niet het geval in factoranalyse, waar de factor loadings alleen de relatie tussen de geobserveerde variabelen en de onderliggende factor meten.
Tevens kan het zijn dat de factor loadings en de item parameters niet overeenkomen. Bijvoorbeeld een item kan een hoge factor loading hebben (wat aangeeft dat het goed discrimineert) maar toch een slechte fit hebben (X2-statistiek) als het bijvoorbeeld verkeerd wordt begrepen door de respondenten. Omgekeerd kan een item goed passen bij het model maar niet goed discrimineren tussen verschillende niveaus van de eigenschap.
In een Graded response model, zoals hier gebruikt, zijn er meerdere moeilijkheids parameters, een voor elke vrijheidsgraaad van de vraag (totaal antwoordmogelijk - 1). 

```{r ouder samenwerken - item parameters, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(params)
params
```


##### zelfvertrouwen

```{r ouder zelfvertrouwen - stat extraction, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
# Model fit

fit <- irt_fit$gedrag_houding_zelfvertrouwen_irt_fit
model_fit <- as.data.frame(M2(fit, na.rm = TRUE))
model_fit <- cbind(model_fit, nrow(data))
model_fit <- rename(model_fit, `N` = `nrow(data)`)
model_fit <- tidyr::pivot_longer(model_fit, cols = -N, names_to = "Fit Index", values_to = "Value")
model_fit <- dplyr::select(model_fit, `Fit Index`, Value)
model_fit$`Cut-Off` <- c("", "", "> .05", "< .05 | .05 - .1 | > .1", "", "", "< .08 | .08 - .10 | > .10", "> .95 | .90 - .95 | < .90", "> .95 | .90 - .95 | < .90")

# item fit
item_fit <- itemfit(fit, na.rm = TRUE, fit_stats = "PV_Q1")
colnames(item_fit) <- c("Item", "X2", "df", "RMSEA", "p")
item_fit <- item_fit[order(item_fit$X2), ]
rownames(item_fit) <- NULL

robust_item_fit <- itemfit(fit, na.rm = TRUE)
colnames(robust_item_fit) <- c("Item", "PV_Q1", "df", "RMSEA", "p")
robust_item_fit <- robust_item_fit[order(robust_item_fit$PV_Q1), ]
rownames(robust_item_fit) <- NULL

# Item parameters
params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
params <- params[, -which(colnames(params) %in% c("items.g", "items.u", "means", "F1"))]
sink("/dev/null")
sum <- suppressWarnings(suppressMessages(summary(fit, silent = TRUE)))
sink()
params <- cbind(params, sum$rotF)
params <- cbind(params, sum$h2)
params <- params[, c("F1", colnames(params)[-which(colnames(params) %in% "F1")])]
colnames(params) <- c("Factor Loading", "Discrimination Parameter", "Difficulty Parameter 1", "Difficulty Parameter 2", "Difficulty Parameter 3", "Difficulty Parameter 4", "Communalities")
params <- params[order(params$`Factor Loading`, decreasing = TRUE), ]


# # Differential Item Functionining DIF
# # Prepare the data
# gender <- data$gender
# gender <- dplyr::recode(gender, "Man" = 1, "Vrouw" = 2, "Onbekend" = 2)
# gender <- as.factor(gender)
# local_df <- data[, !grepl("package_duration_raw|student_number|student_name|birth_date|gender", colnames(data))]

# # Fit the multiple group model
# dif_fit <- mirt::multipleGroup(
#   data = local_df,
#   model = 1,
#   group = gender
# )

# # Perform DIF analysis
# dif <- mirt::DIF(dif_fit, group = gender, which.par = c("a1", "d"), na.rm = TRUE, plotdif = TRUE)

# person fit
# person_fit <- personfit(fit) %>%
#   reframe(
#     infit.outside = prop.table(table(z.infit > 1.96 | z.infit < -1.96)),
#     outfit.outside = prop.table(table(z.outfit > 1.96 | z.outfit < -1.96))
#   ) # lower row = non-fitting people
# person_fit$Correct <- c("Correct", "Incorrect")
# person_fit <- person_fit %>% dplyr::select(Correct, everything())
```

###### Model Fit

Hieronder bevinden zich de verschillende model fit indices.Hierbij een korte uitleg wat het exact betekent:

- M2: Dit is de M2-statistiek van Maydeu-Olivares, die een maat is voor de absolute fit van het model. Een lager getal duidt op een betere fit. Er is geen vaste cut-off waarde, maar over het algemeen wordt een lagere waarde als beter beschouwd. Als de bijbehorende p-waarde kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- df: de degrees of freedom van het model. Dit is het aantal parameters dat is geschat, minus het aantal constraints dat is opgelegd.
- p-value: de p-waarde van de Goodness of fit test. Als deze kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- RMSEA: RMSEA meet hoe goed een model de waargenomen data benadert, rekening houdend met de fout in het model. Lagere waarden duiden op een betere pasvorm, typisch onder 0.05 voor goede pasvorm en tussen 0.05 en 0.08 voor redelijke pasvorm.
  - RMSEA_5 en RMSEA_95: de 95% betrouwbaarheidsinterval van de RMSEA. Hoe kleiner dit interval rondom de RMSEA waarde, hoe zekerder we zijn van die waarde. 
- SRMSR: SRMSR staat voor "Standardized Root Mean Square Residual". Het is een maat voor de gemiddelde discrepantie tussen de waargenomen en voorspelde covarianties. Een lagere waarde duidt op een betere fit. Een SRMSR van minder dan 0.08 wordt over het algemeen als goed beschouwd.
- TLI: de Tucker-Lewis Index. TLI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.
- CFI: de Comparative Fit Index. De CFI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.
- N: het aantal respondenten in de steekproef.

```{r ouder zelfvertrouwen - model fit, echo=FALSE, message=FALSE, warning=FALSE}
model_fit$`Conclusion` <- c("", "", "Poor", "Acceptable", "", "", "Good", "Poor", "Poor")
knitr::kable(model_fit)
```

###### Item Fit

Hieronder bevinden zich de verschillende item fit indices. Hierbij een korte uitleg wat het exact betekent:

- X2: Dit is de X2-statistiek van de item fit test. Een lager getal duidt op een betere fit. Er is geen vaste cut-off waarde, maar over het algemeen wordt een lagere waarde als beter beschouwd. Als de bijbehorende p-waarde kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het item niet goed past.
- df: de degrees of freedom van het model. Dit is het aantal parameters dat is geschat, minus het aantal constraints dat is opgelegd.
- p-value: de p-waarde van de Goodness of fit test. Als deze kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- RMSEA: RMSEA meet hoe goed een model de waargenomen data benadert, rekening houdend met de fout in het model. Lagere waarden duiden op een betere pasvorm, typisch onder 0.05 voor goede pasvorm en tussen 0.05 en 0.08 voor redelijke pasvorm.
 
```{r ouder zelfvertrouwen - item fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(item_fit)
```


Als bepaalde waarden niet goed passen in dit model, kan dit verschillende dingen betekenen.
Ten eerste kan het zijn dat de items niet allemaal dezelfde onderliggende eigenschap meten. Ten tweede kan het zijn dat sommige items verkeerd worden begrepen door de respondenten.
Ten derde kan het zijn dat de veronderstelling van lokale onafhankelijkheid wordt geschonden. Dit betekent dat de reacties op de items afhankelijk zijn van elkaar, zelfs na controle voor de onderliggende eigenschap. Bijvoorbeeld, als het beantwoorden van het ene item helpt bij het beantwoorden van een ander item, dan zullen deze items niet goed passen in het model.

Hieronder volgt een robuste variant van de X2 test, namelijk de PV_Q1 test. Hieronder volgt een korte uitleg wat het exact betekent:

De 'PV_Q1'-statistiek is een afgeleide versie van de Q1-statistiek, een chi-kwadraatstatistiek gebruikt in item respons theorie (IRT) modellen om te testen of een item goed past bij het model. 
Een hoge 'PV_Q1'-waarde duidt op een slechte overeenkomst tussen de waargenomen gegevens en voorspellingen van het model, wat suggereert dat het item mogelijk niet conform het verwachte gedrag 
volgens het IRT-model functioneert. Een niet-significante p-waarde, meestal p > 0.05, in combinatie met lage 'PV_Q1'- en RMSEA.PV_Q1-waarden, geeft aan dat het item goed past bij het IRT-model.
Daarnaast is het belangrijk op te merken dat de 'PV_Q1'-statistiek robuust is tegen invloeden van steekproefomvang en lokale afhankelijkheid, waardoor het een betrouwbaar instrument is om de pasvorm
van een item met het IRT-model te beoordelen, ongeacht de omvang van de steekproef of eventuele lokale onderlinge afhankelijkheden tussen de waarnemingen. Hieronder de resultaten:

```{r ouder zelfvertrouwen - robust item fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(robust_item_fit)
```

Om een beter beeld te krijgen, volgen hieronder the item parameters van de vragen. 
In IRT, meet een factor loading (ook bekend als een discriminatieparameter) hoe goed een item onderscheid maakt tussen personen met verschillende niveaus van de onderliggende eigenschap. Een hoge factor loading betekent dat het item gevoelig is voor verschillen in de onderliggende eigenschap.
Het belangrijkste verschil met factor analyse, is dat in IRT, de factor loadings ook rekening houden met de moeilijkheidsgraad van de items. Dit betekent dat items die dezelfde moeilijkheidsgraad hebben (dat wil zeggen, ze zijn even moeilijk om te beantwoorden) worden geacht dezelfde hoeveelheid informatie te geven over de onderliggende eigenschap, ongeacht hun factor loadings. Dit is niet het geval in factoranalyse, waar de factor loadings alleen de relatie tussen de geobserveerde variabelen en de onderliggende factor meten.
Tevens kan het zijn dat de factor loadings en de item parameters niet overeenkomen. Bijvoorbeeld een item kan een hoge factor loading hebben (wat aangeeft dat het goed discrimineert) maar toch een slechte fit hebben (X2-statistiek) als het bijvoorbeeld verkeerd wordt begrepen door de respondenten. Omgekeerd kan een item goed passen bij het model maar niet goed discrimineren tussen verschillende niveaus van de eigenschap.
In een Graded response model, zoals hier gebruikt, zijn er meerdere moeilijkheids parameters, een voor elke vrijheidsgraaad van de vraag (totaal antwoordmogelijk - 1). 

```{r ouder zelfvertrouwen - item parameters, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(params)
params
```


##### stabiel

```{r ouder stabiel - stat extraction, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
# Model fit

fit <- irt_fit$gedrag_houding_stabiel_irt_fit
model_fit <- as.data.frame(M2(fit, na.rm = TRUE))
model_fit <- cbind(model_fit, nrow(data))
model_fit <- rename(model_fit, `N` = `nrow(data)`)
model_fit <- tidyr::pivot_longer(model_fit, cols = -N, names_to = "Fit Index", values_to = "Value")
model_fit <- dplyr::select(model_fit, `Fit Index`, Value)
model_fit$`Cut-Off` <- c("", "", "> .05", "< .05 | .05 - .1 | > .1", "", "", "< .08 | .08 - .10 | > .10", "> .95 | .90 - .95 | < .90", "> .95 | .90 - .95 | < .90")

# item fit
item_fit <- itemfit(fit, na.rm = TRUE, fit_stats = "PV_Q1")
colnames(item_fit) <- c("Item", "X2", "df", "RMSEA", "p")
item_fit <- item_fit[order(item_fit$X2), ]
rownames(item_fit) <- NULL

robust_item_fit <- itemfit(fit, na.rm = TRUE)
colnames(robust_item_fit) <- c("Item", "PV_Q1", "df", "RMSEA", "p")
robust_item_fit <- robust_item_fit[order(robust_item_fit$PV_Q1), ]
rownames(robust_item_fit) <- NULL

# Item parameters
params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
params <- params[, -which(colnames(params) %in% c("items.g", "items.u", "means", "F1"))]
sink("/dev/null")
sum <- suppressWarnings(suppressMessages(summary(fit, silent = TRUE)))
sink()
params <- cbind(params, sum$rotF)
params <- cbind(params, sum$h2)
params <- params[, c("F1", colnames(params)[-which(colnames(params) %in% "F1")])]
colnames(params) <- c("Factor Loading", "Discrimination Parameter", "Difficulty Parameter 1", "Difficulty Parameter 2", "Difficulty Parameter 3", "Difficulty Parameter 4", "Communalities")
params <- params[order(params$`Factor Loading`, decreasing = TRUE), ]


# # Differential Item Functionining DIF
# # Prepare the data
# gender <- data$gender
# gender <- dplyr::recode(gender, "Man" = 1, "Vrouw" = 2, "Onbekend" = 2)
# gender <- as.factor(gender)
# local_df <- data[, !grepl("package_duration_raw|student_number|student_name|birth_date|gender", colnames(data))]

# # Fit the multiple group model
# dif_fit <- mirt::multipleGroup(
#   data = local_df,
#   model = 1,
#   group = gender
# )

# # Perform DIF analysis
# dif <- mirt::DIF(dif_fit, group = gender, which.par = c("a1", "d"), na.rm = TRUE, plotdif = TRUE)

# person fit
# person_fit <- personfit(fit) %>%
#   reframe(
#     infit.outside = prop.table(table(z.infit > 1.96 | z.infit < -1.96)),
#     outfit.outside = prop.table(table(z.outfit > 1.96 | z.outfit < -1.96))
#   ) # lower row = non-fitting people
# person_fit$Correct <- c("Correct", "Incorrect")
# person_fit <- person_fit %>% dplyr::select(Correct, everything())
```

###### Model Fit

Hieronder bevinden zich de verschillende model fit indices.Hierbij een korte uitleg wat het exact betekent:

- M2: Dit is de M2-statistiek van Maydeu-Olivares, die een maat is voor de absolute fit van het model. Een lager getal duidt op een betere fit. Er is geen vaste cut-off waarde, maar over het algemeen wordt een lagere waarde als beter beschouwd. Als de bijbehorende p-waarde kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- df: de degrees of freedom van het model. Dit is het aantal parameters dat is geschat, minus het aantal constraints dat is opgelegd.
- p-value: de p-waarde van de Goodness of fit test. Als deze kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- RMSEA: RMSEA meet hoe goed een model de waargenomen data benadert, rekening houdend met de fout in het model. Lagere waarden duiden op een betere pasvorm, typisch onder 0.05 voor goede pasvorm en tussen 0.05 en 0.08 voor redelijke pasvorm.
  - RMSEA_5 en RMSEA_95: de 95% betrouwbaarheidsinterval van de RMSEA. Hoe kleiner dit interval rondom de RMSEA waarde, hoe zekerder we zijn van die waarde. 
- SRMSR: SRMSR staat voor "Standardized Root Mean Square Residual". Het is een maat voor de gemiddelde discrepantie tussen de waargenomen en voorspelde covarianties. Een lagere waarde duidt op een betere fit. Een SRMSR van minder dan 0.08 wordt over het algemeen als goed beschouwd.
- TLI: de Tucker-Lewis Index. TLI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.
- CFI: de Comparative Fit Index. De CFI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.
- N: het aantal respondenten in de steekproef.

```{r ouder stabiel - model fit, echo=FALSE, message=FALSE, warning=FALSE}
model_fit$`Conclusion` <- c("", "", "Poor", "Acceptable", "", "", "Good", "Poor", "Poor")
knitr::kable(model_fit)
```

###### Item Fit

Hieronder bevinden zich de verschillende item fit indices. Hierbij een korte uitleg wat het exact betekent:

- X2: Dit is de X2-statistiek van de item fit test. Een lager getal duidt op een betere fit. Er is geen vaste cut-off waarde, maar over het algemeen wordt een lagere waarde als beter beschouwd. Als de bijbehorende p-waarde kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het item niet goed past.
- df: de degrees of freedom van het model. Dit is het aantal parameters dat is geschat, minus het aantal constraints dat is opgelegd.
- p-value: de p-waarde van de Goodness of fit test. Als deze kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- RMSEA: RMSEA meet hoe goed een model de waargenomen data benadert, rekening houdend met de fout in het model. Lagere waarden duiden op een betere pasvorm, typisch onder 0.05 voor goede pasvorm en tussen 0.05 en 0.08 voor redelijke pasvorm.
 
```{r ouder stabiel - item fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(item_fit)
```


Als bepaalde waarden niet goed passen in dit model, kan dit verschillende dingen betekenen.
Ten eerste kan het zijn dat de items niet allemaal dezelfde onderliggende eigenschap meten. Ten tweede kan het zijn dat sommige items verkeerd worden begrepen door de respondenten.
Ten derde kan het zijn dat de veronderstelling van lokale onafhankelijkheid wordt geschonden. Dit betekent dat de reacties op de items afhankelijk zijn van elkaar, zelfs na controle voor de onderliggende eigenschap. Bijvoorbeeld, als het beantwoorden van het ene item helpt bij het beantwoorden van een ander item, dan zullen deze items niet goed passen in het model.

Hieronder volgt een robuste variant van de X2 test, namelijk de PV_Q1 test. Hieronder volgt een korte uitleg wat het exact betekent:

De 'PV_Q1'-statistiek is een afgeleide versie van de Q1-statistiek, een chi-kwadraatstatistiek gebruikt in item respons theorie (IRT) modellen om te testen of een item goed past bij het model. 
Een hoge 'PV_Q1'-waarde duidt op een slechte overeenkomst tussen de waargenomen gegevens en voorspellingen van het model, wat suggereert dat het item mogelijk niet conform het verwachte gedrag 
volgens het IRT-model functioneert. Een niet-significante p-waarde, meestal p > 0.05, in combinatie met lage 'PV_Q1'- en RMSEA.PV_Q1-waarden, geeft aan dat het item goed past bij het IRT-model.
Daarnaast is het belangrijk op te merken dat de 'PV_Q1'-statistiek robuust is tegen invloeden van steekproefomvang en lokale afhankelijkheid, waardoor het een betrouwbaar instrument is om de pasvorm
van een item met het IRT-model te beoordelen, ongeacht de omvang van de steekproef of eventuele lokale onderlinge afhankelijkheden tussen de waarnemingen. Hieronder de resultaten:

```{r ouder stabiel - robust item fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(robust_item_fit)
```

Om een beter beeld te krijgen, volgen hieronder the item parameters van de vragen. 
In IRT, meet een factor loading (ook bekend als een discriminatieparameter) hoe goed een item onderscheid maakt tussen personen met verschillende niveaus van de onderliggende eigenschap. Een hoge factor loading betekent dat het item gevoelig is voor verschillen in de onderliggende eigenschap.
Het belangrijkste verschil met factor analyse, is dat in IRT, de factor loadings ook rekening houden met de moeilijkheidsgraad van de items. Dit betekent dat items die dezelfde moeilijkheidsgraad hebben (dat wil zeggen, ze zijn even moeilijk om te beantwoorden) worden geacht dezelfde hoeveelheid informatie te geven over de onderliggende eigenschap, ongeacht hun factor loadings. Dit is niet het geval in factoranalyse, waar de factor loadings alleen de relatie tussen de geobserveerde variabelen en de onderliggende factor meten.
Tevens kan het zijn dat de factor loadings en de item parameters niet overeenkomen. Bijvoorbeeld een item kan een hoge factor loading hebben (wat aangeeft dat het goed discrimineert) maar toch een slechte fit hebben (X2-statistiek) als het bijvoorbeeld verkeerd wordt begrepen door de respondenten. Omgekeerd kan een item goed passen bij het model maar niet goed discrimineren tussen verschillende niveaus van de eigenschap.
In een Graded response model, zoals hier gebruikt, zijn er meerdere moeilijkheids parameters, een voor elke vrijheidsgraaad van de vraag (totaal antwoordmogelijk - 1). 

```{r ouder stabiel - item parameters, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(params)
params
```

##### zorgvuldig

```{r ouder zorgvuldig - stat extraction, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
# Model fit

fit <- irt_fit$gedrag_houding_zorgvuldig_irt_fit
model_fit <- as.data.frame(M2(fit, na.rm = TRUE))
model_fit <- cbind(model_fit, nrow(data))
model_fit <- rename(model_fit, `N` = `nrow(data)`)
model_fit <- tidyr::pivot_longer(model_fit, cols = -N, names_to = "Fit Index", values_to = "Value")
model_fit <- dplyr::select(model_fit, `Fit Index`, Value)
model_fit$`Cut-Off` <- c("", "", "> .05", "< .05 | .05 - .1 | > .1", "", "", "< .08 | .08 - .10 | > .10", "> .95 | .90 - .95 | < .90", "> .95 | .90 - .95 | < .90")

# item fit
item_fit <- itemfit(fit, na.rm = TRUE, fit_stats = "PV_Q1")
colnames(item_fit) <- c("Item", "X2", "df", "RMSEA", "p")
item_fit <- item_fit[order(item_fit$X2), ]
rownames(item_fit) <- NULL

robust_item_fit <- itemfit(fit, na.rm = TRUE)
colnames(robust_item_fit) <- c("Item", "PV_Q1", "df", "RMSEA", "p")
robust_item_fit <- robust_item_fit[order(robust_item_fit$PV_Q1), ]
rownames(robust_item_fit) <- NULL

# Item parameters
params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
params <- params[, -which(colnames(params) %in% c("items.g", "items.u", "means", "F1"))]
sink("/dev/null")
sum <- suppressWarnings(suppressMessages(summary(fit, silent = TRUE)))
sink()
params <- cbind(params, sum$rotF)
params <- cbind(params, sum$h2)
params <- params[, c("F1", colnames(params)[-which(colnames(params) %in% "F1")])]
colnames(params) <- c("Factor Loading", "Discrimination Parameter", "Difficulty Parameter 1", "Difficulty Parameter 2", "Difficulty Parameter 3", "Difficulty Parameter 4", "Communalities")
params <- params[order(params$`Factor Loading`, decreasing = TRUE), ]


# # Differential Item Functionining DIF
# # Prepare the data
# gender <- data$gender
# gender <- dplyr::recode(gender, "Man" = 1, "Vrouw" = 2, "Onbekend" = 2)
# gender <- as.factor(gender)
# local_df <- data[, !grepl("package_duration_raw|student_number|student_name|birth_date|gender", colnames(data))]

# # Fit the multiple group model
# dif_fit <- mirt::multipleGroup(
#   data = local_df,
#   model = 1,
#   group = gender
# )

# # Perform DIF analysis
# dif <- mirt::DIF(dif_fit, group = gender, which.par = c("a1", "d"), na.rm = TRUE, plotdif = TRUE)

# person fit
# person_fit <- personfit(fit) %>%
#   reframe(
#     infit.outside = prop.table(table(z.infit > 1.96 | z.infit < -1.96)),
#     outfit.outside = prop.table(table(z.outfit > 1.96 | z.outfit < -1.96))
#   ) # lower row = non-fitting people
# person_fit$Correct <- c("Correct", "Incorrect")
# person_fit <- person_fit %>% dplyr::select(Correct, everything())
```

###### Model Fit

Hieronder bevinden zich de verschillende model fit indices.Hierbij een korte uitleg wat het exact betekent:

- M2: Dit is de M2-statistiek van Maydeu-Olivares, die een maat is voor de absolute fit van het model. Een lager getal duidt op een betere fit. Er is geen vaste cut-off waarde, maar over het algemeen wordt een lagere waarde als beter beschouwd. Als de bijbehorende p-waarde kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- df: de degrees of freedom van het model. Dit is het aantal parameters dat is geschat, minus het aantal constraints dat is opgelegd.
- p-value: de p-waarde van de Goodness of fit test. Als deze kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- RMSEA: RMSEA meet hoe goed een model de waargenomen data benadert, rekening houdend met de fout in het model. Lagere waarden duiden op een betere pasvorm, typisch onder 0.05 voor goede pasvorm en tussen 0.05 en 0.08 voor redelijke pasvorm.
  - RMSEA_5 en RMSEA_95: de 95% betrouwbaarheidsinterval van de RMSEA. Hoe kleiner dit interval rondom de RMSEA waarde, hoe zekerder we zijn van die waarde. 
- SRMSR: SRMSR staat voor "Standardized Root Mean Square Residual". Het is een maat voor de gemiddelde discrepantie tussen de waargenomen en voorspelde covarianties. Een lagere waarde duidt op een betere fit. Een SRMSR van minder dan 0.08 wordt over het algemeen als goed beschouwd.
- TLI: de Tucker-Lewis Index. TLI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.
- CFI: de Comparative Fit Index. De CFI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.
- N: het aantal respondenten in de steekproef.

```{r ouder zorgvuldig - model fit, echo=FALSE, message=FALSE, warning=FALSE}
model_fit$`Conclusion` <- c("", "", "Poor", "Acceptable", "", "", "Good", "Poor", "Poor")
knitr::kable(model_fit)
```

###### Item Fit

Hieronder bevinden zich de verschillende item fit indices. Hierbij een korte uitleg wat het exact betekent:

- X2: Dit is de X2-statistiek van de item fit test. Een lager getal duidt op een betere fit. Er is geen vaste cut-off waarde, maar over het algemeen wordt een lagere waarde als beter beschouwd. Als de bijbehorende p-waarde kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het item niet goed past.
- df: de degrees of freedom van het model. Dit is het aantal parameters dat is geschat, minus het aantal constraints dat is opgelegd.
- p-value: de p-waarde van de Goodness of fit test. Als deze kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- RMSEA: RMSEA meet hoe goed een model de waargenomen data benadert, rekening houdend met de fout in het model. Lagere waarden duiden op een betere pasvorm, typisch onder 0.05 voor goede pasvorm en tussen 0.05 en 0.08 voor redelijke pasvorm.
 
```{r ouder zorgvuldig - item fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(item_fit)
```


Als bepaalde waarden niet goed passen in dit model, kan dit verschillende dingen betekenen.
Ten eerste kan het zijn dat de items niet allemaal dezelfde onderliggende eigenschap meten. Ten tweede kan het zijn dat sommige items verkeerd worden begrepen door de respondenten.
Ten derde kan het zijn dat de veronderstelling van lokale onafhankelijkheid wordt geschonden. Dit betekent dat de reacties op de items afhankelijk zijn van elkaar, zelfs na controle voor de onderliggende eigenschap. Bijvoorbeeld, als het beantwoorden van het ene item helpt bij het beantwoorden van een ander item, dan zullen deze items niet goed passen in het model.

Hieronder volgt een robuste variant van de X2 test, namelijk de PV_Q1 test. Hieronder volgt een korte uitleg wat het exact betekent:

De 'PV_Q1'-statistiek is een afgeleide versie van de Q1-statistiek, een chi-kwadraatstatistiek gebruikt in item respons theorie (IRT) modellen om te testen of een item goed past bij het model. 
Een hoge 'PV_Q1'-waarde duidt op een slechte overeenkomst tussen de waargenomen gegevens en voorspellingen van het model, wat suggereert dat het item mogelijk niet conform het verwachte gedrag 
volgens het IRT-model functioneert. Een niet-significante p-waarde, meestal p > 0.05, in combinatie met lage 'PV_Q1'- en RMSEA.PV_Q1-waarden, geeft aan dat het item goed past bij het IRT-model.
Daarnaast is het belangrijk op te merken dat de 'PV_Q1'-statistiek robuust is tegen invloeden van steekproefomvang en lokale afhankelijkheid, waardoor het een betrouwbaar instrument is om de pasvorm
van een item met het IRT-model te beoordelen, ongeacht de omvang van de steekproef of eventuele lokale onderlinge afhankelijkheden tussen de waarnemingen. Hieronder de resultaten:

```{r ouder zorgvuldig - robust item fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(robust_item_fit)
```

Om een beter beeld te krijgen, volgen hieronder the item parameters van de vragen. 
In IRT, meet een factor loading (ook bekend als een discriminatieparameter) hoe goed een item onderscheid maakt tussen personen met verschillende niveaus van de onderliggende eigenschap. Een hoge factor loading betekent dat het item gevoelig is voor verschillen in de onderliggende eigenschap.
Het belangrijkste verschil met factor analyse, is dat in IRT, de factor loadings ook rekening houden met de moeilijkheidsgraad van de items. Dit betekent dat items die dezelfde moeilijkheidsgraad hebben (dat wil zeggen, ze zijn even moeilijk om te beantwoorden) worden geacht dezelfde hoeveelheid informatie te geven over de onderliggende eigenschap, ongeacht hun factor loadings. Dit is niet het geval in factoranalyse, waar de factor loadings alleen de relatie tussen de geobserveerde variabelen en de onderliggende factor meten.
Tevens kan het zijn dat de factor loadings en de item parameters niet overeenkomen. Bijvoorbeeld een item kan een hoge factor loading hebben (wat aangeeft dat het goed discrimineert) maar toch een slechte fit hebben (X2-statistiek) als het bijvoorbeeld verkeerd wordt begrepen door de respondenten. Omgekeerd kan een item goed passen bij het model maar niet goed discrimineren tussen verschillende niveaus van de eigenschap.
In een Graded response model, zoals hier gebruikt, zijn er meerdere moeilijkheids parameters, een voor elke vrijheidsgraaad van de vraag (totaal antwoordmogelijk - 1). 

```{r ouder zorgvuldig - item parameters, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(params)
params
```


##### open

```{r ouder open - stat extraction, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
# Model fit

fit <- irt_fit$gedrag_houding_open_irt_fit
model_fit <- as.data.frame(M2(fit, na.rm = TRUE))
model_fit <- cbind(model_fit, nrow(data))
model_fit <- rename(model_fit, `N` = `nrow(data)`)
model_fit <- tidyr::pivot_longer(model_fit, cols = -N, names_to = "Fit Index", values_to = "Value")
model_fit <- dplyr::select(model_fit, `Fit Index`, Value)
model_fit$`Cut-Off` <- c("", "", "> .05", "< .05 | .05 - .1 | > .1", "", "", "< .08 | .08 - .10 | > .10", "> .95 | .90 - .95 | < .90", "> .95 | .90 - .95 | < .90")

# item fit
item_fit <- itemfit(fit, na.rm = TRUE, fit_stats = "PV_Q1")
colnames(item_fit) <- c("Item", "X2", "df", "RMSEA", "p")
item_fit <- item_fit[order(item_fit$X2), ]
rownames(item_fit) <- NULL

robust_item_fit <- itemfit(fit, na.rm = TRUE)
colnames(robust_item_fit) <- c("Item", "PV_Q1", "df", "RMSEA", "p")
robust_item_fit <- robust_item_fit[order(robust_item_fit$PV_Q1), ]
rownames(robust_item_fit) <- NULL

# Item parameters
params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
params <- params[, -which(colnames(params) %in% c("items.g", "items.u", "means", "F1"))]
sink("/dev/null")
sum <- suppressWarnings(suppressMessages(summary(fit, silent = TRUE)))
sink()
params <- cbind(params, sum$rotF)
params <- cbind(params, sum$h2)
params <- params[, c("F1", colnames(params)[-which(colnames(params) %in% "F1")])]
colnames(params) <- c("Factor Loading", "Discrimination Parameter", "Difficulty Parameter 1", "Difficulty Parameter 2", "Difficulty Parameter 3", "Difficulty Parameter 4", "Communalities")
params <- params[order(params$`Factor Loading`, decreasing = TRUE), ]


# # Differential Item Functionining DIF
# # Prepare the data
# gender <- data$gender
# gender <- dplyr::recode(gender, "Man" = 1, "Vrouw" = 2, "Onbekend" = 2)
# gender <- as.factor(gender)
# local_df <- data[, !grepl("package_duration_raw|student_number|student_name|birth_date|gender", colnames(data))]

# # Fit the multiple group model
# dif_fit <- mirt::multipleGroup(
#   data = local_df,
#   model = 1,
#   group = gender
# )

# # Perform DIF analysis
# dif <- mirt::DIF(dif_fit, group = gender, which.par = c("a1", "d"), na.rm = TRUE, plotdif = TRUE)

# person fit
# person_fit <- personfit(fit) %>%
#   reframe(
#     infit.outside = prop.table(table(z.infit > 1.96 | z.infit < -1.96)),
#     outfit.outside = prop.table(table(z.outfit > 1.96 | z.outfit < -1.96))
#   ) # lower row = non-fitting people
# person_fit$Correct <- c("Correct", "Incorrect")
# person_fit <- person_fit %>% dplyr::select(Correct, everything())
```

###### Model Fit

Hieronder bevinden zich de verschillende model fit indices.Hierbij een korte uitleg wat het exact betekent:

- M2: Dit is de M2-statistiek van Maydeu-Olivares, die een maat is voor de absolute fit van het model. Een lager getal duidt op een betere fit. Er is geen vaste cut-off waarde, maar over het algemeen wordt een lagere waarde als beter beschouwd. Als de bijbehorende p-waarde kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- df: de degrees of freedom van het model. Dit is het aantal parameters dat is geschat, minus het aantal constraints dat is opgelegd.
- p-value: de p-waarde van de Goodness of fit test. Als deze kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- RMSEA: RMSEA meet hoe goed een model de waargenomen data benadert, rekening houdend met de fout in het model. Lagere waarden duiden op een betere pasvorm, typisch onder 0.05 voor goede pasvorm en tussen 0.05 en 0.08 voor redelijke pasvorm.
  - RMSEA_5 en RMSEA_95: de 95% betrouwbaarheidsinterval van de RMSEA. Hoe kleiner dit interval rondom de RMSEA waarde, hoe zekerder we zijn van die waarde. 
- SRMSR: SRMSR staat voor "Standardized Root Mean Square Residual". Het is een maat voor de gemiddelde discrepantie tussen de waargenomen en voorspelde covarianties. Een lagere waarde duidt op een betere fit. Een SRMSR van minder dan 0.08 wordt over het algemeen als goed beschouwd.
- TLI: de Tucker-Lewis Index. TLI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.
- CFI: de Comparative Fit Index. De CFI is een pasvormindex gebruikt in structurele vergelijkingsmodellen om de relatieve verbetering van het gespecificeerde model te beoordelen in vergelijking met een basismodel. Waarden dicht bij 1 geven een betere pasvorm aan, met een gemeenschappelijke grenswaarde voor acceptabele pasvorm van 0.90 of hoger.
- N: het aantal respondenten in de steekproef.

```{r ouder open - model fit, echo=FALSE, message=FALSE, warning=FALSE}
model_fit$`Conclusion` <- c("", "", "Poor", "Acceptable", "", "", "Good", "Poor", "Poor")
knitr::kable(model_fit)
```

###### Item Fit

Hieronder bevinden zich de verschillende item fit indices. Hierbij een korte uitleg wat het exact betekent:

- X2: Dit is de X2-statistiek van de item fit test. Een lager getal duidt op een betere fit. Er is geen vaste cut-off waarde, maar over het algemeen wordt een lagere waarde als beter beschouwd. Als de bijbehorende p-waarde kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het item niet goed past.
- df: de degrees of freedom van het model. Dit is het aantal parameters dat is geschat, minus het aantal constraints dat is opgelegd.
- p-value: de p-waarde van de Goodness of fit test. Als deze kleiner is dan .05, dan is er een significant verschil tussen de empirische en de geschatte chi-square. Dit betekent dat het model niet goed past.
- RMSEA: RMSEA meet hoe goed een model de waargenomen data benadert, rekening houdend met de fout in het model. Lagere waarden duiden op een betere pasvorm, typisch onder 0.05 voor goede pasvorm en tussen 0.05 en 0.08 voor redelijke pasvorm.
 
```{r ouder open - item fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(item_fit)
```


Als bepaalde waarden niet goed passen in dit model, kan dit verschillende dingen betekenen.
Ten eerste kan het zijn dat de items niet allemaal dezelfde onderliggende eigenschap meten. Ten tweede kan het zijn dat sommige items verkeerd worden begrepen door de respondenten.
Ten derde kan het zijn dat de veronderstelling van lokale onafhankelijkheid wordt geschonden. Dit betekent dat de reacties op de items afhankelijk zijn van elkaar, zelfs na controle voor de onderliggende eigenschap. Bijvoorbeeld, als het beantwoorden van het ene item helpt bij het beantwoorden van een ander item, dan zullen deze items niet goed passen in het model.

Hieronder volgt een robuste variant van de X2 test, namelijk de PV_Q1 test. Hieronder volgt een korte uitleg wat het exact betekent:

De 'PV_Q1'-statistiek is een afgeleide versie van de Q1-statistiek, een chi-kwadraatstatistiek gebruikt in item respons theorie (IRT) modellen om te testen of een item goed past bij het model. 
Een hoge 'PV_Q1'-waarde duidt op een slechte overeenkomst tussen de waargenomen gegevens en voorspellingen van het model, wat suggereert dat het item mogelijk niet conform het verwachte gedrag 
volgens het IRT-model functioneert. Een niet-significante p-waarde, meestal p > 0.05, in combinatie met lage 'PV_Q1'- en RMSEA.PV_Q1-waarden, geeft aan dat het item goed past bij het IRT-model.
Daarnaast is het belangrijk op te merken dat de 'PV_Q1'-statistiek robuust is tegen invloeden van steekproefomvang en lokale afhankelijkheid, waardoor het een betrouwbaar instrument is om de pasvorm
van een item met het IRT-model te beoordelen, ongeacht de omvang van de steekproef of eventuele lokale onderlinge afhankelijkheden tussen de waarnemingen. Hieronder de resultaten:

```{r ouder open - robust item fit, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(robust_item_fit)
```

Om een beter beeld te krijgen, volgen hieronder the item parameters van de vragen. 
In IRT, meet een factor loading (ook bekend als een discriminatieparameter) hoe goed een item onderscheid maakt tussen personen met verschillende niveaus van de onderliggende eigenschap. Een hoge factor loading betekent dat het item gevoelig is voor verschillen in de onderliggende eigenschap.
Het belangrijkste verschil met factor analyse, is dat in IRT, de factor loadings ook rekening houden met de moeilijkheidsgraad van de items. Dit betekent dat items die dezelfde moeilijkheidsgraad hebben (dat wil zeggen, ze zijn even moeilijk om te beantwoorden) worden geacht dezelfde hoeveelheid informatie te geven over de onderliggende eigenschap, ongeacht hun factor loadings. Dit is niet het geval in factoranalyse, waar de factor loadings alleen de relatie tussen de geobserveerde variabelen en de onderliggende factor meten.
Tevens kan het zijn dat de factor loadings en de item parameters niet overeenkomen. Bijvoorbeeld een item kan een hoge factor loading hebben (wat aangeeft dat het goed discrimineert) maar toch een slechte fit hebben (X2-statistiek) als het bijvoorbeeld verkeerd wordt begrepen door de respondenten. Omgekeerd kan een item goed passen bij het model maar niet goed discrimineren tussen verschillende niveaus van de eigenschap.
In een Graded response model, zoals hier gebruikt, zijn er meerdere moeilijkheids parameters, een voor elke vrijheidsgraaad van de vraag (totaal antwoordmogelijk - 1). 

```{r ouder open - item parameters, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(params)
params
```


### Interesse

#### Factor Analyse

#### Item Response Analyse


## grafieken


Hieronder zijn alle grafieken te vinden behorend bij de IRT-analyses. Aan het begin van elke variabel zal een tabel staan
die item nummers met item naam verbind.

### Gedrag_houding

#### betrokkenheid thuis
```{r leerling betrokkenheid_thuis - number to names, echo=FALSE, message=FALSE, warning=FALSE}
irt_fit <- readRDS(paste0(here(), "/data/item_analyses_pers2024/irt_fit_gedrag_houding.rds"))


# Model fit
fit <- irt_fit$gedrag_houding_betrokkenheid_thuis_irt_fit

params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
item_names <- extract.mirt(fit, what = "itemnames")

df <- data.frame(
  item_number = paste0("Item ", seq_along(item_names)),
  item_names = item_names,
  item_discrimination = params$items.a,
  item_difficulty1 = params$items.b1,
  item_difficulty2 = params$items.b2,
  item_difficulty3 = params$items.b3,
  item_difficulty4 = params$items.b4
)
knitr::kable(df)
```

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_betrokkenheid_thuis_irt_fit_ICC_combined.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_betrokkenheid_thuis_irt_fit_IIC_combined.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_betrokkenheid_thuis_irt_fit_IIC_separate.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_betrokkenheid_thuis_irt_fit_TCC.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_betrokkenheid_thuis_irt_fit_TIC.png)

#### concentratie

```{r leerling concentratie - number to names, echo=FALSE, message=FALSE, warning=FALSE}
irt_fit <- readRDS(paste0(here(), "/data/item_analyses_pers2024/irt_fit_gedrag_houding.rds"))


# Model fit
fit <- irt_fit$gedrag_houding_concentratie_irt_fit

params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
item_names <- extract.mirt(fit, what = "itemnames")

df <- data.frame(
  item_number = paste0("Item ", seq_along(item_names)),
  item_names = item_names,
  item_discrimination = params$items.a,
  item_difficulty1 = params$items.b1,
  item_difficulty2 = params$items.b2,
  item_difficulty3 = params$items.b3,
  item_difficulty4 = params$items.b4
)
knitr::kable(df)
```

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_concentratie_irt_fit_ICC_combined.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_concentratie_irt_fit_IIC_combined.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_concentratie_irt_fit_IIC_separate.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_concentratie_irt_fit_TCC.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_concentratie_irt_fit_TIC.png)


#### contact met leeftijdsgenoten

```{r leerling contact_met_leeftijdsgenoten - number to names, echo=FALSE, message=FALSE, warning=FALSE}
irt_fit <- readRDS(paste0(here(), "/data/item_analyses_pers2024/irt_fit_gedrag_houding.rds"))


# Model fit
fit <- irt_fit$gedrag_houding_contact_met_leeftijdsgenoten_irt_fit

params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
item_names <- extract.mirt(fit, what = "itemnames")

df <- data.frame(
  item_number = paste0("Item ", seq_along(item_names)),
  item_names = item_names,
  item_discrimination = params$items.a,
  item_difficulty1 = params$items.b1,
  item_difficulty2 = params$items.b2,
  item_difficulty3 = params$items.b3,
  item_difficulty4 = params$items.b4
)
knitr::kable(df)
```

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_contact_met_leeftijdsgenoten_irt_fit_ICC_combined.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_contact_met_leeftijdsgenoten_irt_fit_IIC_combined.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_contact_met_leeftijdsgenoten_irt_fit_IIC_separate.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_contact_met_leeftijdsgenoten_irt_fit_TCC.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_contact_met_leeftijdsgenoten_irt_fit_TIC.png)


#### interesse in school

```{r leerling interesse_in_school - number to names, echo=FALSE, message=FALSE, warning=FALSE}
irt_fit <- readRDS(paste0(here(), "/data/item_analyses_pers2024/irt_fit_gedrag_houding.rds"))


# Model fit
fit <- irt_fit$gedrag_houding_interesse_in_school_irt_fit

params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
item_names <- extract.mirt(fit, what = "itemnames")

df <- data.frame(
  item_number = paste0("Item ", seq_along(item_names)),
  item_names = item_names,
  item_discrimination = params$items.a,
  item_difficulty1 = params$items.b1,
  item_difficulty2 = params$items.b2,
  item_difficulty3 = params$items.b3,
  item_difficulty4 = params$items.b4
)
knitr::kable(df)
```

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_interesse_in_school_irt_fit_ICC_combined.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_interesse_in_school_irt_fit_IIC_combined.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_interesse_in_school_irt_fit_IIC_separate.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_interesse_in_school_irt_fit_TCC.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_interesse_in_school_irt_fit_TIC.png)

#### motivatie

```{r leerling motivatie - number to names, echo=FALSE, message=FALSE, warning=FALSE}
irt_fit <- readRDS(paste0(here(), "/data/item_analyses_pers2024/irt_fit_gedrag_houding.rds"))


# Model fit
fit <- irt_fit$gedrag_houding_motivatie_om_te_presteren_irt_fit

params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
item_names <- extract.mirt(fit, what = "itemnames")
params
df <- data.frame(
  item_number = paste0("Item ", seq_along(item_names)),
  item_names = item_names,
  item_discrimination = params$items.a,
  item_difficulty1 = params$items.b1,
  item_difficulty2 = params$items.b2,
  item_difficulty3 = params$items.b3,
  item_difficulty4 = params$items.b4
)
knitr::kable(df)
```

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_motivatie_om_te_presteren_irt_fit_ICC_combined.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_motivatie_om_te_presteren_irt_fit_IIC_combined.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_motivatie_om_te_presteren_irt_fit_IIC_separate.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_motivatie_om_te_presteren_irt_fit_TCC.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_motivatie_om_te_presteren_irt_fit_TIC.png)

#### open


```{r leerling open - number to names, echo=FALSE, message=FALSE, warning=FALSE}
irt_fit <- readRDS(paste0(here(), "/data/item_analyses_pers2024/irt_fit_gedrag_houding.rds"))


# Model fit
fit <- irt_fit$gedrag_houding_open_irt_fit

params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
item_names <- extract.mirt(fit, what = "itemnames")

df <- data.frame(
  item_number = paste0("Item ", seq_along(item_names)),
  item_names = item_names,
  item_discrimination = params$items.a,
  item_difficulty1 = params$items.b1,
  item_difficulty2 = params$items.b2,
  item_difficulty3 = params$items.b3,
  item_difficulty4 = params$items.b4
)
knitr::kable(df)
```

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_open_irt_fit_ICC_combined.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_open_irt_fit_IIC_combined.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_open_irt_fit_IIC_separate.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_open_irt_fit_TCC.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_open_irt_fit_TIC.png)


#### samenwerken

```{r leerling samenwerken - number to names, echo=FALSE, message=FALSE, warning=FALSE}
irt_fit <- readRDS(paste0(here(), "/data/item_analyses_pers2024/irt_fit_gedrag_houding.rds"))


# Model fit
fit <- irt_fit$gedrag_houding_samenwerken_irt_fit

params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
item_names <- extract.mirt(fit, what = "itemnames")

df <- data.frame(
  item_number = paste0("Item ", seq_along(item_names)),
  item_names = item_names,
  item_discrimination = params$items.a,
  item_difficulty1 = params$items.b1,
  item_difficulty2 = params$items.b2,
  item_difficulty3 = params$items.b3,
  item_difficulty4 = params$items.b4
)
knitr::kable(df)
```

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_samenwerken_irt_fit_ICC_combined.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_samenwerken_irt_fit_IIC_combined.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_samenwerken_irt_fit_IIC_separate.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_samenwerken_irt_fit_TCC.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_samenwerken_irt_fit_TIC.png)

#### stabiel

```{r leerling stabiel - number to names, echo=FALSE, message=FALSE, warning=FALSE}
irt_fit <- readRDS(paste0(here(), "/data/item_analyses_pers2024/irt_fit_gedrag_houding.rds"))


# Model fit
fit <- irt_fit$gedrag_houding_stabiel_irt_fit

params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
item_names <- extract.mirt(fit, what = "itemnames")

df <- data.frame(
  item_number = paste0("Item ", seq_along(item_names)),
  item_names = item_names,
  item_discrimination = params$items.a,
  item_difficulty1 = params$items.b1,
  item_difficulty2 = params$items.b2,
  item_difficulty3 = params$items.b3,
  item_difficulty4 = params$items.b4
)
knitr::kable(df)
```

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_stabiel_irt_fit_ICC_combined.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_stabiel_irt_fit_IIC_combined.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_stabiel_irt_fit_IIC_separate.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_stabiel_irt_fit_TCC.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_stabiel_irt_fit_TIC.png)

#### zelfvertrouwen

```{r leerling zelfvertrouwen - number to names, echo=FALSE, message=FALSE, warning=FALSE}
irt_fit <- readRDS(paste0(here(), "/data/item_analyses_pers2024/irt_fit_gedrag_houding.rds"))


# Model fit
fit <- irt_fit$gedrag_houding_zelfvertrouwen_irt_fit

params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
item_names <- extract.mirt(fit, what = "itemnames")

df <- data.frame(
  item_number = paste0("Item ", seq_along(item_names)),
  item_names = item_names,
  item_discrimination = params$items.a,
  item_difficulty1 = params$items.b1,
  item_difficulty2 = params$items.b2,
  item_difficulty3 = params$items.b3,
  item_difficulty4 = params$items.b4
)
knitr::kable(df)
```

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_zelfvertrouwen_irt_fit_ICC_combined.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_zelfvertrouwen_irt_fit_IIC_combined.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_zelfvertrouwen_irt_fit_IIC_separate.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_zelfvertrouwen_irt_fit_TCC.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_zelfvertrouwen_irt_fit_TIC.png)

#### zorgvuldig

```{r leerling zorgvuldig - number to names, echo=FALSE, message=FALSE, warning=FALSE}
irt_fit <- readRDS(paste0(here(), "/data/item_analyses_pers2024/irt_fit_gedrag_houding.rds"))


# Model fit
fit <- irt_fit$gedrag_houding_zorgvuldig_irt_fit

params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
item_names <- extract.mirt(fit, what = "itemnames")

df <- data.frame(
  item_number = paste0("Item ", seq_along(item_names)),
  item_names = item_names,
  item_discrimination = params$items.a,
  item_difficulty1 = params$items.b1,
  item_difficulty2 = params$items.b2,
  item_difficulty3 = params$items.b3,
  item_difficulty4 = params$items.b4
)
knitr::kable(df)
```

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_zorgvuldig_irt_fit_ICC_combined.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_zorgvuldig_irt_fit_IIC_combined.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_zorgvuldig_irt_fit_IIC_separate.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_zorgvuldig_irt_fit_TCC.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_zorgvuldig_irt_fit_TIC.png)


### Gedrag_houding_leerkracht

#### betrokkenheid thuis
```{r leerkracht betrokkenheid_thuis - number to names, echo=FALSE, message=FALSE, warning=FALSE}
irt_fit <- readRDS(paste0(here(), "/data/item_analyses_pers2024/irt_fit_gedrag_houding_leerkracht.rds"))


# Model fit
fit <- irt_fit$gedrag_houding_leerkracht_betrokkenheid_thuis_irt_fit

params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
item_names <- extract.mirt(fit, what = "itemnames")

df <- data.frame(
  item_number = paste0("Item ", seq_along(item_names)),
  item_names = item_names,
  item_discrimination = params$items.a,
  item_difficulty1 = params$items.b1,
  item_difficulty2 = params$items.b2,
  item_difficulty3 = params$items.b3,
  item_difficulty4 = params$items.b4
)
knitr::kable(df)
```

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_leerkracht_betrokkenheid_thuis_irt_fit_ICC_combined.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_leerkracht_betrokkenheid_thuis_irt_fit_IIC_combined.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_leerkracht_betrokkenheid_thuis_irt_fit_IIC_separate.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_leerkracht_betrokkenheid_thuis_irt_fit_TCC.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_leerkracht_betrokkenheid_thuis_irt_fit_TIC.png)

#### concentratie

```{r leerkracht concentratie - number to names, echo=FALSE, message=FALSE, warning=FALSE}
irt_fit <- readRDS(paste0(here(), "/data/item_analyses_pers2024/irt_fit_gedrag_houding_leerkracht.rds"))


# Model fit
fit <- irt_fit$gedrag_houding_leerkracht_concentratie_irt_fit

params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
item_names <- extract.mirt(fit, what = "itemnames")

df <- data.frame(
  item_number = paste0("Item ", seq_along(item_names)),
  item_names = item_names,
  item_discrimination = params$items.a,
  item_difficulty1 = params$items.b1,
  item_difficulty2 = params$items.b2,
  item_difficulty3 = params$items.b3,
  item_difficulty4 = params$items.b4
)
knitr::kable(df)
```

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_leerkracht_concentratie_irt_fit_ICC_combined.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_leerkracht_concentratie_irt_fit_IIC_combined.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_leerkracht_concentratie_irt_fit_IIC_separate.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_leerkracht_concentratie_irt_fit_TCC.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_leerkracht_concentratie_irt_fit_TIC.png)


#### contact met leeftijdsgenoten

```{r leerkracht contact_met_leeftijdsgenoten - number to names, echo=FALSE, message=FALSE, warning=FALSE}
irt_fit <- readRDS(paste0(here(), "/data/item_analyses_pers2024/irt_fit_gedrag_houding_leerkracht.rds"))


# Model fit
fit <- irt_fit$gedrag_houding_leerkracht_contact_met_leeftijdsgenoten_irt_fit

params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
item_names <- extract.mirt(fit, what = "itemnames")

df <- data.frame(
  item_number = paste0("Item ", seq_along(item_names)),
  item_names = item_names,
  item_discrimination = params$items.a,
  item_difficulty1 = params$items.b1,
  item_difficulty2 = params$items.b2,
  item_difficulty3 = params$items.b3,
  item_difficulty4 = params$items.b4
)
knitr::kable(df)
```

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_leerkracht_contact_met_leeftijdsgenoten_irt_fit_ICC_combined.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_leerkracht_contact_met_leeftijdsgenoten_irt_fit_IIC_combined.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_leerkracht_contact_met_leeftijdsgenoten_irt_fit_IIC_separate.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_leerkracht_contact_met_leeftijdsgenoten_irt_fit_TCC.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_leerkracht_contact_met_leeftijdsgenoten_irt_fit_TIC.png)


#### interesse in school

```{r leerkracht interesse_in_school - number to names, echo=FALSE, message=FALSE, warning=FALSE}
irt_fit <- readRDS(paste0(here(), "/data/item_analyses_pers2024/irt_fit_gedrag_houding_leerkracht.rds"))


# Model fit
fit <- irt_fit$gedrag_houding_leerkracht_interesse_in_school_irt_fit

params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
item_names <- extract.mirt(fit, what = "itemnames")

df <- data.frame(
  item_number = paste0("Item ", seq_along(item_names)),
  item_names = item_names,
  item_discrimination = params$items.a,
  item_difficulty1 = params$items.b1,
  item_difficulty2 = params$items.b2,
  item_difficulty3 = params$items.b3,
  item_difficulty4 = params$items.b4
)
knitr::kable(df)
```

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_leerkracht_interesse_in_school_irt_fit_ICC_combined.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_leerkracht_interesse_in_school_irt_fit_IIC_combined.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_leerkracht_interesse_in_school_irt_fit_IIC_separate.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_leerkracht_interesse_in_school_irt_fit_TCC.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_leerkracht_interesse_in_school_irt_fit_TIC.png)

#### motivatie


```{r leerkracht motivatie - number to names, echo=FALSE, message=FALSE, warning=FALSE}
irt_fit <- readRDS(paste0(here(), "/data/item_analyses_pers2024/irt_fit_gedrag_houding_leerkracht.rds"))


# Model fit
fit <- irt_fit$gedrag_houding_leerkracht_motivatie_om_te_presteren_irt_fit

params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
item_names <- extract.mirt(fit, what = "itemnames")

df <- data.frame(
  item_number = paste0("Item ", seq_along(item_names)),
  item_names = item_names,
  item_discrimination = params$items.a,
  item_difficulty1 = params$items.b1,
  item_difficulty2 = params$items.b2,
  item_difficulty3 = params$items.b3,
  item_difficulty4 = params$items.b4
)
knitr::kable(df)
```

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_leerkracht_motivatie_om_te_presteren_irt_fit_ICC_combined.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_leerkracht_motivatie_om_te_presteren_irt_fit_IIC_combined.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_leerkracht_motivatie_om_te_presteren_irt_fit_IIC_separate.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_leerkracht_motivatie_om_te_presteren_irt_fit_TCC.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_leerkracht_motivatie_om_te_presteren_irt_fit_TIC.png)

#### open


```{r leerkracht open - number to names, echo=FALSE, message=FALSE, warning=FALSE}
irt_fit <- readRDS(paste0(here(), "/data/item_analyses_pers2024/irt_fit_gedrag_houding_leerkracht.rds"))


# Model fit
fit <- irt_fit$gedrag_houding_leerkracht_open_irt_fit

params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
item_names <- extract.mirt(fit, what = "itemnames")

df <- data.frame(
  item_number = paste0("Item ", seq_along(item_names)),
  item_names = item_names,
  item_discrimination = params$items.a,
  item_difficulty1 = params$items.b1,
  item_difficulty2 = params$items.b2,
  item_difficulty3 = params$items.b3,
  item_difficulty4 = params$items.b4
)
knitr::kable(df)
```

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_leerkracht_open_irt_fit_ICC_combined.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_leerkracht_open_irt_fit_IIC_combined.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_leerkracht_open_irt_fit_IIC_separate.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_leerkracht_open_irt_fit_TCC.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_leerkracht_open_irt_fit_TIC.png)


#### samenwerken

```{r leerkracht samenwerken - number to names, echo=FALSE, message=FALSE, warning=FALSE}
irt_fit <- readRDS(paste0(here(), "/data/item_analyses_pers2024/irt_fit_gedrag_houding_leerkracht.rds"))


# Model fit
fit <- irt_fit$gedrag_houding_leerkracht_samenwerken_irt_fit

params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
item_names <- extract.mirt(fit, what = "itemnames")

df <- data.frame(
  item_number = paste0("Item ", seq_along(item_names)),
  item_names = item_names,
  item_discrimination = params$items.a,
  item_difficulty1 = params$items.b1,
  item_difficulty2 = params$items.b2,
  item_difficulty3 = params$items.b3,
  item_difficulty4 = params$items.b4
)
knitr::kable(df)
```

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_leerkracht_samenwerken_irt_fit_ICC_combined.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_leerkracht_samenwerken_irt_fit_IIC_combined.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_leerkracht_samenwerken_irt_fit_IIC_separate.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_leerkracht_samenwerken_irt_fit_TCC.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_leerkracht_samenwerken_irt_fit_TIC.png)

#### stabiel

```{r leerkracht stabiel - number to names, echo=FALSE, message=FALSE, warning=FALSE}
irt_fit <- readRDS(paste0(here(), "/data/item_analyses_pers2024/irt_fit_gedrag_houding_leerkracht.rds"))


# Model fit
fit <- irt_fit$gedrag_houding_leerkracht_stabiel_irt_fit

params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
item_names <- extract.mirt(fit, what = "itemnames")

df <- data.frame(
  item_number = paste0("Item ", seq_along(item_names)),
  item_names = item_names,
  item_discrimination = params$items.a,
  item_difficulty1 = params$items.b1,
  item_difficulty2 = params$items.b2,
  item_difficulty3 = params$items.b3,
  item_difficulty4 = params$items.b4
)
knitr::kable(df)
```

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_leerkracht_stabiel_irt_fit_ICC_combined.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_leerkracht_stabiel_irt_fit_IIC_combined.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_leerkracht_stabiel_irt_fit_IIC_separate.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_leerkracht_stabiel_irt_fit_TCC.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_leerkracht_stabiel_irt_fit_TIC.png)

#### zelfvertrouwen

```{r leerkracht zelfvertrouwen - number to names, echo=FALSE, message=FALSE, warning=FALSE}
irt_fit <- readRDS(paste0(here(), "/data/item_analyses_pers2024/irt_fit_gedrag_houding_leerkracht.rds"))


# Model fit
fit <- irt_fit$gedrag_houding_leerkracht_zelfvertrouwen_irt_fit

params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
item_names <- extract.mirt(fit, what = "itemnames")

df <- data.frame(
  item_number = paste0("Item ", seq_along(item_names)),
  item_names = item_names,
  item_discrimination = params$items.a,
  item_difficulty1 = params$items.b1,
  item_difficulty2 = params$items.b2,
  item_difficulty3 = params$items.b3,
  item_difficulty4 = params$items.b4
)
knitr::kable(df)
```

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_leerkracht_zelfvertrouwen_irt_fit_ICC_combined.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_leerkracht_zelfvertrouwen_irt_fit_IIC_combined.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_leerkracht_zelfvertrouwen_irt_fit_IIC_separate.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_leerkracht_zelfvertrouwen_irt_fit_TCC.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_leerkracht_zelfvertrouwen_irt_fit_TIC.png)

#### zorgvuldig

```{r leerkracht zorgvuldig - number to names, echo=FALSE, message=FALSE, warning=FALSE}
irt_fit <- readRDS(paste0(here(), "/data/item_analyses_pers2024/irt_fit_gedrag_houding_leerkracht.rds"))


# Model fit
fit <- irt_fit$gedrag_houding_leerkracht_zorgvuldig_irt_fit

params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
item_names <- extract.mirt(fit, what = "itemnames")

df <- data.frame(
  item_number = paste0("Item ", seq_along(item_names)),
  item_names = item_names,
  item_discrimination = params$items.a,
  item_difficulty1 = params$items.b1,
  item_difficulty2 = params$items.b2,
  item_difficulty3 = params$items.b3,
  item_difficulty4 = params$items.b4
)
knitr::kable(df)
```

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_leerkracht_zorgvuldig_irt_fit_ICC_combined.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_leerkracht_zorgvuldig_irt_fit_IIC_combined.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_leerkracht_zorgvuldig_irt_fit_IIC_separate.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_leerkracht_zorgvuldig_irt_fit_TCC.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_leerkracht_zorgvuldig_irt_fit_TIC.png)


### Gedrag_houding_ouder


#### betrokkenheid thuis
```{r ouder betrokkenheid_thuis - number to names, echo=FALSE, message=FALSE, warning=FALSE}
irt_fit <- readRDS(paste0(here(), "/data/item_analyses_pers2024/irt_fit_gedrag_houding_ouder.rds"))


# Model fit
fit <- irt_fit$gedrag_houding_ouder_betrokkenheid_thuis_irt_fit

params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
item_names <- extract.mirt(fit, what = "itemnames")

df <- data.frame(
  item_number = paste0("Item ", seq_along(item_names)),
  item_names = item_names,
  item_discrimination = params$items.a,
  item_difficulty1 = params$items.b1,
  item_difficulty2 = params$items.b2,
  item_difficulty3 = params$items.b3,
  item_difficulty4 = params$items.b4
)
knitr::kable(df)
```

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_ouder_betrokkenheid_thuis_irt_fit_ICC_combined.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_ouder_betrokkenheid_thuis_irt_fit_IIC_combined.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_ouder_betrokkenheid_thuis_irt_fit_IIC_separate.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_ouder_betrokkenheid_thuis_irt_fit_TCC.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_ouder_betrokkenheid_thuis_irt_fit_TIC.png)

#### concentratie

```{r ouder concentratie - number to names, echo=FALSE, message=FALSE, warning=FALSE}
irt_fit <- readRDS(paste0(here(), "/data/item_analyses_pers2024/irt_fit_gedrag_houding_ouder.rds"))


# Model fit
fit <- irt_fit$gedrag_houding_ouder_concentratie_irt_fit

params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
item_names <- extract.mirt(fit, what = "itemnames")

df <- data.frame(
  item_number = paste0("Item ", seq_along(item_names)),
  item_names = item_names,
  item_discrimination = params$items.a,
  item_difficulty1 = params$items.b1,
  item_difficulty2 = params$items.b2,
  item_difficulty3 = params$items.b3,
  item_difficulty4 = params$items.b4
)
knitr::kable(df)
```

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_ouder_concentratie_irt_fit_ICC_combined.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_ouder_concentratie_irt_fit_IIC_combined.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_ouder_concentratie_irt_fit_IIC_separate.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_ouder_concentratie_irt_fit_TCC.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_ouder_concentratie_irt_fit_TIC.png)


#### contact met leeftijdsgenoten

```{r ouder contact_met_leeftijdsgenoten - number to names, echo=FALSE, message=FALSE, warning=FALSE}
irt_fit <- readRDS(paste0(here(), "/data/item_analyses_pers2024/irt_fit_gedrag_houding_ouder.rds"))


# Model fit
fit <- irt_fit$gedrag_houding_ouder_contact_met_leeftijdsgenoten_irt_fit

params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
item_names <- extract.mirt(fit, what = "itemnames")

df <- data.frame(
  item_number = paste0("Item ", seq_along(item_names)),
  item_names = item_names,
  item_discrimination = params$items.a,
  item_difficulty1 = params$items.b1,
  item_difficulty2 = params$items.b2,
  item_difficulty3 = params$items.b3,
  item_difficulty4 = params$items.b4
)
knitr::kable(df)
```

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_ouder_contact_met_leeftijdsgenoten_irt_fit_ICC_combined.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_ouder_contact_met_leeftijdsgenoten_irt_fit_IIC_combined.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_ouder_contact_met_leeftijdsgenoten_irt_fit_IIC_separate.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_ouder_contact_met_leeftijdsgenoten_irt_fit_TCC.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_ouder_contact_met_leeftijdsgenoten_irt_fit_TIC.png)


#### interesse in school

```{r ouder interesse_in_school - number to names, echo=FALSE, message=FALSE, warning=FALSE}
irt_fit <- readRDS(paste0(here(), "/data/item_analyses_pers2024/irt_fit_gedrag_houding_ouder.rds"))


# Model fit
fit <- irt_fit$gedrag_houding_ouder_interesse_in_school_irt_fit

params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
item_names <- extract.mirt(fit, what = "itemnames")

df <- data.frame(
  item_number = paste0("Item ", seq_along(item_names)),
  item_names = item_names,
  item_discrimination = params$items.a,
  item_difficulty1 = params$items.b1,
  item_difficulty2 = params$items.b2,
  item_difficulty3 = params$items.b3,
  item_difficulty4 = params$items.b4
)
knitr::kable(df)
```

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_ouder_interesse_in_school_irt_fit_ICC_combined.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_ouder_interesse_in_school_irt_fit_IIC_combined.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_ouder_interesse_in_school_irt_fit_IIC_separate.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_ouder_interesse_in_school_irt_fit_TCC.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_ouder_interesse_in_school_irt_fit_TIC.png)

#### motivatie


```{r ouder motivatie - number to names, echo=FALSE, message=FALSE, warning=FALSE}
irt_fit <- readRDS(paste0(here(), "/data/item_analyses_pers2024/irt_fit_gedrag_houding_ouder.rds"))


# Model fit
fit <- irt_fit$gedrag_houding_ouder_motivatie_om_te_presteren_irt_fit

params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
item_names <- extract.mirt(fit, what = "itemnames")

df <- data.frame(
  item_number = paste0("Item ", seq_along(item_names)),
  item_names = item_names,
  item_discrimination = params$items.a,
  item_difficulty1 = params$items.b1,
  item_difficulty2 = params$items.b2,
  item_difficulty3 = params$items.b3,
  item_difficulty4 = params$items.b4
)
knitr::kable(df)
```

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_ouder_motivatie_om_te_presteren_irt_fit_ICC_combined.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_ouder_motivatie_om_te_presteren_irt_fit_IIC_combined.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_ouder_motivatie_om_te_presteren_irt_fit_IIC_separate.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_ouder_motivatie_om_te_presteren_irt_fit_TCC.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_ouder_motivatie_om_te_presteren_irt_fit_TIC.png)

#### open


```{r ouder open - number to names, echo=FALSE, message=FALSE, warning=FALSE}
irt_fit <- readRDS(paste0(here(), "/data/item_analyses_pers2024/irt_fit_gedrag_houding_ouder.rds"))


# Model fit
fit <- irt_fit$gedrag_houding_ouder_open_irt_fit

params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
item_names <- extract.mirt(fit, what = "itemnames")

df <- data.frame(
  item_number = paste0("Item ", seq_along(item_names)),
  item_names = item_names,
  item_discrimination = params$items.a,
  item_difficulty1 = params$items.b1,
  item_difficulty2 = params$items.b2,
  item_difficulty3 = params$items.b3,
  item_difficulty4 = params$items.b4
)
knitr::kable(df)
```

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_ouder_open_irt_fit_ICC_combined.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_ouder_open_irt_fit_IIC_combined.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_ouder_open_irt_fit_IIC_separate.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_ouder_open_irt_fit_TCC.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_ouder_open_irt_fit_TIC.png)


#### samenwerken

```{r ouder samenwerken - number to names, echo=FALSE, message=FALSE, warning=FALSE}
irt_fit <- readRDS(paste0(here(), "/data/item_analyses_pers2024/irt_fit_gedrag_houding_ouder.rds"))


# Model fit
fit <- irt_fit$gedrag_houding_ouder_samenwerken_irt_fit

params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
item_names <- extract.mirt(fit, what = "itemnames")

df <- data.frame(
  item_number = paste0("Item ", seq_along(item_names)),
  item_names = item_names,
  item_discrimination = params$items.a,
  item_difficulty1 = params$items.b1,
  item_difficulty2 = params$items.b2,
  item_difficulty3 = params$items.b3,
  item_difficulty4 = params$items.b4
)
knitr::kable(df)
```

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_ouder_samenwerken_irt_fit_ICC_combined.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_ouder_samenwerken_irt_fit_IIC_combined.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_ouder_samenwerken_irt_fit_IIC_separate.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_ouder_samenwerken_irt_fit_TCC.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_ouder_samenwerken_irt_fit_TIC.png)

#### stabiel

```{r ouder stabiel - number to names, echo=FALSE, message=FALSE, warning=FALSE}
irt_fit <- readRDS(paste0(here(), "/data/item_analyses_pers2024/irt_fit_gedrag_houding_ouder.rds"))


# Model fit
fit <- irt_fit$gedrag_houding_ouder_stabiel_irt_fit

params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
item_names <- extract.mirt(fit, what = "itemnames")

df <- data.frame(
  item_number = paste0("Item ", seq_along(item_names)),
  item_names = item_names,
  item_discrimination = params$items.a,
  item_difficulty1 = params$items.b1,
  item_difficulty2 = params$items.b2,
  item_difficulty3 = params$items.b3,
  item_difficulty4 = params$items.b4
)
knitr::kable(df)
```

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_ouder_stabiel_irt_fit_ICC_combined.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_ouder_stabiel_irt_fit_IIC_combined.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_ouder_stabiel_irt_fit_IIC_separate.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_ouder_stabiel_irt_fit_TCC.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_ouder_stabiel_irt_fit_TIC.png)

#### zelfvertrouwen

```{r ouder zelfvertrouwen - number to names, echo=FALSE, message=FALSE, warning=FALSE}
irt_fit <- readRDS(paste0(here(), "/data/item_analyses_pers2024/irt_fit_gedrag_houding_ouder.rds"))


# Model fit
fit <- irt_fit$gedrag_houding_ouder_zelfvertrouwen_irt_fit

params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
item_names <- extract.mirt(fit, what = "itemnames")

df <- data.frame(
  item_number = paste0("Item ", seq_along(item_names)),
  item_names = item_names,
  item_discrimination = params$items.a,
  item_difficulty1 = params$items.b1,
  item_difficulty2 = params$items.b2,
  item_difficulty3 = params$items.b3,
  item_difficulty4 = params$items.b4
)
knitr::kable(df)
```

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_ouder_zelfvertrouwen_irt_fit_ICC_combined.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_ouder_zelfvertrouwen_irt_fit_IIC_combined.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_ouder_zelfvertrouwen_irt_fit_IIC_separate.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_ouder_zelfvertrouwen_irt_fit_TCC.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_ouder_zelfvertrouwen_irt_fit_TIC.png)

#### zorgvuldig

```{r ouder zorgvuldig - number to names, echo=FALSE, message=FALSE, warning=FALSE}
irt_fit <- readRDS(paste0(here(), "/data/item_analyses_pers2024/irt_fit_gedrag_houding_ouder.rds"))


# Model fit
fit <- irt_fit$gedrag_houding_ouder_zorgvuldig_irt_fit

params <- as.data.frame(coef(fit, IRTpars = TRUE, simplify = TRUE))
item_names <- extract.mirt(fit, what = "itemnames")

df <- data.frame(
  item_number = paste0("Item ", seq_along(item_names)),
  item_names = item_names,
  item_discrimination = params$items.a,
  item_difficulty1 = params$items.b1,
  item_difficulty2 = params$items.b2,
  item_difficulty3 = params$items.b3,
  item_difficulty4 = params$items.b4
)
knitr::kable(df)
```

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_ouder_zorgvuldig_irt_fit_ICC_combined.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_ouder_zorgvuldig_irt_fit_IIC_combined.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_ouder_zorgvuldig_irt_fit_IIC_separate.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_ouder_zorgvuldig_irt_fit_TCC.png)

![](/Users/jesse/Repos/r-jvb-aansluiting/data/item_analyses_pers`r format(Sys.Date(), "%Y")`/graphs/gedrag_houding_ouder_zorgvuldig_irt_fit_TIC.png)

